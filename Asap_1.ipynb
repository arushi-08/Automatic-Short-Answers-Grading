{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda,Reshape,concatenate,Input, Embedding, LSTM,GRU\n",
    "from keras.layers import Dense,Dropout, Activation ,Flatten ,RepeatVector, Bidirectional,GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.activations import softmax\n",
    "from keras import regularizers\n",
    "\n",
    "from keras import backend as K, regularizers, constraints, initializers\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "from keras.layers import merge\n",
    "from keras.layers.convolutional import Conv1D,Conv2D\n",
    "from keras.layers.convolutional import MaxPooling1D,MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.layers import Concatenate,Dot\n",
    "from keras.layers import Permute, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pickle.load(open(\"ASAP_withoutCosineSim_DATA.dat\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"ASAP_TRAIN_ESSAY1_DATA.dat\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>EssaySet</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>EssayText</th>\n",
       "      <th>clean</th>\n",
       "      <th>X</th>\n",
       "      <th>X_1</th>\n",
       "      <th>error_words</th>\n",
       "      <th>total_errors</th>\n",
       "      <th>CosineSim_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Some additional information that we would need...</td>\n",
       "      <td>some additional information that we would need...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.868468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>After reading the expirement, I realized that ...</td>\n",
       "      <td>after reading the expirement , i realized that...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[expirement, expireiment, amant, yar, expireme...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.836522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What you need is more trials, a control set up...</td>\n",
       "      <td>what you need is more trials , a control set u...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.835054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The student should list what rock is better an...</td>\n",
       "      <td>the student should list what rock is better an...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>For the students to be able to make a replicat...</td>\n",
       "      <td>for the students to be able to make a replicat...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[tipe, expirement]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.878116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  EssaySet  Score1  Score2  \\\n",
       "0   1         1       1       1   \n",
       "1   2         1       1       1   \n",
       "2   3         1       1       1   \n",
       "3   4         1       0       0   \n",
       "4   5         1       2       2   \n",
       "\n",
       "                                           EssayText  \\\n",
       "0  Some additional information that we would need...   \n",
       "1  After reading the expirement, I realized that ...   \n",
       "2  What you need is more trials, a control set up...   \n",
       "3  The student should list what rock is better an...   \n",
       "4  For the students to be able to make a replicat...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  some additional information that we would need...   \n",
       "1  after reading the expirement , i realized that...   \n",
       "2  what you need is more trials , a control set u...   \n",
       "3  the student should list what rock is better an...   \n",
       "4  for the students to be able to make a replicat...   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                                 X_1  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                         error_words  total_errors  \\\n",
       "0                                                 []             0   \n",
       "1  [expirement, expireiment, amant, yar, expireme...             6   \n",
       "2                                                 []             0   \n",
       "3                                                 []             0   \n",
       "4                                 [tipe, expirement]             2   \n",
       "\n",
       "   CosineSim_2  \n",
       "0     0.868468  \n",
       "1     0.836522  \n",
       "2     0.835054  \n",
       "3     0.710318  \n",
       "4     0.878116  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>EssaySet</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>EssayText</th>\n",
       "      <th>clean</th>\n",
       "      <th>X</th>\n",
       "      <th>X_1</th>\n",
       "      <th>error_words</th>\n",
       "      <th>total_errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Some additional information that we would need...</td>\n",
       "      <td>some additional information that we would need...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>After reading the expirement, I realized that ...</td>\n",
       "      <td>after reading the expirement , i realized that...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[expirement, expireiment, amant, yar, expireme...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What you need is more trials, a control set up...</td>\n",
       "      <td>what you need is more trials , a control set u...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The student should list what rock is better an...</td>\n",
       "      <td>the student should list what rock is better an...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>For the students to be able to make a replicat...</td>\n",
       "      <td>for the students to be able to make a replicat...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[tipe, expirement]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  EssaySet  Score1  Score2  \\\n",
       "0   1         1       1       1   \n",
       "1   2         1       1       1   \n",
       "2   3         1       1       1   \n",
       "3   4         1       0       0   \n",
       "4   5         1       2       2   \n",
       "\n",
       "                                           EssayText  \\\n",
       "0  Some additional information that we would need...   \n",
       "1  After reading the expirement, I realized that ...   \n",
       "2  What you need is more trials, a control set up...   \n",
       "3  The student should list what rock is better an...   \n",
       "4  For the students to be able to make a replicat...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  some additional information that we would need...   \n",
       "1  after reading the expirement , i realized that...   \n",
       "2  what you need is more trials , a control set u...   \n",
       "3  the student should list what rock is better an...   \n",
       "4  for the students to be able to make a replicat...   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                                 X_1  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                         error_words  total_errors  \n",
       "0                                                 []             0  \n",
       "1  [expirement, expireiment, amant, yar, expireme...             6  \n",
       "2                                                 []             0  \n",
       "3                                                 []             0  \n",
       "4                                 [tipe, expirement]             2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>EssaySet</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>EssayText</th>\n",
       "      <th>clean</th>\n",
       "      <th>X</th>\n",
       "      <th>X_1</th>\n",
       "      <th>error_words</th>\n",
       "      <th>total_errors</th>\n",
       "      <th>CosineSim_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Some additional information that we would need...</td>\n",
       "      <td>some additional information that we would need...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.868468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>After reading the expirement, I realized that ...</td>\n",
       "      <td>after reading the expirement , i realized that...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[expirement, expireiment, amant, yar, expireme...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.836522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What you need is more trials, a control set up...</td>\n",
       "      <td>what you need is more trials , a control set u...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.835054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The student should list what rock is better an...</td>\n",
       "      <td>the student should list what rock is better an...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>For the students to be able to make a replicat...</td>\n",
       "      <td>for the students to be able to make a replicat...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[tipe, expirement]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.878116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  EssaySet  Score1  Score2  \\\n",
       "0   1         1       1       1   \n",
       "1   2         1       1       1   \n",
       "2   3         1       1       1   \n",
       "3   4         1       0       0   \n",
       "4   5         1       2       2   \n",
       "\n",
       "                                           EssayText  \\\n",
       "0  Some additional information that we would need...   \n",
       "1  After reading the expirement, I realized that ...   \n",
       "2  What you need is more trials, a control set up...   \n",
       "3  The student should list what rock is better an...   \n",
       "4  For the students to be able to make a replicat...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  some additional information that we would need...   \n",
       "1  after reading the expirement , i realized that...   \n",
       "2  what you need is more trials , a control set u...   \n",
       "3  the student should list what rock is better an...   \n",
       "4  for the students to be able to make a replicat...   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                                 X_1  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                         error_words  total_errors  \\\n",
       "0                                                 []             0   \n",
       "1  [expirement, expireiment, amant, yar, expireme...             6   \n",
       "2                                                 []             0   \n",
       "3                                                 []             0   \n",
       "4                                 [tipe, expirement]             2   \n",
       "\n",
       "   CosineSim_2  \n",
       "0     0.868468  \n",
       "1     0.836522  \n",
       "2     0.835054  \n",
       "3     0.710318  \n",
       "4     0.878116  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CosineSim_2'] = data1['CosineSim_2']\n",
    "# data= data.drop(columns='CosineSim')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You need to know how much vinegar was used in each container. You need to know what type of vinegar was used in each container. You need to know what materials to test. You need to know what size/surface area of materials should be used. You need to know how long each sample was rinsed in distilled water.You need to know what drying method to use. You need to know what size/type of container to use. Other acceptable responses.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_ans='You need to know how much vinegar was used in each container. You need to know what type of vinegar was used in each container. You need to know what materials to test. You need to know what size/surface area of materials should be used. You need to know how long each sample was rinsed in distilled water.You need to know what drying method to use. You need to know what size/type of container to use. Other acceptable responses.'\n",
    "ref_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reference_ans'] = ref_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length Ratio Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from string import punctuation,digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuation(s):\n",
    "    list_punctuation = list(punctuation)\n",
    "    for i in list_punctuation:\n",
    "        s = s.replace(i,' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Ref_ans1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1fdabd4f9f85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mref_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRef_ans1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mref_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Ref_ans1' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokens = sentence.split()\n",
    "    tokens = [remove_punctuation(w) for w in tokens]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "ref_sent = clean_sentence(Ref_ans1)\n",
    "ref_length = len(ref_sent.split())\n",
    "\n",
    "sentence = [clean_sentence(x) for x in data['clean']]\n",
    "\n",
    "lengths = [len(s.split()) for s in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_ratio(sentence1, sentence2):\n",
    "    return (sentence1) / (sentence2/2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7419354838709677\n"
     ]
    }
   ],
   "source": [
    "print(length_ratio(lengths[0],lengths[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_ratio = [length_ratio(i,ref_length) for i in lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'length_ratio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-fe9c9c6228df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'length_ratio'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlength_ratio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#data['length_ratio']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'length_ratio' is not defined"
     ]
    }
   ],
   "source": [
    "data['length_ratio'] = length_ratio\n",
    "#data['length_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines) \n",
    "    return tokenizer\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len =  133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aruhi/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = [len(s.split()) for s in data['clean']]\n",
    "\n",
    "print('max len = ',max(lengths))\n",
    "sns.distplot(lengths)\n",
    "max_length  = max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total =[]\n",
    "for i in data['clean']:\n",
    "    total.append(i)\n",
    "total.append(ref_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = \" \".join(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(data['reference_ans']+data['clean'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "X_token = encode_text(tokenizer, data['clean'],max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ref = encode_text(tokenizer,data['reference_ans'],max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430\n"
     ]
    }
   ],
   "source": [
    "length = [len(i) for i in data['reference_ans']]\n",
    "print(max(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  2,  1,  4, 13, 20, 14, 12, 11,  7,  9, 10,  3,  2,  1,  4,  5,\n",
       "       18,  6, 14, 12, 11,  7,  9, 10,  3,  2,  1,  4,  5, 16,  1, 30,  3,\n",
       "        2,  1,  4,  5, 17, 31, 29,  6, 16, 21, 19, 11,  3,  2,  1,  4, 13,\n",
       "       26,  9, 22, 12, 28,  7, 25, 24,  3,  2,  1,  4,  5, 32, 33,  1, 15,\n",
       "        3,  2,  1,  4,  5, 17, 18,  6, 10,  1, 15, 27, 34, 35,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ref[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 1193514  words loaded!\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',errors='ignore')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        try: \n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            model[word] = embedding\n",
    "        except :\n",
    "            pass\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "model = loadGloveModel(r\"/home/aruhi/Word Embeddings/glove.twitter.27B/glove.twitter.27B.200d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 1193514  words loaded!\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',errors='ignore')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        try: \n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            model[word] = embedding\n",
    "        except :\n",
    "            pass\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "model = loadGloveModel(r\"/home/aruhi/Word Embeddings/glove.twitter.27B/glove.twitter.27B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model,open(\"GLOVE_100.DAT\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model,open(\"GLOVE_200.DAT\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(\"GLOVE_200.DAT\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(\"GLOVE_100.DAT\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def get_glove_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,200))\n",
    "    w2v={}\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "            w2v[word]=i\n",
    "    return embedding_matrix,w2v\n",
    "\n",
    "\n",
    "embedding_matrix_glove,word_2_index_glove = get_glove_embedding_matrix(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "def get_word2vec_embedding_matrix(model):\n",
    "    embedding_matrix = np.zeros((vocab_size,300))\n",
    "    w2v={}\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "            w2v[word]=i\n",
    "    return embedding_matrix,w2v\n",
    "\n",
    "word2vec_model= pickle.load(open('D:\\GoogleNews-vectors-negative300.dat','rb'))\n",
    "\n",
    "embedding_matrix_word2vec,word_2_index = get_word2vec_embedding_matrix(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(embedding_matrix_word2vec,open(\"Google_News_Essays.dat\",\"wb\"))\n",
    "#pickle.dump(glove_embedding,open(\"Glove_Essays.dat\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_word2vec = pickle.load(open(\"Google_News_Essays.dat\",\"rb\"))\n",
    "glove_embedding=pickle.load(open(\"Glove_Essays.dat\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "        \n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "#        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number  to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        \n",
    "        print(result.shape)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model to get the intermediate layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "     EarlyStopping(monitor='val_acc' , patience = 5 , verbose=1)\n",
    "    #ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1672, 133) (1672,)\n",
      "(1337, 133) (1337,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "count=0;\n",
    "test_count=0;\n",
    "cvscores = []\n",
    "final_Core= [] \n",
    "ID=[]\n",
    "NUMBER=[]\n",
    "\n",
    "#for i in np.unique(data['EssaySet']):\n",
    "    \n",
    "#train\n",
    "essay_list, resolved_scores = X_token , data['Score1']\n",
    "\n",
    "dataX = essay_list\n",
    "dataY = resolved_scores\n",
    "no_of_classes = len(set(resolved_scores))\n",
    "\n",
    "trainX = [np.asarray(i) for i in essay_list]\n",
    "trainX = np.asarray(trainX)\n",
    "trainY = dataY\n",
    "trainY = np.asarray(trainY)\n",
    "\n",
    "print ((trainX.shape),(trainY.shape))\n",
    "\n",
    "\n",
    "\n",
    "    #print (train_index , test_index)\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainX,trainY,test_size = 0.2)\n",
    "print (X_train.shape, y_train.shape)\n",
    "y_train = to_categorical(y_train , 4)\n",
    "y_test = to_categorical(y_test , 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1337, 133)\n",
      "(335, 133)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape),print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1337, 4)\n",
      "(335, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape),print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model2(dropout,learning_rate,em,em_dim,lstm_out, n_hidden_layer,em_trainable_flag,input_shape_1,n_filters=100):\n",
    "    print(input_shape_1)\n",
    "    input_context= Input(shape=(max_length,),name='Sentence')\n",
    "    input_features = Input(shape=(input_shape_1,),name='Features')\n",
    "    \n",
    "    hidden = Dense(250,activation='relu')(input_features)\n",
    "    hidden = Dense(200,activation='relu') (hidden)\n",
    "\n",
    "    embedding1=Embedding(vocab_size, em_dim, weights = [eval(em)],input_length=max_length,trainable = False)\n",
    "    \n",
    "    context1= embedding1(input_context)\n",
    "    \n",
    "    concat1=Dropout(0.3)(context1)\n",
    "\n",
    "    c3=Conv1D(n_filters,kernel_size=3,activation='relu')(concat1)\n",
    "    drop3 = Dropout(0.2)(c3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat = Flatten()(pool3)\n",
    "\n",
    "    C= concatenate([flat, hidden])    \n",
    "    a=Dense(327,activation='relu')(C)\n",
    "        \n",
    "    out=Dense(4,activation='softmax')(a)\n",
    "    \n",
    "    model= Model(inputs=[input_context,input_features] ,outputs=out)\n",
    "    \n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13218\n"
     ]
    }
   ],
   "source": [
    "model=define_model2(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix_glove',\n",
    "                     em_trainable_flag=False,\n",
    "                     em_dim=200,\n",
    "                    input_shape_1=X_features_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1337 samples, validate on 335 samples\n",
      "Epoch 1/100\n",
      "1337/1337 [==============================] - 10s 8ms/step - loss: 1.5761 - acc: 0.3351 - val_loss: 1.3160 - val_acc: 0.3612\n",
      "Epoch 2/100\n",
      "1337/1337 [==============================] - 7s 5ms/step - loss: 1.1952 - acc: 0.4607 - val_loss: 1.2847 - val_acc: 0.4030\n",
      "Epoch 3/100\n",
      "1337/1337 [==============================] - 7s 5ms/step - loss: 0.8099 - acc: 0.6784 - val_loss: 1.3369 - val_acc: 0.4119\n",
      "Epoch 4/100\n",
      "1337/1337 [==============================] - 6s 4ms/step - loss: 0.3073 - acc: 0.9080 - val_loss: 1.8086 - val_acc: 0.4358\n",
      "Epoch 5/100\n",
      "1337/1337 [==============================] - 7s 5ms/step - loss: 0.0747 - acc: 0.9843 - val_loss: 2.3318 - val_acc: 0.3821\n",
      "Epoch 6/100\n",
      "1337/1337 [==============================] - 6s 5ms/step - loss: 0.0230 - acc: 0.9963 - val_loss: 2.4969 - val_acc: 0.3791\n",
      "Epoch 7/100\n",
      "1337/1337 [==============================] - 6s 5ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 2.7786 - val_acc: 0.3552\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feeddfe85f8>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EarlyStop= EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n",
    "\n",
    "\n",
    "model.fit(x=[X_train,X_features_train],y=y_train, epochs=100,batch_size=32,\\\n",
    "          callbacks=[EarlyStop],validation_data=([X_test,X_features_test],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4699411230752575\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([X_test,X_features_test])\n",
    "out= get_class_from_pred(pred)\n",
    "actual= get_class_from_pred(y_test)\n",
    "print(m.quadratic_weighted_kappa(actual,out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro:  0.2583111147581683\n",
      "Weighted:  0.27000975869456756\n",
      "Accuracy:  0.28955223880597014\n"
     ]
    }
   ],
   "source": [
    "pred= model.predict([X_test,X_features_test])\n",
    "pred= get_class_from_pred(pred)\n",
    "print(\"Macro: \",f1_score(valid_y,pred,average='macro'))\n",
    "print(\"Weighted: \",f1_score(valid_y,pred,average='weighted'))\n",
    "print(\"Accuracy: \" ,accuracy_score(valid_y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Features instead of Intermediate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag,input_shape_1):\n",
    "    input_sentence= Input(shape=(max_length,),name='Sentence')    \n",
    "    input_features = Input(shape=(input_shape_1,),name='Intermediate')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)(input_sentence)\n",
    "    \n",
    "    context=Dropout(0.5)(embedding)\n",
    "        \n",
    "    a = Bidirectional(LSTM(300, return_sequences=True,recurrent_dropout=dropout))(context)\n",
    "    \n",
    "    \n",
    "    a = Attention()(a)\n",
    "    \n",
    "    hidden = Dense(300,activation='relu')(input_features)\n",
    "    hidden = Dense(150,activation='relu') (hidden)\n",
    "    \n",
    "    \n",
    "    C= concatenate([a, hidden])\n",
    "    \n",
    "    x=Dense(300,activation='relu')(C)\n",
    "        \n",
    "    output=Dense(4,activation='softmax')(x)\n",
    "\n",
    "    model= Model(inputs=[input_sentence,input_features] ,outputs=output)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 600)\n"
     ]
    }
   ],
   "source": [
    "model = define_model(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix_word2vec',\n",
    "                     em_trainable_flag=False,\n",
    "                    input_shape_1=X_features_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1337 samples, validate on 335 samples\n",
      "Epoch 1/100\n",
      "1337/1337 [==============================] - 432s 323ms/step - loss: 1.2861 - acc: 0.3807 - val_loss: 1.1670 - val_acc: 0.4269\n",
      "Epoch 2/100\n",
      "1337/1337 [==============================] - 464s 347ms/step - loss: 0.9717 - acc: 0.5931 - val_loss: 1.1079 - val_acc: 0.5015\n",
      "Epoch 3/100\n",
      "1337/1337 [==============================] - 427s 320ms/step - loss: 0.5968 - acc: 0.7651 - val_loss: 1.3508 - val_acc: 0.5134\n",
      "Epoch 4/100\n",
      "1337/1337 [==============================] - 422s 315ms/step - loss: 0.2074 - acc: 0.9289 - val_loss: 1.7841 - val_acc: 0.4716\n",
      "Epoch 5/100\n",
      "1337/1337 [==============================] - 421s 315ms/step - loss: 0.0791 - acc: 0.9738 - val_loss: 2.3197 - val_acc: 0.4478\n",
      "Epoch 6/100\n",
      "1337/1337 [==============================] - 417s 312ms/step - loss: 0.0221 - acc: 0.9963 - val_loss: 2.5754 - val_acc: 0.4716\n",
      "Epoch 7/100\n",
      "1337/1337 [==============================] - 1187s 888ms/step - loss: 0.0069 - acc: 0.9985 - val_loss: 2.7098 - val_acc: 0.4448\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f435f4eb70>"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EarlyStop= EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n",
    "\n",
    "\n",
    "model.fit(x=[X_train,X_features_train],y=y_train, epochs=100,batch_size=32,\n",
    "              callbacks=[EarlyStop],validation_data=([X_test,X_features_test],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.30149253731343284\n",
      "Macro:  0.28424722680140635\n",
      "Weighted:  0.2908761690588778\n"
     ]
    }
   ],
   "source": [
    "pred= model.predict([X_test,X_features_test])\n",
    "pred= get_class_from_pred(pred)\n",
    "print(\"Accuracy: \" ,accuracy_score(valid_y,pred))\n",
    "print(\"Macro: \",f1_score(valid_y,pred,average='macro'))\n",
    "print(\"Weighted: \",f1_score(valid_y,pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with  Intermediate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag,input_shape_1):\n",
    "    input_sentence= Input(shape=(max_length,),name='Sentence')    \n",
    "    input_features = Input(shape=(input_shape_1,),name='Features')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag)(input_sentence)\n",
    "    \n",
    "    context=Dropout(0.5)(embedding)\n",
    "        \n",
    "    a = Bidirectional(LSTM(300, return_sequences=True,recurrent_dropout=dropout))(context)\n",
    "    \n",
    "    \n",
    "    a = Attention()(a)\n",
    "    \n",
    "    hidden = Dense(350,activation='relu')(input_features)\n",
    "    hidden = Dense(300,activation='relu') (hidden)\n",
    "    \n",
    "    \n",
    "    C= concatenate([a, hidden])\n",
    "    \n",
    "\n",
    "    x=Dense(300,activation='relu')(C)\n",
    "        \n",
    "    output=Dense(4,activation='softmax')(x)\n",
    "\n",
    "    model= Model(inputs=[input_sentence,input_features] ,outputs=output)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 600)\n"
     ]
    }
   ],
   "source": [
    "model = define_model(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix_glove',\n",
    "                     em_trainable_flag=False,\n",
    "                    input_shape_1=X_features_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1337 samples, validate on 335 samples\n",
      "Epoch 1/100\n",
      "1337/1337 [==============================] - 76s 56ms/step - loss: 1.3401 - acc: 0.3328 - val_loss: 1.3260 - val_acc: 0.3284\n",
      "Epoch 2/100\n",
      "1337/1337 [==============================] - 68s 51ms/step - loss: 1.0978 - acc: 0.4959 - val_loss: 1.1619 - val_acc: 0.4358\n",
      "Epoch 3/100\n",
      "1337/1337 [==============================] - 69s 52ms/step - loss: 0.5809 - acc: 0.7846 - val_loss: 1.5159 - val_acc: 0.4269\n",
      "Epoch 4/100\n",
      "1337/1337 [==============================] - 77s 57ms/step - loss: 0.1681 - acc: 0.9499 - val_loss: 2.2367 - val_acc: 0.3791\n",
      "Epoch 5/100\n",
      "1337/1337 [==============================] - 70s 52ms/step - loss: 0.0383 - acc: 0.9880 - val_loss: 2.6788 - val_acc: 0.3910\n",
      "Epoch 6/100\n",
      "1337/1337 [==============================] - 69s 52ms/step - loss: 0.0111 - acc: 0.9978 - val_loss: 2.8969 - val_acc: 0.3851\n",
      "Epoch 7/100\n",
      "1337/1337 [==============================] - 69s 52ms/step - loss: 0.0138 - acc: 0.9963 - val_loss: 3.3418 - val_acc: 0.3761\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feee8d92390>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EarlyStop= EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n",
    "\n",
    "\n",
    "model.fit(x=[X_train,X_features_train],y=y_train, epochs=100,batch_size=32,\n",
    "              callbacks=[EarlyStop],validation_data=([X_test,X_features_test],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'intermediate_X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-165-e3d20dc8ea32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mintermediate_X_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mget_class_from_pred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: \"\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Macro: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Weighted: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'intermediate_X_test' is not defined"
     ]
    }
   ],
   "source": [
    "pred= model.predict([X_test,intermediate_X_test])\n",
    "pred= get_class_from_pred(pred)\n",
    "print(\"Accuracy: \" ,accuracy_score(valid_y,pred))\n",
    "print(\"Macro: \",f1_score(valid_y,pred,average='macro'))\n",
    "print(\"Weighted: \",f1_score(valid_y,pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_metrics as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47973560133595317\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([X_test,X_features_test])\n",
    "out= get_class_from_pred(pred)\n",
    "actual= get_class_from_pred(y_test)\n",
    "print(m.quadratic_weighted_kappa(actual,out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense 50n 0-300\n",
    "Accuracy:  0.2716417910447761\n",
    "Macro:  0.2672652222329571\n",
    "Weighted:  0.2725898365250229"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dense 350-300\n",
    "Accuracy:  0.27761194029850744\n",
    "Macro:  0.2722311105266747\n",
    "Weighted:  0.2773415875533062"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dense 250-200\n",
    "Accuracy:  0.2716417910447761\n",
    "Macro:  0.27053123858918887\n",
    "Weighted:  0.2729120423705027"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dense 150-100\n",
    "Accuracy:  0.25970149253731345\n",
    "Macro:  0.25798130203500763\n",
    "Weighted:  0.25956035382275994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate_layer_model = Model(inputs=model.input,\n",
    "#                                 outputs=model.get_layer(\"Feature_Layer\").output)\n",
    "#intermediate_output = intermediate_layer_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02297647, 0.        , 0.        , ..., 0.        , 0.19092862,\n",
       "        0.        ],\n",
       "       [0.07279535, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.06108753, 0.        , 0.        , ..., 0.        , 0.10995848,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.42304373, 0.32567853, ..., 0.55879897, 0.        ,\n",
       "        0.44233015],\n",
       "       [0.        , 0.24653786, 0.2675489 , ..., 0.42380807, 0.        ,\n",
       "        0.32747212],\n",
       "       [0.466357  , 0.        , 0.        , ..., 0.        , 0.3919743 ,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#intermediate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate_X_train = intermediate_output\n",
    "#intermediate_X_test= intermediate_layer_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(intermediate_X_train, open(\"intermediate_X_train.dat\",\"wb\"))\n",
    "#pickle.dump(intermediate_X_test, open(\"intermediate_X_test.dat\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 67.76%\n",
      "model1\n",
      "test\n",
      "67.76% (+/- 0.00%)\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "print(\"model%d\" %1)\n",
    "cvscores.append(score[1] * 100)\n",
    "print(\"test\")\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(intermediate_output,open(\"Intermediate_Layer.dat\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features TFIDF and Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing , metrics\n",
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(data[['clean', 'CosineSim_2','error_words', 'total_errors',]], data['Score1'],test_size = 0.2)\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "\n",
    "tfidf_vect_ngram.fit_transform(data['clean'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x['clean'])\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=None)\n",
    "tfidf_vect.fit(data['clean'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x['clean'])\n",
    "xtest_tfidf =  tfidf_vect.transform(valid_x['clean'])\n",
    "\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', ngram_range=(1,3), max_features=None)\n",
    "tfidf_vect_ngram_chars.fit(data['clean'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x['clean']) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x['clean']) \n",
    "\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word')\n",
    "count_vect.fit(data['clean'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x['clean'])\n",
    "xtest_count =  count_vect.transform(valid_x['clean'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "X_features_train= sparse.hstack([ xtrain_tfidf_ngram,xtrain_tfidf,xtrain_tfidf_ngram_chars,xtrain_count,sparse.csr_matrix(train_x['total_errors']).T],'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_test = sparse.hstack([ xvalid_tfidf_ngram,xtest_tfidf,xtest_tfidf_ngram_chars,xtest_count,sparse.csr_matrix(valid_x['total_errors']).T],'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels (trainY):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(trainY)\n",
    "    temp1 = le.transform(trainY)\n",
    "    return to_categorical(temp1,4), le.classes_, trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,lable_encoding,_=convert_labels(data['Score1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_pred(pred):\n",
    "    return [lable_encoding[x.argmax()] for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "pred= get_class_from_pred(pred)\n",
    "print(\"Accuracy: \" ,accuracy_score(valid_y,pred))\n",
    "print(\"Macro: \",f1_score(valid_y,pred,average='macro'))\n",
    "print(\"Weighted: \",f1_score(valid_y,pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output when  SENTENCE + INTERMEDIATE Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output when FEATURES + SENTENCE + INTERMEDIATE Layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accuracy:  0.20298507462686566\n",
    "Macro:  0.08436724565756824\n",
    "Weighted:  0.06850116662345838"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF:  0.4746268656716418\n"
     ]
    }
   ],
   "source": [
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), X_features_train, train_y, X_features_test)\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4626865671641791"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(xtrain_count, train_y)\n",
    "accuracy_score(valid_y, model_rf.predict(xtest_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.4626865671641791"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC # \"Support Vector Classifier\" \n",
    "clf = SVC(kernel='linear') \n",
    "  \n",
    "# fitting x samples and y classes \n",
    "clf.fit(intermediate_, train_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5791044776119403"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(X_features_test) \n",
    "accuracy_score(valid_y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5791044776119403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "      <th>CosineSim</th>\n",
       "      <th>CosineSim_2</th>\n",
       "      <th>error_words</th>\n",
       "      <th>total_errors</th>\n",
       "      <th>length_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>the additional information that i would need i...</td>\n",
       "      <td>0.406042</td>\n",
       "      <td>0.919919</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.806452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>in order to replicate this experiment thorough...</td>\n",
       "      <td>0.874444</td>\n",
       "      <td>0.900320</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.032258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>one thing you would need to know to replicate ...</td>\n",
       "      <td>0.819692</td>\n",
       "      <td>0.924329</td>\n",
       "      <td>[wouldneed, toknowis]</td>\n",
       "      <td>2</td>\n",
       "      <td>1.193548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>in order to replicate the experiment the infor...</td>\n",
       "      <td>0.867489</td>\n",
       "      <td>0.889303</td>\n",
       "      <td>[refreglatar, reave]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.741935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>some additional information that we would need...</td>\n",
       "      <td>0.918307</td>\n",
       "      <td>0.915313</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.903226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  clean  CosineSim  \\\n",
       "981   the additional information that i would need i...   0.406042   \n",
       "1385  in order to replicate this experiment thorough...   0.874444   \n",
       "1179  one thing you would need to know to replicate ...   0.819692   \n",
       "638   in order to replicate the experiment the infor...   0.867489   \n",
       "1635  some additional information that we would need...   0.918307   \n",
       "\n",
       "      CosineSim_2            error_words  total_errors  length_ratio  \n",
       "981      0.919919                     []             0      0.806452  \n",
       "1385     0.900320                     []             0      1.032258  \n",
       "1179     0.924329  [wouldneed, toknowis]             2      1.193548  \n",
       "638      0.889303   [refreglatar, reave]             2      0.741935  \n",
       "1635     0.915313                     []             0      0.903226  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your grade is ----> 2\n",
      "You have recieved this score because :\n",
      "Your answer is similar to the reference answer\n",
      "Your total error count is  2\n",
      "Your spelling errors are :\n",
      "wouldneed\n",
      "toknowis\n",
      "Your answer is as long enough\n"
     ]
    }
   ],
   "source": [
    "print(\"Your grade is ---->\", clf.predict(X_features_test[2])[0]) \n",
    "print(\"You have recieved this score because :\")\n",
    "\n",
    "if (float(valid_x[\"CosineSim\"][1179])>=0.5):\n",
    "    print(\"Your answer is similar to the reference answer\")\n",
    "else :\n",
    "    print(\"Your answer is not similar to the reference answer\")\n",
    "\n",
    "print(\"Your total error count is \", valid_x['total_errors'][1179] )\n",
    "print(\"Your spelling errors are :\", )\n",
    "for i in valid_x['error_words'][1179]:\n",
    "    print(i,)\n",
    "    \n",
    "if (float(valid_x[\"length_ratio\"][1179])>=0.5):\n",
    "    print(\"Your answer is as long enough\")\n",
    "else :\n",
    "    print(\"Your answer is very short. Try writing more next time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "for i in a:\n",
    "    \n",
    "    if i==True:\n",
    "        print(count)\n",
    "        break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QWK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_test, pred, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    quadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(y_test, pred), where y_test and pred\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    y_test = np.array(y_test, dtype=int)\n",
    "    pred = np.array(pred, dtype=int)\n",
    "    assert(len(y_test) == len(pred))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(y_test), min(pred))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(y_test), max(pred))\n",
    "    conf_mat = confusion_matrix(y_test, pred,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(y_test))\n",
    "\n",
    "    hist_y_test = histogram(y_test, min_rating, max_rating)\n",
    "    hist_pred = histogram(pred, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_y_test[i] * hist_pred[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCE EMBEDDING Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling1D\n",
    "import ml_metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag):\n",
    "    input_sentence= Input(shape=(max_length,),name='Sentence')    \n",
    "    input_reference = Input(shape=(max_length,),name='Reference')\n",
    "    \n",
    "    embedding_1=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = False)\n",
    "    embedding_2=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = True)\n",
    "\n",
    "    context = embedding_1(input_sentence)\n",
    "    reference = embedding_2(input_reference)\n",
    "    \n",
    "    combined= concatenate([context, reference])\n",
    "    combined=Dropout(0.5)(combined)\n",
    "     \n",
    "    hidden,_,_,_,_ = Bidirectional(LSTM(300, return_sequences=True, return_state = True, dropout=0.25, recurrent_dropout=0.1))(combined)\n",
    "    #hidden = GlobalAveragePooling1D()(hidden)\n",
    "    \n",
    "    post_lstm_combine= concatenate([hidden, reference])\n",
    "    post_lstm_combine=Dropout(0.5)(post_lstm_combine)\n",
    "\n",
    "    a = Attention()(post_lstm_combine)\n",
    "    \n",
    "    x=Dense(300,activation='relu')(a)\n",
    "        \n",
    "    output=Dense(4,activation='softmax')(x)\n",
    "\n",
    "    model= Model(inputs=[input_sentence,input_reference] ,outputs=output)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 800)\n"
     ]
    }
   ],
   "source": [
    "model = define_model(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix_glove',\n",
    "                     em_trainable_flag=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.asarray(y_test, dtype=int)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1337 samples, validate on 335 samples\n",
      "Epoch 1/100\n",
      "1337/1337 [==============================] - 74s 55ms/step - loss: 1.2680 - acc: 0.3844 - val_loss: 1.1863 - val_acc: 0.4149\n",
      "Epoch 2/100\n",
      "1337/1337 [==============================] - 76s 57ms/step - loss: 1.1625 - acc: 0.4592 - val_loss: 1.2212 - val_acc: 0.4597\n",
      "Epoch 3/100\n",
      "1337/1337 [==============================] - 74s 55ms/step - loss: 1.1334 - acc: 0.4705 - val_loss: 1.0430 - val_acc: 0.5224\n",
      "Epoch 4/100\n",
      "1337/1337 [==============================] - 73s 55ms/step - loss: 1.0864 - acc: 0.4929 - val_loss: 1.0824 - val_acc: 0.5075\n",
      "Epoch 5/100\n",
      "1337/1337 [==============================] - 74s 55ms/step - loss: 1.0129 - acc: 0.5423 - val_loss: 1.0096 - val_acc: 0.5224\n",
      "Epoch 6/100\n",
      "1337/1337 [==============================] - 76s 57ms/step - loss: 0.9996 - acc: 0.5393 - val_loss: 1.0257 - val_acc: 0.5373\n",
      "Epoch 7/100\n",
      "1337/1337 [==============================] - 73s 55ms/step - loss: 0.9799 - acc: 0.5408 - val_loss: 0.9510 - val_acc: 0.5761\n",
      "Epoch 8/100\n",
      "1337/1337 [==============================] - 74s 55ms/step - loss: 0.9602 - acc: 0.5647 - val_loss: 0.9624 - val_acc: 0.5463\n",
      "Epoch 9/100\n",
      "1337/1337 [==============================] - 74s 55ms/step - loss: 0.9130 - acc: 0.5924 - val_loss: 0.9263 - val_acc: 0.6030\n",
      "Epoch 10/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 0.8965 - acc: 0.5774 - val_loss: 0.9100 - val_acc: 0.5731\n",
      "Epoch 11/100\n",
      "1337/1337 [==============================] - 74s 55ms/step - loss: 0.8654 - acc: 0.6073 - val_loss: 0.9398 - val_acc: 0.5821\n",
      "Epoch 12/100\n",
      "1337/1337 [==============================] - 73s 55ms/step - loss: 0.8940 - acc: 0.5826 - val_loss: 0.9556 - val_acc: 0.5970\n",
      "Epoch 13/100\n",
      "1337/1337 [==============================] - 74s 55ms/step - loss: 0.8415 - acc: 0.6328 - val_loss: 0.8958 - val_acc: 0.6060\n",
      "Epoch 14/100\n",
      "1337/1337 [==============================] - 74s 55ms/step - loss: 0.8155 - acc: 0.6335 - val_loss: 0.9575 - val_acc: 0.5970\n",
      "Epoch 15/100\n",
      "1337/1337 [==============================] - 73s 55ms/step - loss: 0.7970 - acc: 0.6485 - val_loss: 0.9468 - val_acc: 0.5970\n",
      "Epoch 16/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 0.7739 - acc: 0.6731 - val_loss: 0.8736 - val_acc: 0.6269\n",
      "Epoch 17/100\n",
      "1337/1337 [==============================] - 73s 55ms/step - loss: 0.7624 - acc: 0.6627 - val_loss: 0.8446 - val_acc: 0.6687\n",
      "Epoch 18/100\n",
      "1337/1337 [==============================] - 74s 55ms/step - loss: 0.7234 - acc: 0.6956 - val_loss: 0.8424 - val_acc: 0.6388\n",
      "Epoch 19/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 0.6916 - acc: 0.6911 - val_loss: 0.8855 - val_acc: 0.6388\n",
      "Epoch 20/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 0.7112 - acc: 0.6859 - val_loss: 0.8433 - val_acc: 0.6478\n",
      "Epoch 21/100\n",
      "1337/1337 [==============================] - 77s 57ms/step - loss: 0.7037 - acc: 0.6739 - val_loss: 0.8264 - val_acc: 0.6806\n",
      "Epoch 22/100\n",
      "1337/1337 [==============================] - 74s 56ms/step - loss: 0.6226 - acc: 0.7427 - val_loss: 0.9328 - val_acc: 0.6328\n",
      "Epoch 23/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 0.6291 - acc: 0.7315 - val_loss: 0.8326 - val_acc: 0.6567\n",
      "Epoch 24/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 0.6046 - acc: 0.7300 - val_loss: 0.8665 - val_acc: 0.6567\n",
      "Epoch 25/100\n",
      "1337/1337 [==============================] - 74s 56ms/step - loss: 0.6250 - acc: 0.7270 - val_loss: 0.8251 - val_acc: 0.6567\n",
      "Epoch 26/100\n",
      "1337/1337 [==============================] - 77s 57ms/step - loss: 0.5446 - acc: 0.7584 - val_loss: 0.9486 - val_acc: 0.6597\n",
      "Epoch 00026: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef0b4cbf98>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EarlyStop= EarlyStopping(monitor='val_acc',patience=5,verbose=1)\n",
    "\n",
    "model.fit(x=[X_train,X_ref_train],y=y_train, epochs=100,batch_size=64,\n",
    "              callbacks=[EarlyStop],validation_data=([X_test,X_ref_test],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([X_test,X_ref_test])\n",
    "out= get_class_from_pred(pred)\n",
    "actual= get_class_from_pred(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8001041228649807\n"
     ]
    }
   ],
   "source": [
    "print(metrics.quadratic_weighted_kappa(actual,out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ref_train = X_ref[:1337]\n",
    "X_ref_test= X_ref[:335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_ref' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-271afc8f19f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_ref' is not defined"
     ]
    }
   ],
   "source": [
    "X_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1337, 133), (335, 133))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels (trainY):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(trainY)\n",
    "    temp1 = le.transform(trainY)\n",
    "    return to_categorical(temp1,4), le.classes_, trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,lable_encoding,_=convert_labels(data['Score1'])\n",
    "lable_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_pred(pred):\n",
    "    return [lable_encoding[x.argmax()] for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-30858ee82732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mget_class_from_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_class_from_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "out= get_class_from_pred(pred)\n",
    "actual = get_class_from_pred(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-64b45381e536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6567164179104478\n",
      "Macro:  0.6665492481519748\n",
      "Weighted:  0.6589621765811206\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" ,accuracy_score(actual,out))\n",
    "print(\"Macro: \",f1_score(actual,out,average='macro'))\n",
    "print(\"Weighted: \",f1_score(actual,out,average='weighted'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Reference Embedding + Student Embedding\n",
    "\n",
    "Accuracy:  0.6567164179104478\n",
    "Macro:  0.6665492481519748\n",
    "Weighted:  0.6589621765811206"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag):\n",
    "    input_sentence= Input(shape=(max_length,),name='Sentence')    \n",
    "    input_reference = Input(shape=(max_length,),name='Reference')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = False)\n",
    "\n",
    "    context = embedding(input_sentence)\n",
    "    reference = embedding(input_reference)\n",
    "    \n",
    "   \n",
    "    hidden,_,_,_,_ = Bidirectional(LSTM(300, return_sequences=True, return_state = True, dropout=0.25, recurrent_dropout=0.1))(context)\n",
    "    \n",
    "    post_lstm_combine= concatenate([hidden, reference])\n",
    "    post_lstm_combine=Dropout(0.5)(post_lstm_combine)\n",
    "    \n",
    "\n",
    "    a = Attention()(post_lstm_combine)\n",
    "    #a = AveragePooling1D(a)\n",
    "\n",
    "    x=Dense(300,activation='relu')(a)\n",
    "        \n",
    "    output=Dense(4,activation='softmax')(x)\n",
    "\n",
    "    model= Model(inputs=[input_sentence,input_reference] ,outputs=output)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 800)\n"
     ]
    }
   ],
   "source": [
    "model = define_model(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix_glove',\n",
    "                     em_trainable_flag=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1337 samples, validate on 335 samples\n",
      "Epoch 1/100\n",
      "1337/1337 [==============================] - 71s 53ms/step - loss: 1.3333 - acc: 0.3463 - val_loss: 1.2983 - val_acc: 0.3791\n",
      "Epoch 2/100\n",
      "1337/1337 [==============================] - 74s 55ms/step - loss: 1.2736 - acc: 0.3964 - val_loss: 1.1783 - val_acc: 0.4537\n",
      "Epoch 3/100\n",
      "1337/1337 [==============================] - 68s 51ms/step - loss: 1.0960 - acc: 0.4772 - val_loss: 1.0102 - val_acc: 0.5134\n",
      "Epoch 4/100\n",
      "1337/1337 [==============================] - 65s 49ms/step - loss: 0.9793 - acc: 0.5385 - val_loss: 0.9975 - val_acc: 0.5552\n",
      "Epoch 5/100\n",
      "1337/1337 [==============================] - 62s 47ms/step - loss: 0.9233 - acc: 0.5639 - val_loss: 0.9388 - val_acc: 0.5522\n",
      "Epoch 6/100\n",
      "1337/1337 [==============================] - 70s 52ms/step - loss: 0.8555 - acc: 0.6013 - val_loss: 0.8579 - val_acc: 0.6328\n",
      "Epoch 7/100\n",
      "1337/1337 [==============================] - 65s 49ms/step - loss: 0.8313 - acc: 0.6200 - val_loss: 0.8631 - val_acc: 0.6090\n",
      "Epoch 8/100\n",
      "1337/1337 [==============================] - 67s 50ms/step - loss: 0.8157 - acc: 0.6462 - val_loss: 0.8919 - val_acc: 0.6239\n",
      "Epoch 9/100\n",
      "1337/1337 [==============================] - 64s 48ms/step - loss: 0.7461 - acc: 0.6776 - val_loss: 0.8943 - val_acc: 0.6060\n",
      "Epoch 10/100\n",
      "1337/1337 [==============================] - 64s 48ms/step - loss: 0.7166 - acc: 0.6963 - val_loss: 0.8294 - val_acc: 0.6627\n",
      "Epoch 11/100\n",
      "1337/1337 [==============================] - 64s 48ms/step - loss: 0.7087 - acc: 0.6844 - val_loss: 0.8831 - val_acc: 0.6179\n",
      "Epoch 12/100\n",
      "1337/1337 [==============================] - 63s 47ms/step - loss: 0.6549 - acc: 0.7165 - val_loss: 0.8036 - val_acc: 0.6597\n",
      "Epoch 13/100\n",
      "1337/1337 [==============================] - 65s 48ms/step - loss: 0.6012 - acc: 0.7532 - val_loss: 0.8604 - val_acc: 0.6657\n",
      "Epoch 14/100\n",
      "1337/1337 [==============================] - 65s 49ms/step - loss: 0.5865 - acc: 0.7614 - val_loss: 1.0365 - val_acc: 0.6269\n",
      "Epoch 15/100\n",
      "1337/1337 [==============================] - 64s 48ms/step - loss: 0.5090 - acc: 0.8003 - val_loss: 0.8613 - val_acc: 0.6896\n",
      "Epoch 16/100\n",
      "1337/1337 [==============================] - 64s 48ms/step - loss: 0.4632 - acc: 0.8153 - val_loss: 0.8995 - val_acc: 0.6866\n",
      "Epoch 17/100\n",
      "1337/1337 [==============================] - 64s 48ms/step - loss: 0.4446 - acc: 0.8205 - val_loss: 0.9730 - val_acc: 0.6657\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef0f0e6dd8>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EarlyStop= EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n",
    "\n",
    "\n",
    "model.fit(x=[X_train,X_ref_train],y=y_train, epochs=100,batch_size=32,\n",
    "              callbacks=[EarlyStop],validation_data=([X_test,X_ref_test],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7802077638053582\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([X_test,X_ref_test])\n",
    "out= get_class_from_pred(pred)\n",
    "actual= get_class_from_pred(y_test)\n",
    "\n",
    "print(metrics.quadratic_weighted_kappa(actual,out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([X_test,X_ref_train])\n",
    "out= get_class_from_pred(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6567164179104478\n",
      "Macro:  0.6678536352208216\n",
      "Weighted:  0.6606546859613557\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" ,accuracy_score(actual,out))\n",
    "print(\"Macro: \",f1_score(actual,out,average='macro'))\n",
    "print(\"Weighted: \",f1_score(actual,out,average='weighted'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accuracy:  0.6567164179104478\n",
    "Macro:  0.6678536352208216\n",
    "Weighted:  0.6606546859613557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model attempt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag):\n",
    "    input_sentence= Input(shape=(max_length,),name='Sentence')    \n",
    "    input_reference = Input(shape=(max_length,),name='Reference')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = False)\n",
    "\n",
    "    context = embedding(input_sentence)\n",
    "    reference = embedding(input_reference)\n",
    "    \n",
    "    combined= concatenate([context, reference])\n",
    "    combined=Dropout(0.5)(combined)\n",
    "   \n",
    "    hidden,_,_,_,_ = Bidirectional(LSTM(300, return_sequences=True, return_state = True, dropout=0.25, recurrent_dropout=0.1))(combined)\n",
    "    \n",
    "    a = Attention()(hidden)\n",
    "    #a = AveragePooling1D(a)\n",
    "\n",
    "    x=Dense(300,activation='relu')(a)\n",
    "        \n",
    "    output=Dense(4,activation='softmax')(x)\n",
    "\n",
    "    model= Model(inputs=[input_sentence,input_reference] ,outputs=output)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 600)\n"
     ]
    }
   ],
   "source": [
    "model = define_model(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix_glove',\n",
    "                     em_trainable_flag=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1337 samples, validate on 335 samples\n",
      "Epoch 1/100\n",
      "1337/1337 [==============================] - 82s 61ms/step - loss: 1.3357 - acc: 0.3246 - val_loss: 1.4418 - val_acc: 0.3224\n",
      "Epoch 2/100\n",
      "1337/1337 [==============================] - 77s 57ms/step - loss: 1.2772 - acc: 0.4046 - val_loss: 1.2289 - val_acc: 0.3761\n",
      "Epoch 3/100\n",
      "1337/1337 [==============================] - 77s 57ms/step - loss: 1.1176 - acc: 0.4742 - val_loss: 1.0393 - val_acc: 0.5194\n",
      "Epoch 4/100\n",
      "1337/1337 [==============================] - 76s 57ms/step - loss: 1.0607 - acc: 0.5004 - val_loss: 1.0127 - val_acc: 0.5582\n",
      "Epoch 5/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 1.0152 - acc: 0.5288 - val_loss: 0.9478 - val_acc: 0.5612\n",
      "Epoch 6/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 0.9976 - acc: 0.5378 - val_loss: 0.9689 - val_acc: 0.5552\n",
      "Epoch 7/100\n",
      "1337/1337 [==============================] - 76s 57ms/step - loss: 0.9433 - acc: 0.5669 - val_loss: 0.9495 - val_acc: 0.5493\n",
      "Epoch 8/100\n",
      "1337/1337 [==============================] - 76s 57ms/step - loss: 0.8989 - acc: 0.6013 - val_loss: 0.8927 - val_acc: 0.6060\n",
      "Epoch 9/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 0.8727 - acc: 0.6193 - val_loss: 0.9772 - val_acc: 0.5791\n",
      "Epoch 10/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 0.8460 - acc: 0.6111 - val_loss: 0.8607 - val_acc: 0.6269\n",
      "Epoch 11/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 0.8140 - acc: 0.6350 - val_loss: 0.9170 - val_acc: 0.6060\n",
      "Epoch 12/100\n",
      "1337/1337 [==============================] - 76s 57ms/step - loss: 0.7877 - acc: 0.6402 - val_loss: 0.8135 - val_acc: 0.6687\n",
      "Epoch 13/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 0.7782 - acc: 0.6500 - val_loss: 0.8715 - val_acc: 0.6269\n",
      "Epoch 14/100\n",
      "1337/1337 [==============================] - 74s 55ms/step - loss: 0.7527 - acc: 0.6627 - val_loss: 0.8590 - val_acc: 0.6507\n",
      "Epoch 15/100\n",
      "1337/1337 [==============================] - 77s 58ms/step - loss: 0.6793 - acc: 0.7292 - val_loss: 0.8318 - val_acc: 0.6836\n",
      "Epoch 16/100\n",
      "1337/1337 [==============================] - 81s 60ms/step - loss: 0.6725 - acc: 0.7135 - val_loss: 0.8130 - val_acc: 0.6657\n",
      "Epoch 17/100\n",
      "1337/1337 [==============================] - 91s 68ms/step - loss: 0.6271 - acc: 0.7367 - val_loss: 0.8362 - val_acc: 0.6687\n",
      "Epoch 18/100\n",
      "1337/1337 [==============================] - 83s 62ms/step - loss: 0.6221 - acc: 0.7450 - val_loss: 0.7751 - val_acc: 0.6925\n",
      "Epoch 19/100\n",
      "1337/1337 [==============================] - 87s 65ms/step - loss: 0.5697 - acc: 0.7614 - val_loss: 0.8429 - val_acc: 0.6806\n",
      "Epoch 20/100\n",
      "1337/1337 [==============================] - 89s 67ms/step - loss: 0.5276 - acc: 0.7809 - val_loss: 1.0547 - val_acc: 0.6030\n",
      "Epoch 21/100\n",
      "1337/1337 [==============================] - 82s 62ms/step - loss: 0.5060 - acc: 0.7996 - val_loss: 0.9344 - val_acc: 0.6866\n",
      "Epoch 22/100\n",
      "1337/1337 [==============================] - 76s 57ms/step - loss: 0.4741 - acc: 0.8212 - val_loss: 0.9755 - val_acc: 0.6537\n",
      "Epoch 23/100\n",
      "1337/1337 [==============================] - 85s 63ms/step - loss: 0.4719 - acc: 0.8100 - val_loss: 0.9117 - val_acc: 0.6716\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef0d7c6ac8>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EarlyStop= EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n",
    "\n",
    "\n",
    "model.fit(x=[X_train,X_ref_train],y=y_train, epochs=100,batch_size=32,\n",
    "              callbacks=[EarlyStop],validation_data=([X_test,X_ref_test],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808156632100294\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([X_test,X_ref_test])\n",
    "out= get_class_from_pred(pred)\n",
    "actual= get_class_from_pred(y_test)\n",
    "\n",
    "print(metrics.quadratic_weighted_kappa(actual,out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([X_test,X_ref_train])\n",
    "out= get_class_from_pred(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6238805970149254\n",
      "Macro:  0.6243776980921877\n",
      "Weighted:  0.617904553036701\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" ,accuracy_score(actual,out))\n",
    "print(\"Macro: \",f1_score(actual,out,average='macro'))\n",
    "print(\"Weighted: \",f1_score(actual,out,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:  0.6238805970149254\n",
    "Macro:  0.6243776980921877\n",
    "Weighted:  0.617904553036701\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag):\n",
    "    input_sentence= Input(shape=(max_length,),name='Sentence')    \n",
    "    input_reference = Input(shape=(max_length,),name='Reference')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = False)\n",
    "\n",
    "    context = embedding(input_sentence)\n",
    "    reference = embedding(input_reference)\n",
    "    \n",
    "    combined= concatenate([context, reference])\n",
    "    combined=Dropout(0.5)(combined)\n",
    "    c = Conv1D(150,5,activation='relu')(combined)\n",
    "    \n",
    "    hidden,_,_,_,_ = Bidirectional(LSTM(300, return_sequences=True, return_state = True, dropout=0.25, recurrent_dropout=0.1))(c)\n",
    "    \n",
    "    a = Attention()(hidden)\n",
    "    #a = AveragePooling1D(a)\n",
    "\n",
    "    x=Dense(300,activation='relu')(a)\n",
    "        \n",
    "    output=Dense(4,activation='softmax')(x)\n",
    "\n",
    "    model= Model(inputs=[input_sentence,input_reference] ,outputs=output)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aruhi/tensorflow/venv/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, activation=\"relu\", kernel_regularizer=<keras.reg..., bias_regularizer=<keras.reg...)`\n"
     ]
    }
   ],
   "source": [
    "model = define_model(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix_glove',\n",
    "                     em_trainable_flag=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1337 samples, validate on 335 samples\n",
      "Epoch 1/100\n",
      "1337/1337 [==============================] - 81s 61ms/step - loss: 3.4895 - acc: 0.3605 - val_loss: 2.1707 - val_acc: 0.3672\n",
      "Epoch 2/100\n",
      "1337/1337 [==============================] - 73s 55ms/step - loss: 1.6747 - acc: 0.4218 - val_loss: 1.3475 - val_acc: 0.4687\n",
      "Epoch 3/100\n",
      "1337/1337 [==============================] - 77s 58ms/step - loss: 1.2454 - acc: 0.4817 - val_loss: 1.1448 - val_acc: 0.4925\n",
      "Epoch 4/100\n",
      "1337/1337 [==============================] - 77s 58ms/step - loss: 1.1610 - acc: 0.4764 - val_loss: 1.0661 - val_acc: 0.4866\n",
      "Epoch 5/100\n",
      "1337/1337 [==============================] - 76s 57ms/step - loss: 1.0886 - acc: 0.5034 - val_loss: 1.0867 - val_acc: 0.4776\n",
      "Epoch 6/100\n",
      "1337/1337 [==============================] - 76s 57ms/step - loss: 1.0517 - acc: 0.5108 - val_loss: 1.0262 - val_acc: 0.4985\n",
      "Epoch 7/100\n",
      "1337/1337 [==============================] - 72s 54ms/step - loss: 1.0144 - acc: 0.5602 - val_loss: 1.0005 - val_acc: 0.5015\n",
      "Epoch 8/100\n",
      "1337/1337 [==============================] - 77s 57ms/step - loss: 1.0063 - acc: 0.5415 - val_loss: 0.9632 - val_acc: 0.5134\n",
      "Epoch 9/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 0.9794 - acc: 0.5415 - val_loss: 0.9697 - val_acc: 0.5284\n",
      "Epoch 10/100\n",
      "1337/1337 [==============================] - 76s 57ms/step - loss: 0.9860 - acc: 0.5632 - val_loss: 0.9933 - val_acc: 0.5463\n",
      "Epoch 11/100\n",
      "1337/1337 [==============================] - 78s 58ms/step - loss: 0.9614 - acc: 0.5752 - val_loss: 0.8938 - val_acc: 0.5612\n",
      "Epoch 12/100\n",
      "1337/1337 [==============================] - 79s 59ms/step - loss: 0.9325 - acc: 0.5826 - val_loss: 0.8551 - val_acc: 0.6478\n",
      "Epoch 13/100\n",
      "1337/1337 [==============================] - 76s 57ms/step - loss: 0.8910 - acc: 0.6260 - val_loss: 0.9176 - val_acc: 0.5761\n",
      "Epoch 14/100\n",
      "1337/1337 [==============================] - 73s 55ms/step - loss: 0.8773 - acc: 0.6365 - val_loss: 0.8981 - val_acc: 0.6269\n",
      "Epoch 15/100\n",
      "1337/1337 [==============================] - 76s 57ms/step - loss: 0.8524 - acc: 0.6507 - val_loss: 0.8313 - val_acc: 0.6597\n",
      "Epoch 16/100\n",
      "1337/1337 [==============================] - 77s 57ms/step - loss: 0.8197 - acc: 0.6672 - val_loss: 0.7892 - val_acc: 0.6597\n",
      "Epoch 17/100\n",
      "1337/1337 [==============================] - 78s 58ms/step - loss: 0.7862 - acc: 0.6836 - val_loss: 0.8331 - val_acc: 0.6209\n",
      "Epoch 18/100\n",
      "1337/1337 [==============================] - 75s 56ms/step - loss: 0.8185 - acc: 0.6485 - val_loss: 0.8600 - val_acc: 0.6567\n",
      "Epoch 19/100\n",
      "1337/1337 [==============================] - 76s 57ms/step - loss: 0.7527 - acc: 0.7031 - val_loss: 0.7908 - val_acc: 0.6896\n",
      "Epoch 20/100\n",
      "1337/1337 [==============================] - 77s 57ms/step - loss: 0.7378 - acc: 0.6911 - val_loss: 0.7969 - val_acc: 0.6925\n",
      "Epoch 21/100\n",
      "1337/1337 [==============================] - 77s 58ms/step - loss: 0.7714 - acc: 0.6821 - val_loss: 0.8739 - val_acc: 0.6239\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea85b4b198>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EarlyStop= EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n",
    "\n",
    "\n",
    "model.fit(x=[X_train,X_ref_train],y=y_train, epochs=100,batch_size=32,\n",
    "              callbacks=[EarlyStop],validation_data=([X_test,X_ref_test],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7694786812421062\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([X_test,X_ref_test])\n",
    "out= get_class_from_pred(pred)\n",
    "actual= get_class_from_pred(y_test)\n",
    "\n",
    "print(m.quadratic_weighted_kappa(actual,out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([X_test,X_ref_train])\n",
    "out= get_class_from_pred(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" ,accuracy_score(actual,out))\n",
    "print(\"Macro: \",f1_score(actual,out,average='macro'))\n",
    "print(\"Weighted: \",f1_score(actual,out,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag):\n",
    "    input_sentence= Input(shape=(max_length,),name='Sentence')    \n",
    "#    input_reference = Input(shape=(max_length,),name='Reference')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = False)\n",
    "\n",
    "    context = embedding(input_sentence)\n",
    "  #  reference = embedding(input_reference)\n",
    "    \n",
    " #   combined= concatenate([context, reference])\n",
    "    combined=Dropout(0.5)(context)\n",
    "    c = Conv1D(50,5,activation='relu')(combined)\n",
    "    \n",
    "    hidden,_,_ = LSTM(300, return_sequences=True, return_state = True, dropout=0.25, recurrent_dropout=0.1)(c)\n",
    "    \n",
    "    a = Attention()(hidden)    \n",
    " #   a = GlobalAveragePooling1D(hidden)\n",
    "\n",
    "#    x=Dense(300,activation='relu')(a)\n",
    "        \n",
    "    output=Dense(4,activation='linear')(a)\n",
    "\n",
    "    model= Model(inputs=input_sentence ,outputs=output)   #(inputs=[input_sentence,input_reference]\n",
    "    \n",
    "    optimizer = RMSprop(lr=learning_rate , rho = 0.9 , clipnorm = 10)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 300)\n"
     ]
    }
   ],
   "source": [
    "model = define_model(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix_glove',\n",
    "                     em_trainable_flag=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1337 samples, validate on 335 samples\n",
      "Epoch 1/100\n",
      "1337/1337 [==============================] - 32s 24ms/step - loss: 8.4750 - acc: 0.2670 - val_loss: 7.6982 - val_acc: 0.2119\n",
      "Epoch 2/100\n",
      "1337/1337 [==============================] - 32s 24ms/step - loss: 8.4750 - acc: 0.2648 - val_loss: 7.6982 - val_acc: 0.2119\n",
      "Epoch 3/100\n",
      "1337/1337 [==============================] - 33s 25ms/step - loss: 8.4750 - acc: 0.2648 - val_loss: 7.6982 - val_acc: 0.2119\n",
      "Epoch 4/100\n",
      "1337/1337 [==============================] - 39s 29ms/step - loss: 8.4750 - acc: 0.2812 - val_loss: 7.6982 - val_acc: 0.2119\n",
      "Epoch 5/100\n",
      "1337/1337 [==============================] - 32s 24ms/step - loss: 8.4750 - acc: 0.2857 - val_loss: 7.6982 - val_acc: 0.2119\n",
      "Epoch 6/100\n",
      "1337/1337 [==============================] - 32s 24ms/step - loss: 8.4750 - acc: 0.2648 - val_loss: 7.6982 - val_acc: 0.2119\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea7d4f9c50>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EarlyStop= EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n",
    "\n",
    "\n",
    "model.fit(x=X_train,y=y_train, epochs=100,batch_size=32,  #x=[X_train,X_ref_train]\n",
    "              callbacks=[EarlyStop],validation_data=(X_test,y_test))    #[X_test,X_ref_test],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)   #pred = model.predict([X_test,X_ref_train])\n",
    "out= get_class_from_pred(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_metrics as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03363531516328655\n"
     ]
    }
   ],
   "source": [
    "actual= get_class_from_pred(y_test)\n",
    "\n",
    "print(m.quadratic_weighted_kappa(actual,out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" ,accuracy_score(actual,out))\n",
    "print(\"Macro: \",f1_score(actual,out,average='macro'))\n",
    "print(\"Weighted: \",f1_score(actual,out,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
