{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ml_metrics as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 1193514  words loaded!\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r',errors='ignore')\n",
    "    model = {}\n",
    "    \n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        try: \n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            model[word] = embedding\n",
    "        except :\n",
    "            pass\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "#model = loadGloveModel(r\"/home/aruhi/Word Embeddings/glove.twitter.27B/glove.twitter.27B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model,open(\"GLOVE.DAT\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(data['EssaySet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data,open(\"ASAP_TRAIN_DATA.dat\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test,open(\"ASAP_Test_DATA.json\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"After reading the group’s procedure, describe what additional information you would need in order to replicate the experiment. Make sure to include at least three pieces of information.\",\"Draw a conclusion based on the student’s data. Describe two ways the student could have improved the experimental design and/or validity of the results.\",\"Explain how pandas in China are similar to koalas in Australia and how they both are different from pythons. Support your response with information from the article.\",\"Explain the significance of the word “invasive” to the rest of the article. Support your response with information from the article.\",\"Starting with mRNA leaving the nucleus, list and describe four major steps involved in protein synthesis.\",\"List and describe three processes used by cells to control the movement of substances across the cell membrane.\",\"Identify ONE trait that can describe Rose based on her conversations with Anna or Aunt Kolab. Include ONE detail from the story that supports your answer.\",\"During the story, the reader gets background information about Mr. Leonard. Explain the effect that background information has on Paul. Support your response with details from the story.\",\"How does the author organize the article? Support your response with details from the article.\",\"What is the effect of different lid colors on the air temperature inside a glass jar exposed to a lamp?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df,open(\"ASAP_Ques1_DATA.dat\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"ASAP_TRAIN_DATA.dat\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17043"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>EssaySet</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>EssayText</th>\n",
       "      <th>clean</th>\n",
       "      <th>X</th>\n",
       "      <th>X_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Some additional information that we would need...</td>\n",
       "      <td>some additional information that we would need...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>After reading the expirement, I realized that ...</td>\n",
       "      <td>after reading the expirement , i realized that...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What you need is more trials, a control set up...</td>\n",
       "      <td>what you need is more trials , a control set u...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The student should list what rock is better an...</td>\n",
       "      <td>the student should list what rock is better an...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>For the students to be able to make a replicat...</td>\n",
       "      <td>for the students to be able to make a replicat...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  EssaySet  Score1  Score2  \\\n",
       "0   1         1       1       1   \n",
       "1   2         1       1       1   \n",
       "2   3         1       1       1   \n",
       "3   4         1       0       0   \n",
       "4   5         1       2       2   \n",
       "\n",
       "                                           EssayText  \\\n",
       "0  Some additional information that we would need...   \n",
       "1  After reading the expirement, I realized that ...   \n",
       "2  What you need is more trials, a control set up...   \n",
       "3  The student should list what rock is better an...   \n",
       "4  For the students to be able to make a replicat...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  some additional information that we would need...   \n",
       "1  after reading the expirement , i realized that...   \n",
       "2  what you need is more trials , a control set u...   \n",
       "3  the student should list what rock is better an...   \n",
       "4  for the students to be able to make a replicat...   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                                 X_1  \n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...  \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...  \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...  \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...  \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in np.unique(data['EssaySet']):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ac5a291a833b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EssaySet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ref_ans'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'D' is not defined"
     ]
    }
   ],
   "source": [
    "for i in np.unique(data['EssaySet']):\n",
    "    data['Ref_ans'] = D[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...\n",
       "1        [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...\n",
       "2        [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...\n",
       "3        [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...\n",
       "4        [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...\n",
       "5        [34, 15, 49, 1, 67, 7, 171, 32, 15, 408, 1, 57...\n",
       "6        [1, 67, 34, 15, 49, 5, 168, 2, 7093, 147, 1, 5...\n",
       "7        [32, 15, 49, 206, 55, 535, 7, 67, 2, 147, 1, 5...\n",
       "8        [163, 299, 67, 32, 66, 49, 11, 1, 384, 32, 41,...\n",
       "9        [2535, 2, 147, 1, 54, 188, 66, 49, 2, 324, 67,...\n",
       "10       [71, 299, 67, 8, 34, 15, 49, 5, 168, 2, 147, 1...\n",
       "11       [1, 299, 2655, 32, 15, 49, 2, 147, 1, 54, 15, ...\n",
       "12       [103, 11, 157, 535, 7, 299, 67, 5287, 2, 147, ...\n",
       "13       [5, 168, 1, 147, 1, 54, 32, 49, 0, 0, 0, 0, 0,...\n",
       "14       [200, 23, 34, 417, 33, 18, 195, 4, 8, 32, 332,...\n",
       "15       [34, 69, 50, 68, 23, 4, 209, 27, 0, 0, 0, 0, 0...\n",
       "16       [5, 168, 2, 147, 18, 54, 34, 15, 49, 2, 68, 1,...\n",
       "17       [1, 299, 67, 34, 15, 49, 4, 2, 391, 127, 28, 7...\n",
       "18       [5, 168, 2, 147, 18, 54, 34, 15, 49, 6, 3522, ...\n",
       "19       [5, 168, 20, 1, 341, 2, 147, 99, 195, 34, 15, ...\n",
       "20       [32, 15, 49, 2, 29, 196, 998, 52, 835, 184, 32...\n",
       "21       [2535, 2, 147, 1, 54, 34, 15, 49, 2, 68, 1, 15...\n",
       "22       [1, 178, 151, 269, 2, 459, 16, 70, 7, 1, 796, ...\n",
       "23       [20, 18, 1278, 3897, 32, 15, 49, 2, 68, 525, 1...\n",
       "24       [163, 299, 67, 8, 34, 15, 49, 4, 1, 142, 7, 98...\n",
       "25       [114, 225, 1, 807, 195, 34, 15, 49, 2, 68, 23,...\n",
       "26       [10, 269, 2, 459, 23, 145, 10, 584, 3, 48, 10,...\n",
       "27       [109, 57, 110, 667, 651, 491, 40, 139, 176, 73...\n",
       "28       [5, 168, 2, 147, 18, 54, 32, 15, 49, 2, 68, 23...\n",
       "29       [5, 272, 242, 32, 15, 49, 2, 68, 16, 70, 7, 73...\n",
       "                               ...                        \n",
       "17013    [63, 1, 89, 19, 121, 111, 478, 31, 1, 63, 1093...\n",
       "17014    [63, 1106, 1, 90, 140, 63, 66, 192, 1, 197, 33...\n",
       "17015    [63, 63, 32, 14, 1, 231, 113, 20, 730, 3, 1593...\n",
       "17016    [167, 107, 9, 611, 50, 14, 191, 179, 62, 191, ...\n",
       "17017    [63, 1812, 15940, 1182, 1, 131, 59, 1, 90, 193...\n",
       "17018    [89, 1, 113, 89, 15, 192, 1, 1, 90, 268, 0, 0,...\n",
       "17019    [131, 107, 1, 113, 15941, 350, 1, 90, 140, 5, ...\n",
       "17020    [131, 107, 6, 131, 107, 287, 27, 1, 144, 362, ...\n",
       "17021    [63, 34, 166, 9, 15, 14, 63, 13, 31, 48, 34, 2...\n",
       "17022    [131, 107, 9, 66, 350, 1, 90, 140, 30, 823, 9,...\n",
       "17023    [89, 1, 113, 89, 66, 14, 55, 478, 197, 129, 30...\n",
       "17024    [89, 1, 89, 287, 362, 97, 1, 90, 140, 5243, 52...\n",
       "17025    [63, 13, 63, 1761, 37, 2212, 257, 0, 0, 0, 0, ...\n",
       "17026    [63, 63, 15, 14, 208, 13, 451, 1, 5233, 909, 9...\n",
       "17027    [167, 107, 9, 66, 268, 1, 90, 140, 52, 37, 109...\n",
       "17028    [131, 107, 131, 107, 15, 14, 6, 244, 415, 5, 1...\n",
       "17029    [131, 107, 5, 87, 2891, 1, 144, 66, 421, 78, 6...\n",
       "17030    [63, 1, 63, 287, 113, 15, 14, 331, 1742, 452, ...\n",
       "17031    [89, 63, 15, 192, 1, 144, 1, 1767, 1896, 5246,...\n",
       "17032    [63, 9, 130, 37, 2212, 38, 70, 15961, 38, 6, 4...\n",
       "17033    [89, 13, 89, 647, 202, 55, 31, 1, 2591, 904, 3...\n",
       "17034    [63, 1, 113, 63, 647, 422, 202, 508, 917, 2301...\n",
       "17035    [89, 1, 113, 89, 4233, 55, 293, 3, 431, 448, 5...\n",
       "17036    [131, 107, 131, 107, 362, 350, 1, 197, 7, 1, 9...\n",
       "17037    [131, 107, 1106, 1, 144, 131, 107, 15, 192, 20...\n",
       "17038    [63, 63, 1078, 79, 32, 160, 5, 6, 179, 310, 1,...\n",
       "17039    [131, 107, 18, 113, 66, 350, 1, 144, 20, 1, 81...\n",
       "17040    [131, 107, 34, 166, 131, 107, 15, 228, 1, 231,...\n",
       "17041    [167, 107, 79, 10, 864, 1, 144, 167, 107, 31, ...\n",
       "17042    [63, 1, 89, 15, 350, 1, 144, 13, 205, 12, 526,...\n",
       "Name: X, Length: 17043, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(EssaySet):  \n",
    "   \n",
    "\n",
    "    return essays, resolved_score.tolist(), essay_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.unique(data['EssaySet']):\n",
    "    \n",
    "    #train\n",
    "    essay_list, resolved_scores, essay_id = load_training_data(data, i) #REMEMBER THIS ESSAY_SET = 1 intialization\n",
    "\n",
    "    dataX = essay_list\n",
    "    dataY = resolved_scores\n",
    "    no_of_classes = len(set(resolved_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       0\n",
      "4       2\n",
      "5       1\n",
      "6       1\n",
      "7       3\n",
      "8       3\n",
      "9       2\n",
      "10      0\n",
      "11      3\n",
      "12      2\n",
      "13      2\n",
      "14      0\n",
      "15      0\n",
      "16      3\n",
      "17      0\n",
      "18      0\n",
      "19      3\n",
      "20      0\n",
      "21      2\n",
      "22      0\n",
      "23      3\n",
      "24      1\n",
      "25      2\n",
      "26      2\n",
      "27      0\n",
      "28      3\n",
      "29      2\n",
      "       ..\n",
      "1642    2\n",
      "1643    1\n",
      "1644    1\n",
      "1645    1\n",
      "1646    3\n",
      "1647    0\n",
      "1648    2\n",
      "1649    0\n",
      "1650    1\n",
      "1651    1\n",
      "1652    0\n",
      "1653    2\n",
      "1654    2\n",
      "1655    0\n",
      "1656    2\n",
      "1657    3\n",
      "1658    3\n",
      "1659    3\n",
      "1660    3\n",
      "1661    2\n",
      "1662    0\n",
      "1663    3\n",
      "1664    2\n",
      "1665    2\n",
      "1666    2\n",
      "1667    3\n",
      "1668    0\n",
      "1669    0\n",
      "1670    1\n",
      "1671    1\n",
      "Name: Score1, Length: 1672, dtype: int64\n",
      "trainX:  (1672, 1808) (17043,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1672, 17043]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-a6599f40b33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print (train_index , test_index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2029\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2031\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1672, 17043]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "count=0;\n",
    "test_count=0;\n",
    "cvscores = []\n",
    "final_Core= [] \n",
    "ID=[]\n",
    "NUMBER=[]\n",
    "\n",
    "for i in np.unique(data['EssaySet']):\n",
    "    X_ref_train = list(X_ref)[i-1]\n",
    "    X_ref_test = list(X_ref)[i-1]\n",
    "\n",
    "    \n",
    "    #train\n",
    "    essay_list, resolved_scores = X_token[:lengths[i-1]] , data['Score1']\n",
    "    print(data['Score1'][:lengths[i-1]]) \n",
    "    dataX = essay_list\n",
    "    dataY = resolved_scores\n",
    "    no_of_classes = len(set(resolved_scores))\n",
    "\n",
    "    trainX = [np.asarray(i) for i in essay_list]\n",
    "    trainX = np.asarray(trainX)\n",
    "    trainY = dataY\n",
    "    trainY = np.asarray(trainY)\n",
    "\n",
    "    print ('trainX: ',(trainX.shape),(trainY.shape))\n",
    "\n",
    "\n",
    "\n",
    "        #print (train_index , test_index)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(trainX,trainY,test_size = 0.2)\n",
    "    print (X_train.shape, y_train.shape)\n",
    "    y_train = to_categorical(y_train , 4)\n",
    "    y_test = to_categorical(y_test , 4)\n",
    "    \n",
    "    model = define_model(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix_glove',\n",
    "                     em_trainable_flag=False\n",
    "                    )\n",
    "\n",
    "    EarlyStop= EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n",
    "\n",
    "    \n",
    "    model.fit(x=[X_train,X_ref_train],y=y_train, epochs=1,batch_size=32,\n",
    "              callbacks=[EarlyStop],validation_data=([X_test,X_ref_test],y_test))\n",
    "\n",
    "    pred = model.predict([X_test,X_ref_test])\n",
    "    out= get_class_from_pred(pred)\n",
    "    actual= get_class_from_pred(y_test)\n",
    "\n",
    "    print('Kappa: ', m.quadratic_weighted_kappa(actual,out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels (trainY):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(trainY)\n",
    "    temp1 = le.transform(trainY)\n",
    "    return to_categorical(temp1,4), le.classes_, trainY\n",
    "\n",
    "_,lable_encoding,_=convert_labels(data['Score1'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_class_from_pred(pred):\n",
    "    return [lable_encoding[x.argmax()] for x in pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ans = ['You need to know how much vinegar was used in each container. You need to know what type of vinegar was used in each container. You need to know what materials to test. You need to know what size/surface area of materials should be used. You need to know how long each sample was rinsed in distilled water.You need to know what drying method to use. You need to know what size/type of container to use. Other acceptable responses.' , 'Plastic sample B has more stretchability than the other polymer plastics. Plastic sample A has the least amount of stretchability compared to the other polymer plastics. Not all polymer plastics have the same stretchability. Different polymer plastics have different stretchability and are therefore suited for different applications. A reasonable conclusion cannot be drawn due to procedural errors. Provide the before and after measurements for length. Did the samples all start out the same size?. Make sure the samples are all of the same thickness. Variations in thickness could have caused variations in stretchability. Perform additional trials. Some of the samples have similar stretchability A and C, B and D. Two trials may not be enough to conclusively state that one is more stretchable than the other. Indicate how many weights were added to the clamps. Was it the same number for each sample?.','''BUSHNELL, Fla—RobRoy Maclnnes is the man to see if you want to buy a crocodile. Or a scorpion, a rattlesnake, a boa constrictor. Got a hankering for a cobra? Just pony up $600 and you can have one of the more lethal species.\"It is a very effective threat display,\" Maclnnes, 49, says as a Pakistan black cobra, six feet long, hissing, hood spread, writhes in its enclosure and strikes again and again and again at the thin glass separating the creature from a visitor. \"A snake like that, coining at you, you would leave him alone.\" Or simply die of fright. Maclnnes is co-owner of Glades Herp Farms, an empire of claws, spines, scales, fangs and darting tongues. The reptile trade, he is happy to report, is booming. The pet industry estimates that about 4.8 million households now contain at least one pet reptile, a number that has nearly doubled in a decade. Reptiles are increasingly popular in a crowded, urbanized nation. They do not need a yard. You do not have to take a lizard for a walk. But biologists see the trade in nonnative creatures as a factor in the rising number of invasive species, such as the Burmese python, which is breeding up a storm in the Everglades, and the Nile monitor lizard, a toothy carnivore that can reach seven feet in length and has found a happy home along the canals of Cape Coral. Under a new state law, a customer must obtain a $100 annual permit to buy a monitor lizard or some of the largest snakes-four species of pythons and the green anaconda. The animal must also be implanted with a microchip. That tag could help officials identify the animal if it turns up later in the wild. Maclnnes contends that the government overestimates the threat posed by invasive reptiles. He says he is being blocked by the U.S. Fish and Wildlife Service from importing some commercially attractive animals, such as Fiji island iguanas and radiated tortoises from Madagascar. Even the term \"invasive species\" is unfair, he said. \"They are introduced.\" I think that invasive is passing judgment.Of the pythons, he said: \"To me, it is a wonderful introduction. I think it is the best thing to happen to the Everglades in the last 200 years.\" Biologists, however, say that invasive species, unchecked by natural predators, are major threats to biodiversity. Life on Earth has always moved around, but never so fast. Organisms evolve in niche environments. What happens when the natural barriers are removed? When anything can go anywhere?Complications ensue. Skip Snow, a wildlife biologist for Everglades National Park, has helped drag hundreds of Burmese pythons out of the weeds, of roadways and even from under the hood of a tourist's car. He calls Maclnnes's argument \"ridiculous.\" The snakes, he says, are imperiling five endangered species in the Florida Keys, including the Key Largo wood rat, one specimen of which, tagged with a radio transmitter, was tracked all the way to the belly of a python. No one knows how the snakes went native, but there's speculation that Hurricane Andrew, which obliterated thousands of homes, played a factor in a wholesale python jailbreak in 1992. Many invasive species undergo a lag before proliferating. What's certain is that, by 2002, pythons were seen in multiple locations in remote regions of the Everglades. Then one morning in early 2003 a bunch of tourists on the park's Anhinga Trail, a reliable location for viewing wildlife, were startled to see an alligator with a python in its mouth. Even more dramatic was what happened in the Everglades in 2005: A python swallowed an alligator and—there’s not a delicate way to put it—exploded. The photograph ran around the world; it wasn't pretty, but you had to look. This February, the U.S. Geological Survey reported that pythons in Asia inhabit climates that are similar to those in about a third of the continental United States. A USGS map showed potential python habitat stretching from California to Delaware and including much of the South. You could conceivably have pythons snacking their way right up the Potomac. The map wasn't a prediction of where the snakes will actually spread, however. Media coverage of it was overly sensational, argues the map’s co-author, Robert Reed. \"When was the last snake story that didn't get sensationalized?\" he asked. \"Ecophobia is playing a role,\" said Jamie K. Reaser, a science and policy adviser to the Pet Industry Joint Advisory Council. \"Mammals are warm and fuzzy. Birds tend to have quite a following. But animals such as lizards and snakes tend, at least in this culture, to be less well respected or supported.\" What is happening in Florida illustrates a broader fact about life on Earth: We live in an age that favors generalists rather than specialists. A generalist is a raccoon, a python, a cockroach, a white-tailed deer. The ultimate generalist is, arguably, a human being, who with the assistance of technology can live anywhere from Florida to Antarctica to outer space. It's no accident that the species that have become most abundant are often those that do best in and around humans. A specialist is China's panda, which eats almost nothing but bamboo, or Australia's koala bear, which eats eucalyptus leaves almost exclusively. Maclnnes is not without an environmental conscience. \"We're degrading the Earth at an alarming rate,\" he said. \"Will man go extinct before we reach the point where we figure it out?\" He added: \"What favors generalists is change. What favors specialists is stability. Right now, mankind has chosen to make Earth a rapidly changing place.\" Down in the Everglades, Skip Snow would agree with that part of Maclnnes's philosophy. We are all part of a vast experiment in the blending of organisms from around the world, he said. \"The thing about the experiment is, it's not planned, and there's no one in control,\" Snow added. \"It's an experiment run amok.\"''', '''BUSHNELL, Fla—RobRoy Maclnnes is the man to see if you want to buy a crocodile. Or a scorpion, a rattlesnake, a boa constrictor. Got a hankering for a cobra? Just pony up $600 and you can have one of the more lethal species.\"It is a very effective threat display,\" Maclnnes, 49, says as a Pakistan black cobra, six feet long, hissing, hood spread, writhes in its enclosure and strikes again and again and again at the thin glass separating the creature from a visitor. \"A snake like that, coining at you, you would leave him alone.\" Or simply die of fright. Maclnnes is co-owner of Glades Herp Farms, an empire of claws, spines, scales, fangs and darting tongues. The reptile trade, he is happy to report, is booming. The pet industry estimates that about 4.8 million households now contain at least one pet reptile, a number that has nearly doubled in a decade. Reptiles are increasingly popular in a crowded, urbanized nation. They do not need a yard. You do not have to take a lizard for a walk. But biologists see the trade in nonnative creatures as a factor in the rising number of invasive species, such as the Burmese python, which is breeding up a storm in the Everglades, and the Nile monitor lizard, a toothy carnivore that can reach seven feet in length and has found a happy home along the canals of Cape Coral. Under a new state law, a customer must obtain a $100 annual permit to buy a monitor lizard or some of the largest snakes-four species of pythons and the green anaconda. The animal must also be implanted with a microchip. That tag could help officials identify the animal if it turns up later in the wild. Maclnnes contends that the government overestimates the threat posed by invasive reptiles. He says he is being blocked by the U.S. Fish and Wildlife Service from importing some commercially attractive animals, such as Fiji island iguanas and radiated tortoises from Madagascar. Even the term \"invasive species\" is unfair, he said. \"They are introduced.\" I think that invasive is passing judgment.Of the pythons, he said: \"To me, it is a wonderful introduction. I think it is the best thing to happen to the Everglades in the last 200 years.\" Biologists, however, say that invasive species, unchecked by natural predators, are major threats to biodiversity. Life on Earth has always moved around, but never so fast. Organisms evolve in niche environments. What happens when the natural barriers are removed? When anything can go anywhere?Complications ensue. Skip Snow, a wildlife biologist for Everglades National Park, has helped drag hundreds of Burmese pythons out of the weeds, of roadways and even from under the hood of a tourist's car. He calls Maclnnes's argument \"ridiculous.\" The snakes, he says, are imperiling five endangered species in the Florida Keys, including the Key Largo wood rat, one specimen of which, tagged with a radio transmitter, was tracked all the way to the belly of a python. No one knows how the snakes went native, but there's speculation that Hurricane Andrew, which obliterated thousands of homes, played a factor in a wholesale python jailbreak in 1992. Many invasive species undergo a lag before proliferating. What's certain is that, by 2002, pythons were seen in multiple locations in remote regions of the Everglades. Then one morning in early 2003 a bunch of tourists on the park's Anhinga Trail, a reliable location for viewing wildlife, were startled to see an alligator with a python in its mouth. Even more dramatic was what happened in the Everglades in 2005: A python swallowed an alligator and—there’s not a delicate way to put it—exploded. The photograph ran around the world; it wasn't pretty, but you had to look. This February, the U.S. Geological Survey reported that pythons in Asia inhabit climates that are similar to those in about a third of the continental United States. A USGS map showed potential python habitat stretching from California to Delaware and including much of the South. You could conceivably have pythons snacking their way right up the Potomac. The map wasn't a prediction of where the snakes will actually spread, however. Media coverage of it was overly sensational, argues the map’s co-author, Robert Reed. \"When was the last snake story that didn't get sensationalized?\" he asked. \"Ecophobia is playing a role,\" said Jamie K. Reaser, a science and policy adviser to the Pet Industry Joint Advisory Council. \"Mammals are warm and fuzzy. Birds tend to have quite a following. But animals such as lizards and snakes tend, at least in this culture, to be less well respected or supported.\" What is happening in Florida illustrates a broader fact about life on Earth: We live in an age that favors generalists rather than specialists. A generalist is a raccoon, a python, a cockroach, a white-tailed deer. The ultimate generalist is, arguably, a human being, who with the assistance of technology can live anywhere from Florida to Antarctica to outer space. It's no accident that the species that have become most abundant are often those that do best in and around humans. A specialist is China's panda, which eats almost nothing but bamboo, or Australia's koala bear, which eats eucalyptus leaves almost exclusively. Maclnnes is not without an environmental conscience. \"We're degrading the Earth at an alarming rate,\" he said. \"Will man go extinct before we reach the point where we figure it out?\" He added: \"What favors generalists is change. What favors specialists is stability. Right now, mankind has chosen to make Earth a rapidly changing place.\" Down in the Everglades, Skip Snow would agree with that part of Maclnnes's philosophy. We are all part of a vast experiment in the blending of organisms from around the world, he said. \"The thing about the experiment is, it's not planned, and there's no one in control,\" Snow added. \"It's an experiment run amok.\"''', '''mRNA exits nucleus via nuclear pore.  mRNA travels through the cytoplasm to the ribosome or enters the rough endoplasmic reticulum. mRNA bases are read in triplets called codons (by rRNA). tRNA carrying the complementary (U=A, C+G) anticodon recognizes the complementary codon of the mRNA. The corresponding amino acids on the other end of the tRNA are bonded to adjacent tRNA’s amino acids. A new corresponding amino acid is added to the tRNA. Amino acids are linked together to make a protein beginning with a START codon in the P site (initiation). Amino acids continue to be linked until a STOP codon is read on the mRNA in the A site (elongation and termination).''' ,'''Selective permeability is used by the cell membrane to allow certain substances to move across. Passive transport occurs when substances move from an area of higher concentration to an area of lower concentration. Osmosis is the diffusion of water across the cell membrane. Facilitated diffusion occurs when the membrane controls the pathway for a particle to enter or leave a cell. Active transport occurs when a cell uses energy to move a substance across the cell membrane, and/or a substance moves from an area of low to high concentration, or against the concentration gradient. Pumps are used to move charged particles like sodium and potassium ions through membranes using energy and carrier proteins. Membrane-assisted transport occurs when the membrane of the vesicle fuses with the cell membrane forcing large molecules out of the cell as in exocytosis. Membrane-assisted transport occurs when molecules are engulfed by the cell membrane as in endocytosis. Membrane-assisted transport occurs when vesicles are formed around large molecules as in phagocytosis. Membrane-assisted transport occurs when vesicles are formed around liquid droplets as in pinocytosis. Protein channels or channel proteins allow for the movement of specific molecules or substances into or out of the cell.''', '''Rose’s head jerked up from her chest. “Oh no,” she groaned, rubbing the back of her neck and blinking at the bright light in the kitchen. For a split second she was confused. Then she remembered: her essay for the state competition. She’d been struggling to think of a topic. Her brain must have surrendered to exhaustion.The day, like most of her days, had been too long, too demanding. From school she’d gone straight to the restaurant to work a four-hour shift, then straight home to help Aunt Kolab prepare a quick supper. After that it was time to do homework.  When would she squeeze in writing a flawless three-thousand-word essay? “I’m insane,” she said grimly as she gathered books and papers.  Even if I win, she thought, I won’t get to travel to Sacramento to receive the prize. She’d already had to miss a lot of shifts, and her supervisor was on the verge of firing her. Her younger sister walked in rubbing her eyes.  “Anna,” Rose said. “What’s wrong? You feel okay?” “I’m fine,” her sister said.  “I just had another bad dream.” “I fell asleep working on my essay,” Rose said.  Anna poured two glasses of orange juice and handed one to Rose. “Mama’s not home yet, is she.” It wasn’t a question. “I hate how late she has to work.” Her voice sank to a fierce whisper. “I’m so lonesome for Papa. It seems like he’s been gone for years.” “It’s only been four months,” Rose said as gently as she could. “He had to go. The job in Los Angeles paid three times what he was making here.” Anna glared at Rose.  “Money isn’t everything.”  “Only if you already have everything,” Rose said. She tried a laugh that sounded fake even to her. “We have our part to do to help Paul finish  college. Then he’ll get a good job, Anna, and he’ll pay for you and me to go to college.” Anna rolled her eyes and shoved her chai r away from the table. “You sound just like Mama.” She stood and stalked out of the kitchen. By the time Rose tiptoed into their room, Anna was already snoring lightly. Rose slid into bed and watched the lights from passing cars move across the walls. They became the lights that had illuminated the stage at Paul’s high school graduation. As her brother accepted his diploma, Rose had glanced at her parents’ faces. Four eyes shining with tears. The work, the sheer weight of it, to get him on that stage slid from them in that moment; only a sweet, triumphant ache remained. Surely they remembered the ship, their young son and daughters clinging to their necks, Cambodia behind them, the United States before them. On that ship perhaps they had imagined their children’s futures, imagined this very day would come. In the dark Rose clasped then cupped her hands.  Paul’s fate lies partly in these, she thought. She felt too young for so much responsibility. Then she shivered, imagining how her brother must feel. Only three years older, he held the fate of two people—both his sisters—in his hands. Rose dreamed that she swam through clear, green-tinted water, enjoying the pure simplicity of a fish’s life. She stopped moving and looked up. She saw Paul jump from a boulder and crash into the water just above her. His body sank as if it were made of stone, pushing her beneath him down to the sandy bottom. She struggled to get out from under him, but he seemed unaware of her. When she opened her mouth to scream get off, water rushed in. Rose woke gasping for air. The walls of her room were bathed in pale sunlight. When her heart had slowed back down, she got up. Anna was still asleep. In the hall Rose stopped at her mother’s room. She was also sleeping. So it was Aunt Kolab making the muted noises coming from the kitchen. “Good morning, Rose,” her aunt said. Rose felt an urgent need to relate the dream, to expose it so it would loosen its grip on her. After she’d finished, her aunt said, with a puzzled look, “Do you feel so weighed down by what you’re doing to help this family?” Rose didn’t answer. If she told the truth, she would hurt her aunt. And probably her aunt would tell her mother.  “In Cambodia, our first country, what we’re all doing would be quite normal,” her aunt said. “But now I realize that you’re seeing the situation through other eyes—as you should, I suppose, because you grew up here. This must be difficult for you. Yes?” Rose nodded. “Hmm. Maybe we can find a way to do things differently. A way better for you.” Her aunt’s face lit up. “Maybe I can sew for ladies. Or I could make special treats from our country and sell them.” Rose kept nodding. Maybe her life would get easier. Maybe it wouldn’t. But her aunt’s offer had somehow made her feel lighter. Suddenly, it occurred to her: here was the topic for her essay, although it was still vague. Cambodian tradition and sense of family, she realized, could survive an ocean crossing.''','''I met Mr. Leonard when I started middle school. He was a hall monitor whose job it was to keep students moving along from one classroom to the next. “Move along, people, move along!” he’d advise the shuffling crowd, and everyone complied. I distinguished myself from the masses by being one of a select few in the remedial reading program. Twice a week, I left English class early for the learning center in the basement, where I worked with a tutor. On my first trip, Mr. Leonard confronted me in the stairwell. “Hey, my friend, where do you think you’re going?” he asked, arms folded across his chest.“Learning center,” I muttered, showing him my hall pass.“Why?” he asked from behind a hard stare.“Why?” I answered automatically, “I can’t read.” His gaze softened. “Fair enough. On your way, then. Work hard.” For the next few weeks, that was the extent of our conversations. He’d meet me in the stairwell, I’d show him my pass. Then one day he surprised me by asking what I did after school. “Nothing,” I answered. “Just some homework.”“Meet me in the gym. 2:30.” Since this gave me a legitimate reason to delay my daily homework battles, I agreed. When I arrived, the gym was crowded with kids warming up for intramurals. Mr. Leonard was seated in a corner, watching. He waved me over, then pointed at the kids chasing basketballs. “None of this appeals to you?” he asked. I shook my head. When you’re the last guy chosen for teams in gym class, you don’t seek out more of that treatment after school. “Follow me,” he directed, and, obediently, I followed. We left the building and went to the track. Spread along the inside lane were hurdles. Mr. Leonard pointed at the closest one.“Know what that thing is called?” he asked. “A hurdle,” I answered.“Know what to do with it?” he questioned. “You jump it,” I replied.“Well?” he responded. “On your way then.” It never occurred to me to refuse--perhaps I’d been conditioned by hearing those words every day. I got into a slow jog and awkwardly hopped over each barrier for a whole lap.“Not a bad first effort,” commented Mr. Leonard as I staggered in. “That was terrible,” I gasped.“You’ll do better next time,” he responded. “Bring sneakers and shorts tomorrow.”“Right,” I panted. Mr. Leonard began walking back toward the school, then turned and asked, “Say, what’s your name?”“Paul.” It didn’t occur to me until later that this was an odd question for someone who had checked my hall pass twice a week. And so it began. Monday through Friday, rain or shine, I was out on the track with Mr. Leonard shouting from the side. “Open your stride!” “Pump your arms!” “Lean, . . .  NOW!”  I improved steadily until one day I found myself standing before the high school track coach. “How’d you get so fast, son?” he asked. “Well, I’ve been training,” I replied. “Someone’s helping me.”“Mr. Leonard Grabowski?” I nodded. The coach smiled and asked me to work out with the high school team. Then he scribbled on a scrap of paper and handed it me. It was a URL for a track and field website. “Visit this site. Do a search for ‘Grabowski.’” The next day, I told Mr. Leonard about my conversation with the coach and asked if he thought I should work out with the team. “Absolutely,” he replied with a grin. “A little competition will only help.” I pulled the printout I’d downloaded the night before from my pocket. “Why didn’t you tell me about this?” He looked at me quizzically, then smiled sadly at the image on the page. “I looked good back then, didn’t I?” he chuckled. I moved beside him and pointed to the photograph. “You were a college freshman who won the 400 meter hurdles at the nationals. You broke records.” “I remember,” he said solemnly. “Best race of my life.” “Well, what happened after that?” I pressed.Mr. Leonard handed the paper back and looked at the ground, his brow furrowed, his voice cracked as he spoke.“I was a good athlete,” he said softly, “but not a good student.  We had no learning centers in our school. I relied on friends to help me get by, but even then the work was always too hard.”  His voice trailed off. “But you went to college,” I said.“Things were different back then,” he replied. “The college scouts told me that my grades didn’t matter, that I’d have tutors to help me, but college work is a whole lot harder than high school work. I lost my scholarship and flunked out. No other school wanted a runner who couldn’t read.” The emotions in Mr. Leonard’s words were all too familiar to me. I knew them well--feelings of embarrassment when I was called upon to read aloud or when I didn’t know an answer everyone else knew. This man had given his time to help me excel at something. Suddenly I realized what I could do for him. “C’mon, Mr. Leonard,” I said, walking back toward school. “It’s time to start your training.”''','''Grab your telescope! Look up in the sky! It’s a comet! It’s a meteor! It’s a tool bag?Such an observation isn’t as strange as it seems. Orbital pathways around our planet that were once clear are now cluttered with the remains of numerous space exploration and satellite missions. This “space junk” is currently of great concern to government space agencies around the globe.What Is Space Junk?In 1957, the Soviet Union launched the first artificial satellite. The United States followed suit, and thus began the human race’s great space invasion.Over the past 52 years, a variety of spacecraft, including space capsules, telescopes, and satellites, have been sent beyond Earth’s atmosphere. They explore the vast reaches of our solar system, monitor atmospheric conditions, and make global wireless communication possible. The rockets that are used to power these spacecraft typically fall back to Earth and disintegrate in the intense heat that results from friction with Earth’s atmosphere. The objects themselves, however, are positioned hundreds of miles above Earth, far from elements that would cause them to degrade or burn up. In this airless environment, some of them continue to circle the planet indefinitely. While this is ideal for a fully functioning object that was launched for that purpose—for example, a communications satellite—what happens when a satellite “dies” or malfunctions and can’t be repaired? The disabled object becomes a piece of high-tech junk, circling the globe in uncontrolled orbit.With no one at the controls, dead satellites run the risk of colliding with each other. That’s exactly what happened in February 2009. Two communications satellites, one American and one Russian, both traveling at more than 20,000 miles per hour, crashed into each other 491 miles above the Earth. The impact created hundreds of pieces of debris, each assuming its own orbital path. Now, instead of two disabled satellites, there are hundreds of microsatellites flying through space. It’s not only spectacular crashes that create debris. Any objects released into space become free-orbiting satellites, which means that astronauts must take great care when they leave their spacecraft to make repairs or do experiments. Still, accidents do happen: in 2008, a tool bag escaped from the grip of an astronaut doing repairs on the International Space Station (ISS). Little Bits, But a Big Deal. So who cares about a lost tool bag or tiny bits of space trash? Actually, many people do. Those bits of space debris present a very serious problem. Tiny fragments traveling at a speed of five miles per second can inflict serious damage on the most carefully designed spacecraft. If you find that hard to believe, compare grains of sand blown by a gentle breeze to those shot from a sandblaster to strip paint from a concrete wall. At extreme speeds, little bits can pack a punch powerful enough to create disastrous holes in an object moving through space. Scientists are hard-pressed for an easy solution to the problem of space junk. Both the National Aeronautics and Space Agency (NASA) and the European Space Agency maintain catalogues of known objects. The lost tool bag, for example, is listed as Satellite 33442. But while military radar can identify objects the size of a baseball, anything smaller goes undetected. This makes it difficult for spacecraft to steer clear of microdebris fields. Accepting the inevitability of contact, engineers have added multiple walls to spacecraft and stronger materials to spacesuits to diminish the effects of impact.Yet the problem is certain to persist. In fact, the amount of space trash is actually increasing because commercial space travel is on the rise and more nations have undertaken space exploration. Space agencies hope that the corporations and nations involved can work together to come up with a viable solution to space pollution. ''','''Black. The doghouse will be warmer. The black lid made the jar warmest. Dark gray. The inside will be a little warmer, but not too hot. The dark gray lid increased 6º C more than the white. Light gray. The inside will stay cooler, but not too cool. The light gray lid was 8º C cooler than the black. White. The inside will be cooler. The white lid only went up to 42º C.''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You need to know how much vinegar was used in each container. You need to know what type of vinegar was used in each container. You need to know what materials to test. You need to know what size/surface area of materials should be used. You need to know how long each sample was rinsed in distilled water.You need to know what drying method to use. You need to know what size/type of container to use. Other acceptable responses.',\n",
       " 'Plastic sample B has more stretchability than the other polymer plastics. Plastic sample A has the least amount of stretchability compared to the other polymer plastics. Not all polymer plastics have the same stretchability. Different polymer plastics have different stretchability and are therefore suited for different applications. A reasonable conclusion cannot be drawn due to procedural errors. Provide the before and after measurements for length. Did the samples all start out the same size?. Make sure the samples are all of the same thickness. Variations in thickness could have caused variations in stretchability. Perform additional trials. Some of the samples have similar stretchability A and C, B and D. Two trials may not be enough to conclusively state that one is more stretchable than the other. Indicate how many weights were added to the clamps. Was it the same number for each sample?.',\n",
       " 'BUSHNELL, Fla—RobRoy Maclnnes is the man to see if you want to buy a crocodile. Or a scorpion, a rattlesnake, a boa constrictor. Got a hankering for a cobra? Just pony up $600 and you can have one of the more lethal species.\"It is a very effective threat display,\" Maclnnes, 49, says as a Pakistan black cobra, six feet long, hissing, hood spread, writhes in its enclosure and strikes again and again and again at the thin glass separating the creature from a visitor. \"A snake like that, coining at you, you would leave him alone.\" Or simply die of fright. Maclnnes is co-owner of Glades Herp Farms, an empire of claws, spines, scales, fangs and darting tongues. The reptile trade, he is happy to report, is booming. The pet industry estimates that about 4.8 million households now contain at least one pet reptile, a number that has nearly doubled in a decade. Reptiles are increasingly popular in a crowded, urbanized nation. They do not need a yard. You do not have to take a lizard for a walk. But biologists see the trade in nonnative creatures as a factor in the rising number of invasive species, such as the Burmese python, which is breeding up a storm in the Everglades, and the Nile monitor lizard, a toothy carnivore that can reach seven feet in length and has found a happy home along the canals of Cape Coral. Under a new state law, a customer must obtain a $100 annual permit to buy a monitor lizard or some of the largest snakes-four species of pythons and the green anaconda. The animal must also be implanted with a microchip. That tag could help officials identify the animal if it turns up later in the wild. Maclnnes contends that the government overestimates the threat posed by invasive reptiles. He says he is being blocked by the U.S. Fish and Wildlife Service from importing some commercially attractive animals, such as Fiji island iguanas and radiated tortoises from Madagascar. Even the term \"invasive species\" is unfair, he said. \"They are introduced.\" I think that invasive is passing judgment.Of the pythons, he said: \"To me, it is a wonderful introduction. I think it is the best thing to happen to the Everglades in the last 200 years.\" Biologists, however, say that invasive species, unchecked by natural predators, are major threats to biodiversity. Life on Earth has always moved around, but never so fast. Organisms evolve in niche environments. What happens when the natural barriers are removed? When anything can go anywhere?Complications ensue. Skip Snow, a wildlife biologist for Everglades National Park, has helped drag hundreds of Burmese pythons out of the weeds, of roadways and even from under the hood of a tourist\\'s car. He calls Maclnnes\\'s argument \"ridiculous.\" The snakes, he says, are imperiling five endangered species in the Florida Keys, including the Key Largo wood rat, one specimen of which, tagged with a radio transmitter, was tracked all the way to the belly of a python. No one knows how the snakes went native, but there\\'s speculation that Hurricane Andrew, which obliterated thousands of homes, played a factor in a wholesale python jailbreak in 1992. Many invasive species undergo a lag before proliferating. What\\'s certain is that, by 2002, pythons were seen in multiple locations in remote regions of the Everglades. Then one morning in early 2003 a bunch of tourists on the park\\'s Anhinga Trail, a reliable location for viewing wildlife, were startled to see an alligator with a python in its mouth. Even more dramatic was what happened in the Everglades in 2005: A python swallowed an alligator and—there’s not a delicate way to put it—exploded. The photograph ran around the world; it wasn\\'t pretty, but you had to look. This February, the U.S. Geological Survey reported that pythons in Asia inhabit climates that are similar to those in about a third of the continental United States. A USGS map showed potential python habitat stretching from California to Delaware and including much of the South. You could conceivably have pythons snacking their way right up the Potomac. The map wasn\\'t a prediction of where the snakes will actually spread, however. Media coverage of it was overly sensational, argues the map’s co-author, Robert Reed. \"When was the last snake story that didn\\'t get sensationalized?\" he asked. \"Ecophobia is playing a role,\" said Jamie K. Reaser, a science and policy adviser to the Pet Industry Joint Advisory Council. \"Mammals are warm and fuzzy. Birds tend to have quite a following. But animals such as lizards and snakes tend, at least in this culture, to be less well respected or supported.\" What is happening in Florida illustrates a broader fact about life on Earth: We live in an age that favors generalists rather than specialists. A generalist is a raccoon, a python, a cockroach, a white-tailed deer. The ultimate generalist is, arguably, a human being, who with the assistance of technology can live anywhere from Florida to Antarctica to outer space. It\\'s no accident that the species that have become most abundant are often those that do best in and around humans. A specialist is China\\'s panda, which eats almost nothing but bamboo, or Australia\\'s koala bear, which eats eucalyptus leaves almost exclusively. Maclnnes is not without an environmental conscience. \"We\\'re degrading the Earth at an alarming rate,\" he said. \"Will man go extinct before we reach the point where we figure it out?\" He added: \"What favors generalists is change. What favors specialists is stability. Right now, mankind has chosen to make Earth a rapidly changing place.\" Down in the Everglades, Skip Snow would agree with that part of Maclnnes\\'s philosophy. We are all part of a vast experiment in the blending of organisms from around the world, he said. \"The thing about the experiment is, it\\'s not planned, and there\\'s no one in control,\" Snow added. \"It\\'s an experiment run amok.\"',\n",
       " 'BUSHNELL, Fla—RobRoy Maclnnes is the man to see if you want to buy a crocodile. Or a scorpion, a rattlesnake, a boa constrictor. Got a hankering for a cobra? Just pony up $600 and you can have one of the more lethal species.\"It is a very effective threat display,\" Maclnnes, 49, says as a Pakistan black cobra, six feet long, hissing, hood spread, writhes in its enclosure and strikes again and again and again at the thin glass separating the creature from a visitor. \"A snake like that, coining at you, you would leave him alone.\" Or simply die of fright. Maclnnes is co-owner of Glades Herp Farms, an empire of claws, spines, scales, fangs and darting tongues. The reptile trade, he is happy to report, is booming. The pet industry estimates that about 4.8 million households now contain at least one pet reptile, a number that has nearly doubled in a decade. Reptiles are increasingly popular in a crowded, urbanized nation. They do not need a yard. You do not have to take a lizard for a walk. But biologists see the trade in nonnative creatures as a factor in the rising number of invasive species, such as the Burmese python, which is breeding up a storm in the Everglades, and the Nile monitor lizard, a toothy carnivore that can reach seven feet in length and has found a happy home along the canals of Cape Coral. Under a new state law, a customer must obtain a $100 annual permit to buy a monitor lizard or some of the largest snakes-four species of pythons and the green anaconda. The animal must also be implanted with a microchip. That tag could help officials identify the animal if it turns up later in the wild. Maclnnes contends that the government overestimates the threat posed by invasive reptiles. He says he is being blocked by the U.S. Fish and Wildlife Service from importing some commercially attractive animals, such as Fiji island iguanas and radiated tortoises from Madagascar. Even the term \"invasive species\" is unfair, he said. \"They are introduced.\" I think that invasive is passing judgment.Of the pythons, he said: \"To me, it is a wonderful introduction. I think it is the best thing to happen to the Everglades in the last 200 years.\" Biologists, however, say that invasive species, unchecked by natural predators, are major threats to biodiversity. Life on Earth has always moved around, but never so fast. Organisms evolve in niche environments. What happens when the natural barriers are removed? When anything can go anywhere?Complications ensue. Skip Snow, a wildlife biologist for Everglades National Park, has helped drag hundreds of Burmese pythons out of the weeds, of roadways and even from under the hood of a tourist\\'s car. He calls Maclnnes\\'s argument \"ridiculous.\" The snakes, he says, are imperiling five endangered species in the Florida Keys, including the Key Largo wood rat, one specimen of which, tagged with a radio transmitter, was tracked all the way to the belly of a python. No one knows how the snakes went native, but there\\'s speculation that Hurricane Andrew, which obliterated thousands of homes, played a factor in a wholesale python jailbreak in 1992. Many invasive species undergo a lag before proliferating. What\\'s certain is that, by 2002, pythons were seen in multiple locations in remote regions of the Everglades. Then one morning in early 2003 a bunch of tourists on the park\\'s Anhinga Trail, a reliable location for viewing wildlife, were startled to see an alligator with a python in its mouth. Even more dramatic was what happened in the Everglades in 2005: A python swallowed an alligator and—there’s not a delicate way to put it—exploded. The photograph ran around the world; it wasn\\'t pretty, but you had to look. This February, the U.S. Geological Survey reported that pythons in Asia inhabit climates that are similar to those in about a third of the continental United States. A USGS map showed potential python habitat stretching from California to Delaware and including much of the South. You could conceivably have pythons snacking their way right up the Potomac. The map wasn\\'t a prediction of where the snakes will actually spread, however. Media coverage of it was overly sensational, argues the map’s co-author, Robert Reed. \"When was the last snake story that didn\\'t get sensationalized?\" he asked. \"Ecophobia is playing a role,\" said Jamie K. Reaser, a science and policy adviser to the Pet Industry Joint Advisory Council. \"Mammals are warm and fuzzy. Birds tend to have quite a following. But animals such as lizards and snakes tend, at least in this culture, to be less well respected or supported.\" What is happening in Florida illustrates a broader fact about life on Earth: We live in an age that favors generalists rather than specialists. A generalist is a raccoon, a python, a cockroach, a white-tailed deer. The ultimate generalist is, arguably, a human being, who with the assistance of technology can live anywhere from Florida to Antarctica to outer space. It\\'s no accident that the species that have become most abundant are often those that do best in and around humans. A specialist is China\\'s panda, which eats almost nothing but bamboo, or Australia\\'s koala bear, which eats eucalyptus leaves almost exclusively. Maclnnes is not without an environmental conscience. \"We\\'re degrading the Earth at an alarming rate,\" he said. \"Will man go extinct before we reach the point where we figure it out?\" He added: \"What favors generalists is change. What favors specialists is stability. Right now, mankind has chosen to make Earth a rapidly changing place.\" Down in the Everglades, Skip Snow would agree with that part of Maclnnes\\'s philosophy. We are all part of a vast experiment in the blending of organisms from around the world, he said. \"The thing about the experiment is, it\\'s not planned, and there\\'s no one in control,\" Snow added. \"It\\'s an experiment run amok.\"',\n",
       " 'mRNA exits nucleus via nuclear pore.  mRNA travels through the cytoplasm to the ribosome or enters the rough endoplasmic reticulum. mRNA bases are read in triplets called codons (by rRNA). tRNA carrying the complementary (U=A, C+G) anticodon recognizes the complementary codon of the mRNA. The corresponding amino acids on the other end of the tRNA are bonded to adjacent tRNA’s amino acids. A new corresponding amino acid is added to the tRNA. Amino acids are linked together to make a protein beginning with a START codon in the P site (initiation). Amino acids continue to be linked until a STOP codon is read on the mRNA in the A site (elongation and termination).',\n",
       " 'Selective permeability is used by the cell membrane to allow certain substances to move across. Passive transport occurs when substances move from an area of higher concentration to an area of lower concentration. Osmosis is the diffusion of water across the cell membrane. Facilitated diffusion occurs when the membrane controls the pathway for a particle to enter or leave a cell. Active transport occurs when a cell uses energy to move a substance across the cell membrane, and/or a substance moves from an area of low to high concentration, or against the concentration gradient. Pumps are used to move charged particles like sodium and potassium ions through membranes using energy and carrier proteins. Membrane-assisted transport occurs when the membrane of the vesicle fuses with the cell membrane forcing large molecules out of the cell as in exocytosis. Membrane-assisted transport occurs when molecules are engulfed by the cell membrane as in endocytosis. Membrane-assisted transport occurs when vesicles are formed around large molecules as in phagocytosis. Membrane-assisted transport occurs when vesicles are formed around liquid droplets as in pinocytosis. Protein channels or channel proteins allow for the movement of specific molecules or substances into or out of the cell.',\n",
       " 'Rose’s head jerked up from her chest. “Oh no,” she groaned, rubbing the back of her neck and blinking at the bright light in the kitchen. For a split second she was confused. Then she remembered: her essay for the state competition. She’d been struggling to think of a topic. Her brain must have surrendered to exhaustion.The day, like most of her days, had been too long, too demanding. From school she’d gone straight to the restaurant to work a four-hour shift, then straight home to help Aunt Kolab prepare a quick supper. After that it was time to do homework.  When would she squeeze in writing a flawless three-thousand-word essay? “I’m insane,” she said grimly as she gathered books and papers.  Even if I win, she thought, I won’t get to travel to Sacramento to receive the prize. She’d already had to miss a lot of shifts, and her supervisor was on the verge of firing her. Her younger sister walked in rubbing her eyes.  “Anna,” Rose said. “What’s wrong? You feel okay?” “I’m fine,” her sister said.  “I just had another bad dream.” “I fell asleep working on my essay,” Rose said.  Anna poured two glasses of orange juice and handed one to Rose. “Mama’s not home yet, is she.” It wasn’t a question. “I hate how late she has to work.” Her voice sank to a fierce whisper. “I’m so lonesome for Papa. It seems like he’s been gone for years.” “It’s only been four months,” Rose said as gently as she could. “He had to go. The job in Los Angeles paid three times what he was making here.” Anna glared at Rose.  “Money isn’t everything.”  “Only if you already have everything,” Rose said. She tried a laugh that sounded fake even to her. “We have our part to do to help Paul finish  college. Then he’ll get a good job, Anna, and he’ll pay for you and me to go to college.” Anna rolled her eyes and shoved her chai r away from the table. “You sound just like Mama.” She stood and stalked out of the kitchen. By the time Rose tiptoed into their room, Anna was already snoring lightly. Rose slid into bed and watched the lights from passing cars move across the walls. They became the lights that had illuminated the stage at Paul’s high school graduation. As her brother accepted his diploma, Rose had glanced at her parents’ faces. Four eyes shining with tears. The work, the sheer weight of it, to get him on that stage slid from them in that moment; only a sweet, triumphant ache remained. Surely they remembered the ship, their young son and daughters clinging to their necks, Cambodia behind them, the United States before them. On that ship perhaps they had imagined their children’s futures, imagined this very day would come. In the dark Rose clasped then cupped her hands.  Paul’s fate lies partly in these, she thought. She felt too young for so much responsibility. Then she shivered, imagining how her brother must feel. Only three years older, he held the fate of two people—both his sisters—in his hands. Rose dreamed that she swam through clear, green-tinted water, enjoying the pure simplicity of a fish’s life. She stopped moving and looked up. She saw Paul jump from a boulder and crash into the water just above her. His body sank as if it were made of stone, pushing her beneath him down to the sandy bottom. She struggled to get out from under him, but he seemed unaware of her. When she opened her mouth to scream get off, water rushed in. Rose woke gasping for air. The walls of her room were bathed in pale sunlight. When her heart had slowed back down, she got up. Anna was still asleep. In the hall Rose stopped at her mother’s room. She was also sleeping. So it was Aunt Kolab making the muted noises coming from the kitchen. “Good morning, Rose,” her aunt said. Rose felt an urgent need to relate the dream, to expose it so it would loosen its grip on her. After she’d finished, her aunt said, with a puzzled look, “Do you feel so weighed down by what you’re doing to help this family?” Rose didn’t answer. If she told the truth, she would hurt her aunt. And probably her aunt would tell her mother.  “In Cambodia, our first country, what we’re all doing would be quite normal,” her aunt said. “But now I realize that you’re seeing the situation through other eyes—as you should, I suppose, because you grew up here. This must be difficult for you. Yes?” Rose nodded. “Hmm. Maybe we can find a way to do things differently. A way better for you.” Her aunt’s face lit up. “Maybe I can sew for ladies. Or I could make special treats from our country and sell them.” Rose kept nodding. Maybe her life would get easier. Maybe it wouldn’t. But her aunt’s offer had somehow made her feel lighter. Suddenly, it occurred to her: here was the topic for her essay, although it was still vague. Cambodian tradition and sense of family, she realized, could survive an ocean crossing.',\n",
       " 'I met Mr. Leonard when I started middle school. He was a hall monitor whose job it was to keep students moving along from one classroom to the next. “Move along, people, move along!” he’d advise the shuffling crowd, and everyone complied. I distinguished myself from the masses by being one of a select few in the remedial reading program. Twice a week, I left English class early for the learning center in the basement, where I worked with a tutor. On my first trip, Mr. Leonard confronted me in the stairwell. “Hey, my friend, where do you think you’re going?” he asked, arms folded across his chest.“Learning center,” I muttered, showing him my hall pass.“Why?” he asked from behind a hard stare.“Why?” I answered automatically, “I can’t read.” His gaze softened. “Fair enough. On your way, then. Work hard.” For the next few weeks, that was the extent of our conversations. He’d meet me in the stairwell, I’d show him my pass. Then one day he surprised me by asking what I did after school. “Nothing,” I answered. “Just some homework.”“Meet me in the gym. 2:30.” Since this gave me a legitimate reason to delay my daily homework battles, I agreed. When I arrived, the gym was crowded with kids warming up for intramurals. Mr. Leonard was seated in a corner, watching. He waved me over, then pointed at the kids chasing basketballs. “None of this appeals to you?” he asked. I shook my head. When you’re the last guy chosen for teams in gym class, you don’t seek out more of that treatment after school. “Follow me,” he directed, and, obediently, I followed. We left the building and went to the track. Spread along the inside lane were hurdles. Mr. Leonard pointed at the closest one.“Know what that thing is called?” he asked. “A hurdle,” I answered.“Know what to do with it?” he questioned. “You jump it,” I replied.“Well?” he responded. “On your way then.” It never occurred to me to refuse--perhaps I’d been conditioned by hearing those words every day. I got into a slow jog and awkwardly hopped over each barrier for a whole lap.“Not a bad first effort,” commented Mr. Leonard as I staggered in. “That was terrible,” I gasped.“You’ll do better next time,” he responded. “Bring sneakers and shorts tomorrow.”“Right,” I panted. Mr. Leonard began walking back toward the school, then turned and asked, “Say, what’s your name?”“Paul.” It didn’t occur to me until later that this was an odd question for someone who had checked my hall pass twice a week. And so it began. Monday through Friday, rain or shine, I was out on the track with Mr. Leonard shouting from the side. “Open your stride!” “Pump your arms!” “Lean, . . .  NOW!”  I improved steadily until one day I found myself standing before the high school track coach. “How’d you get so fast, son?” he asked. “Well, I’ve been training,” I replied. “Someone’s helping me.”“Mr. Leonard Grabowski?” I nodded. The coach smiled and asked me to work out with the high school team. Then he scribbled on a scrap of paper and handed it me. It was a URL for a track and field website. “Visit this site. Do a search for ‘Grabowski.’” The next day, I told Mr. Leonard about my conversation with the coach and asked if he thought I should work out with the team. “Absolutely,” he replied with a grin. “A little competition will only help.” I pulled the printout I’d downloaded the night before from my pocket. “Why didn’t you tell me about this?” He looked at me quizzically, then smiled sadly at the image on the page. “I looked good back then, didn’t I?” he chuckled. I moved beside him and pointed to the photograph. “You were a college freshman who won the 400 meter hurdles at the nationals. You broke records.” “I remember,” he said solemnly. “Best race of my life.” “Well, what happened after that?” I pressed.Mr. Leonard handed the paper back and looked at the ground, his brow furrowed, his voice cracked as he spoke.“I was a good athlete,” he said softly, “but not a good student.  We had no learning centers in our school. I relied on friends to help me get by, but even then the work was always too hard.”  His voice trailed off. “But you went to college,” I said.“Things were different back then,” he replied. “The college scouts told me that my grades didn’t matter, that I’d have tutors to help me, but college work is a whole lot harder than high school work. I lost my scholarship and flunked out. No other school wanted a runner who couldn’t read.” The emotions in Mr. Leonard’s words were all too familiar to me. I knew them well--feelings of embarrassment when I was called upon to read aloud or when I didn’t know an answer everyone else knew. This man had given his time to help me excel at something. Suddenly I realized what I could do for him. “C’mon, Mr. Leonard,” I said, walking back toward school. “It’s time to start your training.”',\n",
       " 'Grab your telescope! Look up in the sky! It’s a comet! It’s a meteor! It’s a tool bag?Such an observation isn’t as strange as it seems. Orbital pathways around our planet that were once clear are now cluttered with the remains of numerous space exploration and satellite missions. This “space junk” is currently of great concern to government space agencies around the globe.What Is Space Junk?In 1957, the Soviet Union launched the first artificial satellite. The United States followed suit, and thus began the human race’s great space invasion.Over the past 52 years, a variety of spacecraft, including space capsules, telescopes, and satellites, have been sent beyond Earth’s atmosphere. They explore the vast reaches of our solar system, monitor atmospheric conditions, and make global wireless communication possible. The rockets that are used to power these spacecraft typically fall back to Earth and disintegrate in the intense heat that results from friction with Earth’s atmosphere. The objects themselves, however, are positioned hundreds of miles above Earth, far from elements that would cause them to degrade or burn up. In this airless environment, some of them continue to circle the planet indefinitely. While this is ideal for a fully functioning object that was launched for that purpose—for example, a communications satellite—what happens when a satellite “dies” or malfunctions and can’t be repaired? The disabled object becomes a piece of high-tech junk, circling the globe in uncontrolled orbit.With no one at the controls, dead satellites run the risk of colliding with each other. That’s exactly what happened in February 2009. Two communications satellites, one American and one Russian, both traveling at more than 20,000 miles per hour, crashed into each other 491 miles above the Earth. The impact created hundreds of pieces of debris, each assuming its own orbital path. Now, instead of two disabled satellites, there are hundreds of microsatellites flying through space. It’s not only spectacular crashes that create debris. Any objects released into space become free-orbiting satellites, which means that astronauts must take great care when they leave their spacecraft to make repairs or do experiments. Still, accidents do happen: in 2008, a tool bag escaped from the grip of an astronaut doing repairs on the International Space Station (ISS). Little Bits, But a Big Deal. So who cares about a lost tool bag or tiny bits of space trash? Actually, many people do. Those bits of space debris present a very serious problem. Tiny fragments traveling at a speed of five miles per second can inflict serious damage on the most carefully designed spacecraft. If you find that hard to believe, compare grains of sand blown by a gentle breeze to those shot from a sandblaster to strip paint from a concrete wall. At extreme speeds, little bits can pack a punch powerful enough to create disastrous holes in an object moving through space. Scientists are hard-pressed for an easy solution to the problem of space junk. Both the National Aeronautics and Space Agency (NASA) and the European Space Agency maintain catalogues of known objects. The lost tool bag, for example, is listed as Satellite 33442. But while military radar can identify objects the size of a baseball, anything smaller goes undetected. This makes it difficult for spacecraft to steer clear of microdebris fields. Accepting the inevitability of contact, engineers have added multiple walls to spacecraft and stronger materials to spacesuits to diminish the effects of impact.Yet the problem is certain to persist. In fact, the amount of space trash is actually increasing because commercial space travel is on the rise and more nations have undertaken space exploration. Space agencies hope that the corporations and nations involved can work together to come up with a viable solution to space pollution. ',\n",
       " 'Black. The doghouse will be warmer. The black lid made the jar warmest. Dark gray. The inside will be a little warmer, but not too hot. The dark gray lid increased 6º C more than the white. Light gray. The inside will stay cooler, but not too cool. The light gray lid was 8º C cooler than the black. White. The inside will be cooler. The white lid only went up to 42º C.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''BUSHNELL, Fla—RobRoy Maclnnes is the man to see if you want to buy a crocodile. Or a scorpion, a rattlesnake, a boa constrictor. Got a hankering for a cobra? Just pony up $600 and you can have one of the more lethal species.\"It is a very effective threat display,\" Maclnnes, 49, says as a Pakistan black cobra, six feet long, hissing, hood spread, writhes in its enclosure and strikes again and again and again at the thin glass separating the creature from a visitor. \"A snake like that, coining at you, you would leave him alone.\" Or simply die of fright. Maclnnes is co-owner of Glades Herp Farms, an empire of claws, spines, scales, fangs and darting tongues. The reptile trade, he is happy to report, is booming. The pet industry estimates that about 4.8 million households now contain at least one pet reptile, a number that has nearly doubled in a decade. Reptiles are increasingly popular in a crowded, urbanized nation. They do not need a yard. You do not have to take a lizard for a walk. But biologists see the trade in nonnative creatures as a factor in the rising number of invasive species, such as the Burmese python, which is breeding up a storm in the Everglades, and the Nile monitor lizard, a toothy carnivore that can reach seven feet in length and has found a happy home along the canals of Cape Coral. Under a new state law, a customer must obtain a $100 annual permit to buy a monitor lizard or some of the largest snakes-four species of pythons and the green anaconda. The animal must also be implanted with a microchip. That tag could help officials identify the animal if it turns up later in the wild. Maclnnes contends that the government overestimates the threat posed by invasive reptiles. He says he is being blocked by the U.S. Fish and Wildlife Service from importing some commercially attractive animals, such as Fiji island iguanas and radiated tortoises from Madagascar. Even the term \"invasive species\" is unfair, he said. \"They are introduced.\" I think that invasive is passing judgment.Of the pythons, he said: \"To me, it is a wonderful introduction. I think it is the best thing to happen to the Everglades in the last 200 years.\" Biologists, however, say that invasive species, unchecked by natural predators, are major threats to biodiversity. Life on Earth has always moved around, but never so fast. Organisms evolve in niche environments. What happens when the natural barriers are removed? When anything can go anywhere?Complications ensue. Skip Snow, a wildlife biologist for Everglades National Park, has helped drag hundreds of Burmese pythons out of the weeds, of roadways and even from under the hood of a tourist's car. He calls Maclnnes's argument \"ridiculous.\" The snakes, he says, are imperiling five endangered species in the Florida Keys, including the Key Largo wood rat, one specimen of which, tagged with a radio transmitter, was tracked all the way to the belly of a python. No one knows how the snakes went native, but there's speculation that Hurricane Andrew, which obliterated thousands of homes, played a factor in a wholesale python jailbreak in 1992. Many invasive species undergo a lag before proliferating. What's certain is that, by 2002, pythons were seen in multiple locations in remote regions of the Everglades. Then one morning in early 2003 a bunch of tourists on the park's Anhinga Trail, a reliable location for viewing wildlife, were startled to see an alligator with a python in its mouth. Even more dramatic was what happened in the Everglades in 2005: A python swallowed an alligator and—there’s not a delicate way to put it—exploded. The photograph ran around the world; it wasn't pretty, but you had to look. This February, the U.S. Geological Survey reported that pythons in Asia inhabit climates that are similar to those in about a third of the continental United States. A USGS map showed potential python habitat stretching from California to Delaware and including much of the South. You could conceivably have pythons snacking their way right up the Potomac. The map wasn't a prediction of where the snakes will actually spread, however. Media coverage of it was overly sensational, argues the map’s co-author, Robert Reed. \"When was the last snake story that didn't get sensationalized?\" he asked. \"Ecophobia is playing a role,\" said Jamie K. Reaser, a science and policy adviser to the Pet Industry Joint Advisory Council. \"Mammals are warm and fuzzy. Birds tend to have quite a following. But animals such as lizards and snakes tend, at least in this culture, to be less well respected or supported.\" What is happening in Florida illustrates a broader fact about life on Earth: We live in an age that favors generalists rather than specialists. A generalist is a raccoon, a python, a cockroach, a white-tailed deer. The ultimate generalist is, arguably, a human being, who with the assistance of technology can live anywhere from Florida to Antarctica to outer space. It's no accident that the species that have become most abundant are often those that do best in and around humans. A specialist is China's panda, which eats almost nothing but bamboo, or Australia's koala bear, which eats eucalyptus leaves almost exclusively. Maclnnes is not without an environmental conscience. \"We're degrading the Earth at an alarming rate,\" he said. \"Will man go extinct before we reach the point where we figure it out?\" He added: \"What favors generalists is change. What favors specialists is stability. Right now, mankind has chosen to make Earth a rapidly changing place.\" Down in the Everglades, Skip Snow would agree with that part of Maclnnes's philosophy. We are all part of a vast experiment in the blending of organisms from around the world, he said. \"The thing about the experiment is, it's not planned, and there's no one in control,\" Snow added. \"It's an experiment run amok.\"''', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Selective permeability is used by the cell membrane to allow certain substances to move across. Passive transport occurs when substances move from an area of higher concentration to an area of lower concentration. Osmosis is the diffusion of water across the cell membrane. Facilitated diffusion occurs when the membrane controls the pathway for a particle to enter or leave a cell. Active transport occurs when a cell uses energy to move a substance across the cell membrane, and/or a substance moves from an area of low to high concentration, or against the concentration gradient. Pumps are used to move charged particles like sodium and potassium ions through membranes using energy and carrier proteins. Membrane-assisted transport occurs when the membrane of the vesicle fuses with the cell membrane forcing large molecules out of the cell as in exocytosis. Membrane-assisted transport occurs when molecules are engulfed by the cell membrane as in endocytosis. Membrane-assisted transport occurs when vesicles are formed around large molecules as in phagocytosis. Membrane-assisted transport occurs when vesicles are formed around liquid droplets as in pinocytosis. Protein channels or channel proteins allow for the movement of specific molecules or substances into or out of the cell.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Rose’s head jerked up from her chest. “Oh no,” she groaned, rubbing the back of her neck and blinking at the bright light in the kitchen. For a split second she was confused. Then she remembered: her essay for the state competition. She’d been struggling to think of a topic. Her brain must have surrendered to exhaustion.The day, like most of her days, had been too long, too demanding. From school she’d gone straight to the restaurant to work a four-hour shift, then straight home to help Aunt Kolab prepare a quick supper. After that it was time to do homework.  When would she squeeze in writing a flawless three-thousand-word essay? “I’m insane,” she said grimly as she gathered books and papers.  Even if I win, she thought, I won’t get to travel to Sacramento to receive the prize. She’d already had to miss a lot of shifts, and her supervisor was on the verge of firing her. Her younger sister walked in rubbing her eyes.  “Anna,” Rose said. “What’s wrong? You feel okay?” “I’m fine,” her sister said.  “I just had another bad dream.” “I fell asleep working on my essay,” Rose said.  Anna poured two glasses of orange juice and handed one to Rose. “Mama’s not home yet, is she.” It wasn’t a question. “I hate how late she has to work.” Her voice sank to a fierce whisper. “I’m so lonesome for Papa. It seems like he’s been gone for years.” “It’s only been four months,” Rose said as gently as she could. “He had to go. The job in Los Angeles paid three times what he was making here.” Anna glared at Rose.  “Money isn’t everything.”  “Only if you already have everything,” Rose said. She tried a laugh that sounded fake even to her. “We have our part to do to help Paul finish  college. Then he’ll get a good job, Anna, and he’ll pay for you and me to go to college.” Anna rolled her eyes and shoved her chai r away from the table. “You sound just like Mama.” She stood and stalked out of the kitchen. By the time Rose tiptoed into their room, Anna was already snoring lightly. Rose slid into bed and watched the lights from passing cars move across the walls. They became the lights that had illuminated the stage at Paul’s high school graduation. As her brother accepted his diploma, Rose had glanced at her parents’ faces. Four eyes shining with tears. The work, the sheer weight of it, to get him on that stage slid from them in that moment; only a sweet, triumphant ache remained. Surely they remembered the ship, their young son and daughters clinging to their necks, Cambodia behind them, the United States before them. On that ship perhaps they had imagined their children’s futures, imagined this very day would come. In the dark Rose clasped then cupped her hands.  Paul’s fate lies partly in these, she thought. She felt too young for so much responsibility. Then she shivered, imagining how her brother must feel. Only three years older, he held the fate of two people—both his sisters—in his hands. Rose dreamed that she swam through clear, green-tinted water, enjoying the pure simplicity of a fish’s life. She stopped moving and looked up. She saw Paul jump from a boulder and crash into the water just above her. His body sank as if it were made of stone, pushing her beneath him down to the sandy bottom. She struggled to get out from under him, but he seemed unaware of her. When she opened her mouth to scream get off, water rushed in. Rose woke gasping for air. The walls of her room were bathed in pale sunlight. When her heart had slowed back down, she got up. Anna was still asleep. In the hall Rose stopped at her mother’s room. She was also sleeping. So it was Aunt Kolab making the muted noises coming from the kitchen. “Good morning, Rose,” her aunt said. Rose felt an urgent need to relate the dream, to expose it so it would loosen its grip on her. After she’d finished, her aunt said, with a puzzled look, “Do you feel so weighed down by what you’re doing to help this family?” Rose didn’t answer. If she told the truth, she would hurt her aunt. And probably her aunt would tell her mother.  “In Cambodia, our first country, what we’re all doing would be quite normal,” her aunt said. “But now I realize that you’re seeing the situation through other eyes—as you should, I suppose, because you grew up here. This must be difficult for you. Yes?” Rose nodded. “Hmm. Maybe we can find a way to do things differently. A way better for you.” Her aunt’s face lit up. “Maybe I can sew for ladies. Or I could make special treats from our country and sell them.” Rose kept nodding. Maybe her life would get easier. Maybe it wouldn’t. But her aunt’s offer had somehow made her feel lighter. Suddenly, it occurred to her: here was the topic for her essay, although it was still vague. Cambodian tradition and sense of family, she realized, could survive an ocean crossing.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''I met Mr. Leonard when I started middle school. He was a hall monitor whose job it was to keep students moving along from one classroom to the next. “Move along, people, move along!” he’d advise the shuffling crowd, and everyone complied. I distinguished myself from the masses by being one of a select few in the remedial reading program. Twice a week, I left English class early for the learning center in the basement, where I worked with a tutor. On my first trip, Mr. Leonard confronted me in the stairwell. “Hey, my friend, where do you think you’re going?” he asked, arms folded across his chest.“Learning center,” I muttered, showing him my hall pass.“Why?” he asked from behind a hard stare.“Why?” I answered automatically, “I can’t read.” His gaze softened. “Fair enough. On your way, then. Work hard.” For the next few weeks, that was the extent of our conversations. He’d meet me in the stairwell, I’d show him my pass. Then one day he surprised me by asking what I did after school. “Nothing,” I answered. “Just some homework.”“Meet me in the gym. 2:30.” Since this gave me a legitimate reason to delay my daily homework battles, I agreed. When I arrived, the gym was crowded with kids warming up for intramurals. Mr. Leonard was seated in a corner, watching. He waved me over, then pointed at the kids chasing basketballs. “None of this appeals to you?” he asked. I shook my head. When you’re the last guy chosen for teams in gym class, you don’t seek out more of that treatment after school. “Follow me,” he directed, and, obediently, I followed. We left the building and went to the track. Spread along the inside lane were hurdles. Mr. Leonard pointed at the closest one.“Know what that thing is called?” he asked. “A hurdle,” I answered.“Know what to do with it?” he questioned. “You jump it,” I replied.“Well?” he responded. “On your way then.” It never occurred to me to refuse--perhaps I’d been conditioned by hearing those words every day. I got into a slow jog and awkwardly hopped over each barrier for a whole lap.“Not a bad first effort,” commented Mr. Leonard as I staggered in. “That was terrible,” I gasped.“You’ll do better next time,” he responded. “Bring sneakers and shorts tomorrow.”“Right,” I panted. Mr. Leonard began walking back toward the school, then turned and asked, “Say, what’s your name?”“Paul.” It didn’t occur to me until later that this was an odd question for someone who had checked my hall pass twice a week. And so it began. Monday through Friday, rain or shine, I was out on the track with Mr. Leonard shouting from the side. “Open your stride!” “Pump your arms!” “Lean, . . .  NOW!”  I improved steadily until one day I found myself standing before the high school track coach. “How’d you get so fast, son?” he asked. “Well, I’ve been training,” I replied. “Someone’s helping me.”“Mr. Leonard Grabowski?” I nodded. The coach smiled and asked me to work out with the high school team. Then he scribbled on a scrap of paper and handed it me. It was a URL for a track and field website. “Visit this site. Do a search for ‘Grabowski.’” The next day, I told Mr. Leonard about my conversation with the coach and asked if he thought I should work out with the team. “Absolutely,” he replied with a grin. “A little competition will only help.” I pulled the printout I’d downloaded the night before from my pocket. “Why didn’t you tell me about this?” He looked at me quizzically, then smiled sadly at the image on the page. “I looked good back then, didn’t I?” he chuckled. I moved beside him and pointed to the photograph. “You were a college freshman who won the 400 meter hurdles at the nationals. You broke records.” “I remember,” he said solemnly. “Best race of my life.” “Well, what happened after that?” I pressed.Mr. Leonard handed the paper back and looked at the ground, his brow furrowed, his voice cracked as he spoke.“I was a good athlete,” he said softly, “but not a good student.  We had no learning centers in our school. I relied on friends to help me get by, but even then the work was always too hard.”  His voice trailed off. “But you went to college,” I said.“Things were different back then,” he replied. “The college scouts told me that my grades didn’t matter, that I’d have tutors to help me, but college work is a whole lot harder than high school work. I lost my scholarship and flunked out. No other school wanted a runner who couldn’t read.” The emotions in Mr. Leonard’s words were all too familiar to me. I knew them well--feelings of embarrassment when I was called upon to read aloud or when I didn’t know an answer everyone else knew. This man had given his time to help me excel at something. Suddenly I realized what I could do for him. “C’mon, Mr. Leonard,” I said, walking back toward school. “It’s time to start your training.”'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Grab your telescope! Look up in the sky! It’s a comet! It’s a meteor! It’s a tool bag?Such an observation isn’t as strange as it seems. Orbital pathways around our planet that were once clear are now cluttered with the remains of numerous space exploration and satellite missions. This “space junk” is currently of great concern to government space agencies around the globe.What Is Space Junk?In 1957, the Soviet Union launched the first artificial satellite. The United States followed suit, and thus began the human race’s great space invasion.Over the past 52 years, a variety of spacecraft, including space capsules, telescopes, and satellites, have been sent beyond Earth’s atmosphere. They explore the vast reaches of our solar system, monitor atmospheric conditions, and make global wireless communication possible. The rockets that are used to power these spacecraft typically fall back to Earth and disintegrate in the intense heat that results from friction with Earth’s atmosphere. The objects themselves, however, are positioned hundreds of miles above Earth, far from elements that would cause them to degrade or burn up. In this airless environment, some of them continue to circle the planet indefinitely. While this is ideal for a fully functioning object that was launched for that purpose—for example, a communications satellite—what happens when a satellite “dies” or malfunctions and can’t be repaired? The disabled object becomes a piece of high-tech junk, circling the globe in uncontrolled orbit.With no one at the controls, dead satellites run the risk of colliding with each other. That’s exactly what happened in February 2009. Two communications satellites, one American and one Russian, both traveling at more than 20,000 miles per hour, crashed into each other 491 miles above the Earth. The impact created hundreds of pieces of debris, each assuming its own orbital path. Now, instead of two disabled satellites, there are hundreds of microsatellites flying through space. It’s not only spectacular crashes that create debris. Any objects released into space become free-orbiting satellites, which means that astronauts must take great care when they leave their spacecraft to make repairs or do experiments. Still, accidents do happen: in 2008, a tool bag escaped from the grip of an astronaut doing repairs on the International Space Station (ISS). Little Bits, But a Big Deal. So who cares about a lost tool bag or tiny bits of space trash? Actually, many people do. Those bits of space debris present a very serious problem. Tiny fragments traveling at a speed of five miles per second can inflict serious damage on the most carefully designed spacecraft. If you find that hard to believe, compare grains of sand blown by a gentle breeze to those shot from a sandblaster to strip paint from a concrete wall. At extreme speeds, little bits can pack a punch powerful enough to create disastrous holes in an object moving through space. Scientists are hard-pressed for an easy solution to the problem of space junk. Both the National Aeronautics and Space Agency (NASA) and the European Space Agency maintain catalogues of known objects. The lost tool bag, for example, is listed as Satellite 33442. But while military radar can identify objects the size of a baseball, anything smaller goes undetected. This makes it difficult for spacecraft to steer clear of microdebris fields. Accepting the inevitability of contact, engineers have added multiple walls to spacecraft and stronger materials to spacesuits to diminish the effects of impact.Yet the problem is certain to persist. In fact, the amount of space trash is actually increasing because commercial space travel is on the rise and more nations have undertaken space exploration. Space agencies hope that the corporations and nations involved can work together to come up with a viable solution to space pollution. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-56-e782d55ec302>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-56-e782d55ec302>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Black. The doghouse will be warmer. The black lid made the jar warmest. Dark gray. The inside will be a little warmer, but not too hot. The dark gray lid increased 6º C more than the white. Light gray. The inside will stay cooler, but not too cool. The light gray lid was 8º C cooler than the black. White. The inside will be cooler. The white lid only went up to 42º C.\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''Black. The doghouse will be warmer. The black lid made the jar warmest. Dark gray. The inside will be a little warmer, but not too hot. The dark gray lid increased 6º C more than the white. Light gray. The inside will stay cooler, but not too cool. The light gray lid was 8º C cooler than the black. White. The inside will be cooler. The white lid only went up to 42º C.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.Series( (v for v in ref_ans) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    some additional information that we would need...\n",
       "Name: clean, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean'][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = [len(i) for i in data['clean']]\n",
    "max_length = (max(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "for i in data['clean']:\n",
    "    all_data.append(i)\n",
    "for i in ref_ans:\n",
    "    all_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(all_data)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4ca6cb8b5b22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mX_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mX_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-ffde8f436b1b>\u001b[0m in \u001b[0;36mencode_text\u001b[0;34m(tokenizer, lines, length)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_token = []\n",
    "for i in range(len(data)):\n",
    "    X_tok = encode_text(tokenizer, data['clean'],max_length)\n",
    "    X_token.append(X_tok)\n",
    "X_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = pd.DataFrame({'X_token' : X_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = [len(i) for i in ref_ans]\n",
    "max_length_1 = (max(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ref = encode_text(tokenizer, ref_ans,max_length_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([32, 49,  2, ...,  0,  0,  0], dtype=int32),\n",
       " array([ 41, 169,  81, ...,   0,   0,   0], dtype=int32),\n",
       " array([7171, 7172, 2400, ...,    0,    0,    0], dtype=int32),\n",
       " array([7171, 7172, 2400, ...,    0,    0,    0], dtype=int32),\n",
       " array([  86, 2136,  180, ...,    0,    0,    0], dtype=int32),\n",
       " array([1430, 2476,    4, ...,    0,    0,    0], dtype=int32),\n",
       " array([16046,  1471,  6612, ...,     0,     0,     0], dtype=int32),\n",
       " array([  34, 2294,   22, ...,    0,    0,    0], dtype=int32),\n",
       " array([1043,  285, 1319, ...,    0,    0,    0], dtype=int32),\n",
       " array([ 89,   1, 144, ...,   0,   0,   0], dtype=int32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1672"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data['EssaySet']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[        Id  EssaySet  Score1  Score2  \\\n",
       " 0        1         1       1       1   \n",
       " 1        2         1       1       1   \n",
       " 2        3         1       1       1   \n",
       " 3        4         1       0       0   \n",
       " 4        5         1       2       2   \n",
       " 5        6         1       1       0   \n",
       " 6        7         1       1       0   \n",
       " 7        8         1       3       3   \n",
       " 8        9         1       3       3   \n",
       " 9       10         1       2       2   \n",
       " 10      11         1       0       0   \n",
       " 11      12         1       3       3   \n",
       " 12      13         1       2       2   \n",
       " 13      14         1       2       2   \n",
       " 14      15         1       0       0   \n",
       " 15      16         1       0       0   \n",
       " 16      17         1       3       3   \n",
       " 17      18         1       0       0   \n",
       " 18      19         1       0       0   \n",
       " 19      20         1       3       3   \n",
       " 20      21         1       0       0   \n",
       " 21      22         1       2       2   \n",
       " 22      23         1       0       0   \n",
       " 23      24         1       3       3   \n",
       " 24      25         1       1       1   \n",
       " 25      26         1       2       2   \n",
       " 26      27         1       2       2   \n",
       " 27      28         1       0       0   \n",
       " 28      29         1       3       3   \n",
       " 29      30         1       2       0   \n",
       " ...    ...       ...     ...     ...   \n",
       " 1642  1643         1       2       2   \n",
       " 1643  1644         1       1       1   \n",
       " 1644  1645         1       1       1   \n",
       " 1645  1646         1       1       1   \n",
       " 1646  1647         1       3       3   \n",
       " 1647  1648         1       0       0   \n",
       " 1648  1649         1       2       2   \n",
       " 1649  1650         1       0       0   \n",
       " 1650  1651         1       1       1   \n",
       " 1651  1652         1       1       1   \n",
       " 1652  1653         1       0       0   \n",
       " 1653  1654         1       2       2   \n",
       " 1654  1655         1       2       2   \n",
       " 1655  1656         1       0       0   \n",
       " 1656  1657         1       2       2   \n",
       " 1657  1658         1       3       3   \n",
       " 1658  1659         1       3       3   \n",
       " 1659  1660         1       3       3   \n",
       " 1660  1661         1       3       3   \n",
       " 1661  1662         1       2       2   \n",
       " 1662  1663         1       0       0   \n",
       " 1663  1664         1       3       3   \n",
       " 1664  1665         1       2       2   \n",
       " 1665  1666         1       2       2   \n",
       " 1666  1667         1       2       2   \n",
       " 1667  1668         1       3       3   \n",
       " 1668  1669         1       0       0   \n",
       " 1669  1670         1       0       0   \n",
       " 1670  1671         1       1       1   \n",
       " 1671  1672         1       1       1   \n",
       " \n",
       "                                               EssayText  \\\n",
       " 0     Some additional information that we would need...   \n",
       " 1     After reading the expirement, I realized that ...   \n",
       " 2     What you need is more trials, a control set up...   \n",
       " 3     The student should list what rock is better an...   \n",
       " 4     For the students to be able to make a replicat...   \n",
       " 5     I would need the information of why you would ...   \n",
       " 6     The information I would need in order to suces...   \n",
       " 7     You would need many more pieces of information...   \n",
       " 8     Some additional information you will need are ...   \n",
       " 9     Inorder to replicate the experiment, we will n...   \n",
       " 10    An additional information that i would need in...   \n",
       " 11    The additional infomation you would need to re...   \n",
       " 12    There are two pieces of additional information...   \n",
       " 13       In order the replicate the experiment you need   \n",
       " 14    Well what i understand about this procedure is...   \n",
       " 15                       I don't know what is going on!   \n",
       " 16    In order to replicate this experiment, I would...   \n",
       " 17    The additional information I would need is to ...   \n",
       " 18    In order to replicate this experiment I would ...   \n",
       " 19    In order for the me to replicate their procedu...   \n",
       " 20    You would need to have four seperate but ident...   \n",
       " 21    Inorder to replicate the experiment, I would n...   \n",
       " 22    The students data needed to include how much o...   \n",
       " 23    For this experiments duplication, you would ne...   \n",
       " 24    Some additional information that I would need ...   \n",
       " 25    After reading the groups procedure I would nee...   \n",
       " 26    They needed to include what time they started ...   \n",
       " 27    1). Get 4 different samples: marble, limestone...   \n",
       " 28    In order to replicate this experiment, you wou...   \n",
       " 29    In step three, you would need to know how much...   \n",
       " ...                                                 ...   \n",
       " 1642  To replicate the experiment, the first thing t...   \n",
       " 1643  In order to replicate this experiment, we woul...   \n",
       " 1644  The additional information that would be neede...   \n",
       " 1645  In their procedure, the studants weren't very ...   \n",
       " 1646  The additional information that i would need t...   \n",
       " 1647  In order to replicate the experiment, one woul...   \n",
       " 1648  The experimental procedure does not specify th...   \n",
       " 1649  In order to replicate the experiment, you woul...   \n",
       " 1650  After reading the group procedure I would add ...   \n",
       " 1651  I would need to know the amount of vinegar tha...   \n",
       " 1652  In this experiment you will need to add more i...   \n",
       " 1653  If you were to replicate the experiment descri...   \n",
       " 1654  To repeat this experiment you would have to kn...   \n",
       " 1655      Determine the mass of four different samples.   \n",
       " 1656  I would need to know the amount of vinegar to ...   \n",
       " 1657  To replicate this procedure I'll need to know ...   \n",
       " 1658  The additional information I need in order to ...   \n",
       " 1659  You would need to know the amount of vinegar p...   \n",
       " 1660  In order to replicate the experiment I would n...   \n",
       " 1661  Three additional pieces of information i would...   \n",
       " 1662  I think they would need safety goggles and glo...   \n",
       " 1663  In order to replicate the \"acid rain\" experime...   \n",
       " 1664  In order to replicate this experiment, more in...   \n",
       " 1665  You would need to know what to make the four s...   \n",
       " 1666  One piece of information you would need to rep...   \n",
       " 1667  The additional information you would need to k...   \n",
       " 1668  Number 2, pour vinegar in eatch of the four se...   \n",
       " 1669  To determine the mass of four different sample...   \n",
       " 1670  We would need to replicate steps 1-6 for sampl...   \n",
       " 1671  In order to replicate this experiment, we woul...   \n",
       " \n",
       "                                                   clean  \\\n",
       " 0     some additional information that we would need...   \n",
       " 1     after reading the expirement , i realized that...   \n",
       " 2     what you need is more trials , a control set u...   \n",
       " 3     the student should list what rock is better an...   \n",
       " 4     for the students to be able to make a replicat...   \n",
       " 5     i would need the information of why you would ...   \n",
       " 6     the information i would need in order to suces...   \n",
       " 7     you would need many more pieces of information...   \n",
       " 8     some additional information you will need are ...   \n",
       " 9     inorder to replicate the experiment , we will ...   \n",
       " 10    an additional information that i would need in...   \n",
       " 11    the additional infomation you would need to re...   \n",
       " 12    there are two pieces of additional information...   \n",
       " 13       in order the replicate the experiment you need   \n",
       " 14    well what i understand about this procedure is...   \n",
       " 15                     i do n't know what is going on !   \n",
       " 16    in order to replicate this experiment , i woul...   \n",
       " 17    the additional information i would need is to ...   \n",
       " 18    in order to replicate this experiment i would ...   \n",
       " 19    in order for the me to replicate their procedu...   \n",
       " 20    you would need to have four seperate but ident...   \n",
       " 21    inorder to replicate the experiment , i would ...   \n",
       " 22    the students data needed to include how much o...   \n",
       " 23    for this experiments duplication , you would n...   \n",
       " 24    some additional information that i would need ...   \n",
       " 25    after reading the groups procedure i would nee...   \n",
       " 26    they needed to include what time they started ...   \n",
       " 27    ) get  different samples marble , limestone , ...   \n",
       " 28    in order to replicate this experiment , you wo...   \n",
       " 29    in step three , you would need to know how muc...   \n",
       " ...                                                 ...   \n",
       " 1642  to replicate the experiment , the first thing ...   \n",
       " 1643  in order to replicate this experiment , we wou...   \n",
       " 1644  the additional information that would be neede...   \n",
       " 1645  in their procedure , the studants were n't ver...   \n",
       " 1646  the additional information that i would need t...   \n",
       " 1647  in order to replicate the experiment , one wou...   \n",
       " 1648  the experimental procedure does not specify th...   \n",
       " 1649  in order to replicate the experiment , you wou...   \n",
       " 1650  after reading the group procedure i would add ...   \n",
       " 1651  i would need to know the amount of vinegar tha...   \n",
       " 1652  in this experiment you will need to add more i...   \n",
       " 1653  if you were to replicate the experiment descri...   \n",
       " 1654  to repeat this experiment you would have to kn...   \n",
       " 1655       determine the mass of four different samples   \n",
       " 1656  i would need to know the amount of vinegar to ...   \n",
       " 1657  to replicate this procedure i 'll need to know...   \n",
       " 1658  the additional information i need in order to ...   \n",
       " 1659  you would need to know the amount of vinegar p...   \n",
       " 1660  in order to replicate the experiment i would n...   \n",
       " 1661  three additional pieces of information i would...   \n",
       " 1662  i think they would need safety goggles and glo...   \n",
       " 1663  in order to replicate the acid rain experiment...   \n",
       " 1664  in order to replicate this experiment , more i...   \n",
       " 1665  you would need to know what to make the four s...   \n",
       " 1666  one piece of information you would need to rep...   \n",
       " 1667  the additional information you would need to k...   \n",
       " 1668  number  , pour vinegar in eatch of the four se...   \n",
       " 1669  to determine the mass of four different sample...   \n",
       " 1670  we would need to replicate steps   for samples...   \n",
       " 1671  in order to replicate this experiment , we wou...   \n",
       " \n",
       "                                                       X  \\\n",
       " 0     [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       " 1     [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       " 2     [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       " 3     [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       " 4     [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       " 5     [34, 15, 49, 1, 67, 7, 171, 32, 15, 408, 1, 57...   \n",
       " 6     [1, 67, 34, 15, 49, 5, 168, 2, 7093, 147, 1, 5...   \n",
       " 7     [32, 15, 49, 206, 55, 535, 7, 67, 2, 147, 1, 5...   \n",
       " 8     [163, 299, 67, 32, 66, 49, 11, 1, 384, 32, 41,...   \n",
       " 9     [2535, 2, 147, 1, 54, 188, 66, 49, 2, 324, 67,...   \n",
       " 10    [71, 299, 67, 8, 34, 15, 49, 5, 168, 2, 147, 1...   \n",
       " 11    [1, 299, 2655, 32, 15, 49, 2, 147, 1, 54, 15, ...   \n",
       " 12    [103, 11, 157, 535, 7, 299, 67, 5287, 2, 147, ...   \n",
       " 13    [5, 168, 1, 147, 1, 54, 32, 49, 0, 0, 0, 0, 0,...   \n",
       " 14    [200, 23, 34, 417, 33, 18, 195, 4, 8, 32, 332,...   \n",
       " 15    [34, 69, 50, 68, 23, 4, 209, 27, 0, 0, 0, 0, 0...   \n",
       " 16    [5, 168, 2, 147, 18, 54, 34, 15, 49, 2, 68, 1,...   \n",
       " 17    [1, 299, 67, 34, 15, 49, 4, 2, 391, 127, 28, 7...   \n",
       " 18    [5, 168, 2, 147, 18, 54, 34, 15, 49, 6, 3522, ...   \n",
       " 19    [5, 168, 20, 1, 341, 2, 147, 99, 195, 34, 15, ...   \n",
       " 20    [32, 15, 49, 2, 29, 196, 998, 52, 835, 184, 32...   \n",
       " 21    [2535, 2, 147, 1, 54, 34, 15, 49, 2, 68, 1, 15...   \n",
       " 22    [1, 178, 151, 269, 2, 459, 16, 70, 7, 1, 796, ...   \n",
       " 23    [20, 18, 1278, 3897, 32, 15, 49, 2, 68, 525, 1...   \n",
       " 24    [163, 299, 67, 8, 34, 15, 49, 4, 1, 142, 7, 98...   \n",
       " 25    [114, 225, 1, 807, 195, 34, 15, 49, 2, 68, 23,...   \n",
       " 26    [10, 269, 2, 459, 23, 145, 10, 584, 3, 48, 10,...   \n",
       " 27    [109, 57, 110, 667, 651, 491, 40, 139, 176, 73...   \n",
       " 28    [5, 168, 2, 147, 18, 54, 32, 15, 49, 2, 68, 23...   \n",
       " 29    [5, 272, 242, 32, 15, 49, 2, 68, 16, 70, 7, 73...   \n",
       " ...                                                 ...   \n",
       " 1642  [2, 147, 1, 54, 1, 120, 170, 8, 6, 352, 15, 49...   \n",
       " 1643  [5, 168, 2, 147, 18, 54, 188, 15, 49, 2, 68, 1...   \n",
       " 1644  [1, 299, 67, 8, 15, 14, 269, 4, 6, 237, 77, 45...   \n",
       " 1645  [5, 99, 195, 1, 7735, 132, 50, 104, 778, 27, 2...   \n",
       " 1646  [1, 299, 67, 8, 34, 15, 49, 2, 147, 18, 54, 4,...   \n",
       " 1647  [5, 168, 2, 147, 1, 54, 44, 15, 49, 2, 68, 163...   \n",
       " 1648  [1, 457, 195, 130, 37, 721, 1, 142, 7, 98, 2, ...   \n",
       " 1649  [5, 168, 2, 147, 1, 54, 32, 15, 49, 2, 68, 33,...   \n",
       " 1650  [114, 225, 1, 352, 195, 34, 15, 324, 23, 1, 19...   \n",
       " 1651  [34, 15, 49, 2, 68, 1, 142, 7, 98, 8, 4, 148, ...   \n",
       " 1652  [5, 18, 54, 32, 66, 49, 2, 324, 55, 67, 74, 5,...   \n",
       " 1653  [79, 32, 132, 2, 147, 1, 54, 918, 5, 1, 195, 3...   \n",
       " 1654  [2, 698, 18, 54, 32, 15, 29, 2, 68, 16, 70, 7,...   \n",
       " 1655  [460, 1, 153, 7, 196, 57, 110, 0, 0, 0, 0, 0, ...   \n",
       " 1656  [34, 15, 49, 2, 68, 1, 142, 7, 98, 2, 176, 5, ...   \n",
       " 1657  [2, 147, 18, 195, 34, 607, 49, 2, 68, 1, 359, ...   \n",
       " 1658  [1, 299, 67, 34, 49, 5, 168, 2, 147, 1, 54, 4,...   \n",
       " 1659  [32, 15, 49, 2, 68, 1, 142, 7, 98, 579, 5, 73,...   \n",
       " 1660  [5, 168, 2, 147, 1, 54, 34, 15, 49, 2, 68, 1, ...   \n",
       " 1661  [242, 299, 535, 7, 67, 34, 15, 49, 2, 14, 290,...   \n",
       " 1662  [34, 166, 10, 15, 49, 1733, 3005, 3, 3242, 2, ...   \n",
       " 1663  [5, 168, 2, 147, 1, 337, 985, 54, 9, 4, 2657, ...   \n",
       " 1664  [5, 168, 2, 147, 18, 54, 55, 67, 4, 269, 9, 15...   \n",
       " 1665  [32, 15, 49, 2, 68, 23, 2, 97, 1, 196, 110, 53...   \n",
       " 1666  [44, 574, 7, 67, 32, 15, 49, 2, 147, 18, 54, 1...   \n",
       " 1667  [1, 299, 67, 32, 15, 49, 2, 68, 42, 1, 807, 19...   \n",
       " 1668  [701, 342, 98, 5, 5469, 7, 1, 196, 998, 37, 83...   \n",
       " 1669  [2, 460, 1, 153, 7, 196, 57, 110, 2, 342, 98, ...   \n",
       " 1670  [188, 15, 49, 2, 147, 490, 20, 110, 139, 3008,...   \n",
       " 1671  [5, 168, 2, 147, 18, 54, 188, 15, 49, 2, 68, 1...   \n",
       " \n",
       "                                                     X_1  \n",
       " 0     [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...  \n",
       " 1     [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...  \n",
       " 2     [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...  \n",
       " 3     [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...  \n",
       " 4     [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...  \n",
       " 5     [34, 15, 49, 1, 67, 7, 171, 32, 15, 408, 1, 57...  \n",
       " 6     [1, 67, 34, 15, 49, 5, 168, 2, 7093, 147, 1, 5...  \n",
       " 7     [32, 15, 49, 206, 55, 535, 7, 67, 2, 147, 1, 5...  \n",
       " 8     [163, 299, 67, 32, 66, 49, 11, 1, 384, 32, 41,...  \n",
       " 9     [2535, 2, 147, 1, 54, 188, 66, 49, 2, 324, 67,...  \n",
       " 10    [71, 299, 67, 8, 34, 15, 49, 5, 168, 2, 147, 1...  \n",
       " 11    [1, 299, 2655, 32, 15, 49, 2, 147, 1, 54, 15, ...  \n",
       " 12    [103, 11, 157, 535, 7, 299, 67, 5287, 2, 147, ...  \n",
       " 13    [5, 168, 1, 147, 1, 54, 32, 49, 0, 0, 0, 0, 0,...  \n",
       " 14    [200, 23, 34, 417, 33, 18, 195, 4, 8, 32, 332,...  \n",
       " 15    [34, 69, 50, 68, 23, 4, 209, 27, 0, 0, 0, 0, 0...  \n",
       " 16    [5, 168, 2, 147, 18, 54, 34, 15, 49, 2, 68, 1,...  \n",
       " 17    [1, 299, 67, 34, 15, 49, 4, 2, 391, 127, 28, 7...  \n",
       " 18    [5, 168, 2, 147, 18, 54, 34, 15, 49, 6, 3522, ...  \n",
       " 19    [5, 168, 20, 1, 341, 2, 147, 99, 195, 34, 15, ...  \n",
       " 20    [32, 15, 49, 2, 29, 196, 998, 52, 835, 184, 32...  \n",
       " 21    [2535, 2, 147, 1, 54, 34, 15, 49, 2, 68, 1, 15...  \n",
       " 22    [1, 178, 151, 269, 2, 459, 16, 70, 7, 1, 796, ...  \n",
       " 23    [20, 18, 1278, 3897, 32, 15, 49, 2, 68, 525, 1...  \n",
       " 24    [163, 299, 67, 8, 34, 15, 49, 4, 1, 142, 7, 98...  \n",
       " 25    [114, 225, 1, 807, 195, 34, 15, 49, 2, 68, 23,...  \n",
       " 26    [10, 269, 2, 459, 23, 145, 10, 584, 3, 48, 10,...  \n",
       " 27    [109, 57, 110, 667, 651, 491, 40, 139, 176, 73...  \n",
       " 28    [5, 168, 2, 147, 18, 54, 32, 15, 49, 2, 68, 23...  \n",
       " 29    [5, 272, 242, 32, 15, 49, 2, 68, 16, 70, 7, 73...  \n",
       " ...                                                 ...  \n",
       " 1642  [2, 147, 1, 54, 1, 120, 170, 8, 6, 352, 15, 49...  \n",
       " 1643  [5, 168, 2, 147, 18, 54, 188, 15, 49, 2, 68, 1...  \n",
       " 1644  [1, 299, 67, 8, 15, 14, 269, 4, 6, 237, 77, 45...  \n",
       " 1645  [5, 99, 195, 1, 7735, 132, 50, 104, 778, 27, 2...  \n",
       " 1646  [1, 299, 67, 8, 34, 15, 49, 2, 147, 18, 54, 4,...  \n",
       " 1647  [5, 168, 2, 147, 1, 54, 44, 15, 49, 2, 68, 163...  \n",
       " 1648  [1, 457, 195, 130, 37, 721, 1, 142, 7, 98, 2, ...  \n",
       " 1649  [5, 168, 2, 147, 1, 54, 32, 15, 49, 2, 68, 33,...  \n",
       " 1650  [114, 225, 1, 352, 195, 34, 15, 324, 23, 1, 19...  \n",
       " 1651  [34, 15, 49, 2, 68, 1, 142, 7, 98, 8, 4, 148, ...  \n",
       " 1652  [5, 18, 54, 32, 66, 49, 2, 324, 55, 67, 74, 5,...  \n",
       " 1653  [79, 32, 132, 2, 147, 1, 54, 918, 5, 1, 195, 3...  \n",
       " 1654  [2, 698, 18, 54, 32, 15, 29, 2, 68, 16, 70, 7,...  \n",
       " 1655  [460, 1, 153, 7, 196, 57, 110, 0, 0, 0, 0, 0, ...  \n",
       " 1656  [34, 15, 49, 2, 68, 1, 142, 7, 98, 2, 176, 5, ...  \n",
       " 1657  [2, 147, 18, 195, 34, 607, 49, 2, 68, 1, 359, ...  \n",
       " 1658  [1, 299, 67, 34, 49, 5, 168, 2, 147, 1, 54, 4,...  \n",
       " 1659  [32, 15, 49, 2, 68, 1, 142, 7, 98, 579, 5, 73,...  \n",
       " 1660  [5, 168, 2, 147, 1, 54, 34, 15, 49, 2, 68, 1, ...  \n",
       " 1661  [242, 299, 535, 7, 67, 34, 15, 49, 2, 14, 290,...  \n",
       " 1662  [34, 166, 10, 15, 49, 1733, 3005, 3, 3242, 2, ...  \n",
       " 1663  [5, 168, 2, 147, 1, 337, 985, 54, 9, 4, 2657, ...  \n",
       " 1664  [5, 168, 2, 147, 18, 54, 55, 67, 4, 269, 9, 15...  \n",
       " 1665  [32, 15, 49, 2, 68, 23, 2, 97, 1, 196, 110, 53...  \n",
       " 1666  [44, 574, 7, 67, 32, 15, 49, 2, 147, 18, 54, 1...  \n",
       " 1667  [1, 299, 67, 32, 15, 49, 2, 68, 42, 1, 807, 19...  \n",
       " 1668  [701, 342, 98, 5, 5469, 7, 1, 196, 998, 37, 83...  \n",
       " 1669  [2, 460, 1, 153, 7, 196, 57, 110, 2, 342, 98, ...  \n",
       " 1670  [188, 15, 49, 2, 147, 490, 20, 110, 139, 3008,...  \n",
       " 1671  [5, 168, 2, 147, 18, 54, 188, 15, 49, 2, 68, 1...  \n",
       " \n",
       " [1672 rows x 8 columns],         Id  EssaySet  Score1  Score2  \\\n",
       " 1672  2788         2       1       1   \n",
       " 1673  2789         2       3       3   \n",
       " 1674  2790         2       1       1   \n",
       " 1675  2791         2       1       1   \n",
       " 1676  2792         2       3       3   \n",
       " 1677  2793         2       2       3   \n",
       " 1678  2794         2       1       1   \n",
       " 1679  2795         2       2       1   \n",
       " 1680  2796         2       2       2   \n",
       " 1681  2797         2       3       3   \n",
       " 1682  2798         2       3       3   \n",
       " 1683  2799         2       1       2   \n",
       " 1684  2800         2       3       2   \n",
       " 1685  2801         2       3       3   \n",
       " 1686  2802         2       1       2   \n",
       " 1687  2803         2       3       3   \n",
       " 1688  2804         2       2       2   \n",
       " 1689  2805         2       0       0   \n",
       " 1690  2806         2       1       1   \n",
       " 1691  2807         2       3       3   \n",
       " 1692  2808         2       1       1   \n",
       " 1693  2809         2       0       0   \n",
       " 1694  2810         2       1       2   \n",
       " 1695  2811         2       3       3   \n",
       " 1696  2812         2       1       1   \n",
       " 1697  2813         2       1       0   \n",
       " 1698  2814         2       3       3   \n",
       " 1699  2815         2       2       2   \n",
       " 1700  2816         2       1       1   \n",
       " 1701  2817         2       3       3   \n",
       " ...    ...       ...     ...     ...   \n",
       " 2920  4036         2       1       2   \n",
       " 2921  4037         2       2       2   \n",
       " 2922  4038         2       0       1   \n",
       " 2923  4039         2       2       2   \n",
       " 2924  4040         2       3       3   \n",
       " 2925  4041         2       3       3   \n",
       " 2926  4042         2       0       0   \n",
       " 2927  4043         2       2       2   \n",
       " 2928  4044         2       2       2   \n",
       " 2929  4045         2       1       1   \n",
       " 2930  4046         2       3       2   \n",
       " 2931  4047         2       0       0   \n",
       " 2932  4048         2       1       1   \n",
       " 2933  4049         2       1       2   \n",
       " 2934  4050         2       2       2   \n",
       " 2935  4051         2       3       2   \n",
       " 2936  4052         2       1       1   \n",
       " 2937  4053         2       3       3   \n",
       " 2938  4054         2       3       3   \n",
       " 2939  4055         2       3       3   \n",
       " 2940  4056         2       0       0   \n",
       " 2941  4057         2       1       1   \n",
       " 2942  4058         2       0       0   \n",
       " 2943  4059         2       3       2   \n",
       " 2944  4060         2       1       1   \n",
       " 2945  4061         2       3       3   \n",
       " 2946  4062         2       2       2   \n",
       " 2947  4063         2       0       0   \n",
       " 2948  4064         2       2       2   \n",
       " 2949  4065         2       2       2   \n",
       " \n",
       "                                               EssayText  \\\n",
       " 1672  Changing the type of grafin would improve the ...   \n",
       " 1673  Concluding from the students data that plastic...   \n",
       " 1674  Two ways that the stundent could've improved t...   \n",
       " 1675  A conclusion I can make from this experiment i...   \n",
       " 1676  a.One conclusion I can draw is that plastic B ...   \n",
       " 1677  My conclusion to this experiment is that plast...   \n",
       " 1678  You can conclude that plastic B was the most m...   \n",
       " 1679  By looking at the results, one can see that pl...   \n",
       " 1680  Material type A stretched the least, and mater...   \n",
       " 1681  a) A conclusion about this data is that plasti...   \n",
       " 1682  a. Plastic type B is proven to be the most str...   \n",
       " 1683  A) Based on the student's data, I can draw the...   \n",
       " 1684  Based on the student's data, plastic B stretch...   \n",
       " 1685  a) The students concluded that platic B was th...   \n",
       " 1686  Based on the students data, it looks like all ...   \n",
       " 1687  A conclusion that can be drawn from the studen...   \n",
       " 1688  A conclusion that can be made based on this ex...   \n",
       " 1689  A. Based on this student's data, I conclude th...   \n",
       " 1690  a) In conclusion, plastic type A is the strong...   \n",
       " 1691  As shown in the data table, I can conclude tha...   \n",
       " 1692  a. Based on the students data I can conclude t...   \n",
       " 1693  The second trail the first two numbers are hig...   \n",
       " 1694  The question in this experiment is how many we...   \n",
       " 1695  (a) Plastic type \"B\" is made of the stretchies...   \n",
       " 1696  Plastic type ''B'' contains the most elasticit...   \n",
       " 1697  In conclusion, based on the students data coll...   \n",
       " 1698  a) The plastic type B has the greatest stretch...   \n",
       " 1699  Based on the student's data it looks like they...   \n",
       " 1700  I conclude, based on the data, that out of the...   \n",
       " 1701  A: Based on the data I can conclude that plast...   \n",
       " ...                                                 ...   \n",
       " 2920  Based on the students data I can conclude plas...   \n",
       " 2921  Overall plastic A is the most durable. Plastic...   \n",
       " 2922  The conclusion im gonna draw based on the stud...   \n",
       " 2923  The amount that plastic type A stretched was 1...   \n",
       " 2924  (a) Based on the students data, I can conclude...   \n",
       " 2925  a. Plastic type B stretched the most with 22-2...   \n",
       " 2926  The student could of improved their experiment...   \n",
       " 2927  A) I can conclude that this experiment was not...   \n",
       " 2928  a) Plastic type B stretched the most because i...   \n",
       " 2929  (A) A conclusion based on the student's data i...   \n",
       " 2930  a) Based on the students data, I conclude that...   \n",
       " 2931  Plastic type A was the only type of plastic th...   \n",
       " 2932  The students could have impared the experiment...   \n",
       " 2933  A . A conclusion based on the students data is...   \n",
       " 2934  Polymer B is the most stretchable plastic. ^p ...   \n",
       " 2935  Based on the students data results, plastic ty...   \n",
       " 2936  a) According to my results plastic type B had ...   \n",
       " 2937  a) In conclusion plastic type B had stretched ...   \n",
       " 2938  Conclusively  plastic type B tested to stretch...   \n",
       " 2939  A conclusion on could draw is that plastic typ...   \n",
       " 2940  a) Based on the data plastic A-C shows a decre...   \n",
       " 2941  Based on the students data plastic type A and ...   \n",
       " 2942  b) We need material they are plastic, tape cla...   \n",
       " 2943  Plastic type B was the most stretchable. It st...   \n",
       " 2944  a.) After testing this experiment, the results...   \n",
       " 2945  a) We can conclude that plastic B is the most ...   \n",
       " 2946  a. One conclusion I have fro this data was tha...   \n",
       " 2947  3. The second trial(12) is not exactly the sam...   \n",
       " 2948  A) I have concluded that based on the students...   \n",
       " 2949  Plastic type B stretchable most 22mm in T1 & 2...   \n",
       " \n",
       "                                                   clean  \\\n",
       " 1672  changing the type of grafin would improve the ...   \n",
       " 1673  concluding from the students data that plastic...   \n",
       " 1674  two ways that the stundent could 've improved ...   \n",
       " 1675  a conclusion i can make from this experiment i...   \n",
       " 1676  a one conclusion i can draw is that plastic b ...   \n",
       " 1677  my conclusion to this experiment is that plast...   \n",
       " 1678  you can conclude that plastic b was the most m...   \n",
       " 1679  by looking at the results , one can see that p...   \n",
       " 1680  material type a stretched the least , and mate...   \n",
       " 1681  a ) a conclusion about this data is that plast...   \n",
       " 1682  a plastic type b is proven to be the most stre...   \n",
       " 1683  a ) based on the student 's data , i can draw ...   \n",
       " 1684  based on the student 's data , plastic b stret...   \n",
       " 1685  a ) the students concluded that platic b was t...   \n",
       " 1686  based on the students data , it looks like all...   \n",
       " 1687  a conclusion that can be drawn from the studen...   \n",
       " 1688  a conclusion that can be made based on this ex...   \n",
       " 1689  a based on this student 's data , i conclude t...   \n",
       " 1690  a ) in conclusion , plastic type a is the stro...   \n",
       " 1691  as shown in the data table , i can conclude th...   \n",
       " 1692  a based on the students data i can conclude th...   \n",
       " 1693  the second trail the first two numbers are hig...   \n",
       " 1694  the question in this experiment is how many we...   \n",
       " 1695  ( a ) plastic type b is made of the stretchies...   \n",
       " 1696  plastic type ''b'' contains the most elasticit...   \n",
       " 1697  in conclusion , based on the students data col...   \n",
       " 1698  a ) the plastic type b has the greatest stretc...   \n",
       " 1699  based on the student 's data it looks like the...   \n",
       " 1700  i conclude , based on the data , that out of t...   \n",
       " 1701  a based on the data i can conclude that plasti...   \n",
       " ...                                                 ...   \n",
       " 2920  based on the students data i can conclude plas...   \n",
       " 2921  overall plastic a is the most durable plastic ...   \n",
       " 2922  the conclusion im gonna draw based on the stud...   \n",
       " 2923  the amount that plastic type a stretched was m...   \n",
       " 2924  ( a ) based on the students data , i can concl...   \n",
       " 2925  a plastic type b stretched the most with   mm ...   \n",
       " 2926  the student could of improved their experiment...   \n",
       " 2927  a ) i can conclude that this experiment was no...   \n",
       " 2928  a ) plastic type b stretched the most because ...   \n",
       " 2929  ( a ) a conclusion based on the student 's dat...   \n",
       " 2930  a ) based on the students data , i conclude th...   \n",
       " 2931  plastic type a was the only type of plastic th...   \n",
       " 2932  the students could have impared the experiment...   \n",
       " 2933  a a conclusion based on the students data is t...   \n",
       " 2934  polymer b is the most stretchable plastic p th...   \n",
       " 2935  based on the students data results , plastic t...   \n",
       " 2936  a ) according to my results plastic type b had...   \n",
       " 2937  a ) in conclusion plastic type b had stretched...   \n",
       " 2938  conclusively plastic type b tested to stretch ...   \n",
       " 2939  a conclusion on could draw is that plastic typ...   \n",
       " 2940  a ) based on the data plastic a c shows a decr...   \n",
       " 2941  based on the students data plastic type a and ...   \n",
       " 2942  b ) we need material they are plastic , tape c...   \n",
       " 2943  plastic type b was the most stretchable it str...   \n",
       " 2944  a ) after testing this experiment , the result...   \n",
       " 2945  a ) we can conclude that plastic b is the most...   \n",
       " 2946  a one conclusion i have fro this data was that...   \n",
       " 2947  the second trial (  ) is not exactly the same ...   \n",
       " 2948  a ) i have concluded that based on the student...   \n",
       " 2949  plastic type b stretchable most mm in t mm in ...   \n",
       " \n",
       "                                                       X  \\\n",
       " 1672  [1005, 1, 76, 7, 7740, 15, 338, 1, 152, 36, 54...   \n",
       " 1673  [2830, 42, 1, 178, 151, 8, 40, 76, 81, 19, 1, ...   \n",
       " 1674  [157, 276, 8, 1, 7741, 45, 692, 215, 1, 54, 4,...   \n",
       " 1675  [6, 239, 34, 26, 97, 42, 18, 54, 4, 8, 40, 76,...   \n",
       " 1676  [6, 44, 239, 34, 26, 975, 4, 8, 40, 81, 128, 1...   \n",
       " 1677  [445, 239, 2, 18, 54, 4, 8, 40, 169, 81, 732, ...   \n",
       " 1678  [32, 26, 519, 8, 40, 81, 19, 1, 123, 7742, 3, ...   \n",
       " 1679  [30, 831, 78, 1, 224, 44, 26, 265, 8, 40, 76, ...   \n",
       " 1680  [384, 76, 6, 128, 1, 292, 3, 384, 76, 81, 128,...   \n",
       " 1681  [6, 6, 239, 33, 18, 151, 4, 8, 40, 76, 6, 81, ...   \n",
       " 1682  [6, 40, 76, 81, 4, 2017, 2, 14, 1, 123, 7743, ...   \n",
       " 1683  [6, 249, 27, 1, 152, 36, 151, 34, 26, 975, 1, ...   \n",
       " 1684  [249, 27, 1, 152, 36, 151, 40, 81, 128, 55, 81...   \n",
       " 1685  [6, 1, 178, 1123, 8, 7745, 81, 19, 1, 123, 128...   \n",
       " 1686  [249, 27, 1, 178, 151, 9, 1028, 74, 87, 327, 7...   \n",
       " 1687  [6, 239, 8, 26, 14, 1212, 42, 1, 178, 151, 4, ...   \n",
       " 1688  [6, 239, 8, 26, 14, 226, 249, 27, 18, 54, 4, 4...   \n",
       " 1689  [6, 249, 27, 18, 152, 36, 151, 34, 519, 8, 165...   \n",
       " 1690  [6, 5, 239, 40, 76, 6, 4, 1, 679, 40, 3, 40, 7...   \n",
       " 1691  [38, 669, 5, 1, 151, 394, 34, 26, 519, 8, 40, ...   \n",
       " 1692  [6, 249, 27, 1, 178, 151, 34, 26, 519, 8, 205,...   \n",
       " 1693  [1, 253, 1286, 1, 120, 157, 1244, 11, 758, 31,...   \n",
       " 1694  [1, 770, 5, 18, 54, 4, 16, 206, 250, 118, 9, 3...   \n",
       " 1695  [6, 40, 76, 81, 4, 226, 7, 1, 850, 384, 81, 9,...   \n",
       " 1696  [40, 76, 4527, 1779, 1, 123, 2233, 1, 45, 29, ...   \n",
       " 1697  [5, 239, 249, 27, 1, 178, 151, 2440, 40, 76, 8...   \n",
       " 1698  [6, 1, 40, 76, 81, 75, 1, 1110, 388, 53, 7, 1,...   \n",
       " 1699  [249, 27, 1, 152, 36, 151, 9, 1028, 74, 10, 37...   \n",
       " 1700  [34, 519, 249, 27, 1, 151, 8, 53, 7, 1, 196, 5...   \n",
       " 1701  [6, 249, 27, 1, 151, 34, 26, 519, 8, 40, 81, 8...   \n",
       " ...                                                 ...   \n",
       " 2920  [249, 27, 1, 178, 151, 34, 26, 519, 40, 76, 81...   \n",
       " 2921  [1351, 40, 6, 4, 1, 123, 1124, 40, 81, 19, 1, ...   \n",
       " 2922  [1, 239, 2004, 2853, 975, 249, 27, 1, 178, 151...   \n",
       " 2923  [1, 142, 8, 40, 76, 6, 128, 19, 149, 27, 1, 12...   \n",
       " 2924  [6, 249, 27, 1, 178, 151, 34, 26, 519, 8, 40, ...   \n",
       " 2925  [6, 40, 76, 81, 128, 1, 123, 28, 149, 3, 40, 7...   \n",
       " 2926  [1, 152, 45, 7, 215, 99, 54, 30, 5600, 9, 128,...   \n",
       " 2927  [6, 34, 26, 519, 8, 18, 54, 19, 37, 696, 18, 1...   \n",
       " 2928  [6, 40, 76, 81, 128, 1, 123, 13, 9, 128, 71, 1...   \n",
       " 2929  [6, 6, 239, 249, 27, 1, 152, 36, 151, 4, 40, 7...   \n",
       " 2930  [6, 249, 27, 1, 178, 151, 34, 519, 8, 40, 76, ...   \n",
       " 2931  [40, 76, 6, 19, 1, 92, 76, 7, 40, 8, 128, 1, 1...   \n",
       " 2932  [1, 178, 45, 29, 8284, 1, 54, 30, 273, 1, 142,...   \n",
       " 2933  [6, 6, 239, 249, 27, 1, 178, 151, 4, 8, 1, 113...   \n",
       " 2934  [552, 81, 4, 1, 123, 565, 40, 139, 1, 338, 1, ...   \n",
       " 2935  [249, 27, 1, 178, 151, 224, 40, 76, 241, 128, ...   \n",
       " 2936  [6, 420, 2, 445, 224, 40, 76, 81, 83, 1, 1110,...   \n",
       " 2937  [6, 5, 239, 40, 76, 81, 83, 128, 1, 123, 5, 56...   \n",
       " 2938  [8286, 40, 76, 81, 811, 2, 401, 1, 1125, 304, ...   \n",
       " 2939  [6, 239, 27, 45, 975, 4, 8, 40, 76, 81, 75, 1,...   \n",
       " 2940  [6, 249, 27, 1, 151, 40, 6, 111, 219, 6, 1738,...   \n",
       " 2941  [249, 27, 1, 178, 151, 40, 76, 6, 3, 81, 5, 16...   \n",
       " 2942  [81, 188, 49, 384, 10, 11, 40, 1209, 561, 9, 2...   \n",
       " 2943  [40, 76, 81, 19, 1, 123, 565, 9, 128, 27, 187,...   \n",
       " 2944  [6, 114, 1000, 18, 54, 1, 224, 423, 8, 40, 169...   \n",
       " 2945  [6, 188, 26, 519, 8, 40, 81, 4, 1, 123, 565, 4...   \n",
       " 2946  [6, 44, 239, 34, 29, 2340, 18, 151, 19, 8, 40,...   \n",
       " 2947  [1, 253, 165, 4, 37, 525, 1, 102, 38, 1, 120, ...   \n",
       " 2948  [6, 34, 29, 1123, 8, 249, 27, 1, 178, 151, 8, ...   \n",
       " 2949  [40, 76, 81, 565, 123, 149, 5, 205, 149, 5, 20...   \n",
       " \n",
       "                                                     X_1  \n",
       " 1672  [1005, 1, 76, 7, 7740, 15, 338, 1, 152, 36, 54...  \n",
       " 1673  [2830, 42, 1, 178, 151, 8, 40, 76, 81, 19, 1, ...  \n",
       " 1674  [157, 276, 8, 1, 7741, 45, 692, 215, 1, 54, 4,...  \n",
       " 1675  [6, 239, 34, 26, 97, 42, 18, 54, 4, 8, 40, 76,...  \n",
       " 1676  [6, 44, 239, 34, 26, 975, 4, 8, 40, 81, 128, 1...  \n",
       " 1677  [445, 239, 2, 18, 54, 4, 8, 40, 169, 81, 732, ...  \n",
       " 1678  [32, 26, 519, 8, 40, 81, 19, 1, 123, 7742, 3, ...  \n",
       " 1679  [30, 831, 78, 1, 224, 44, 26, 265, 8, 40, 76, ...  \n",
       " 1680  [384, 76, 6, 128, 1, 292, 3, 384, 76, 81, 128,...  \n",
       " 1681  [6, 6, 239, 33, 18, 151, 4, 8, 40, 76, 6, 81, ...  \n",
       " 1682  [6, 40, 76, 81, 4, 2017, 2, 14, 1, 123, 7743, ...  \n",
       " 1683  [6, 249, 27, 1, 152, 36, 151, 34, 26, 975, 1, ...  \n",
       " 1684  [249, 27, 1, 152, 36, 151, 40, 81, 128, 55, 81...  \n",
       " 1685  [6, 1, 178, 1123, 8, 7745, 81, 19, 1, 123, 128...  \n",
       " 1686  [249, 27, 1, 178, 151, 9, 1028, 74, 87, 327, 7...  \n",
       " 1687  [6, 239, 8, 26, 14, 1212, 42, 1, 178, 151, 4, ...  \n",
       " 1688  [6, 239, 8, 26, 14, 226, 249, 27, 18, 54, 4, 4...  \n",
       " 1689  [6, 249, 27, 18, 152, 36, 151, 34, 519, 8, 165...  \n",
       " 1690  [6, 5, 239, 40, 76, 6, 4, 1, 679, 40, 3, 40, 7...  \n",
       " 1691  [38, 669, 5, 1, 151, 394, 34, 26, 519, 8, 40, ...  \n",
       " 1692  [6, 249, 27, 1, 178, 151, 34, 26, 519, 8, 205,...  \n",
       " 1693  [1, 253, 1286, 1, 120, 157, 1244, 11, 758, 31,...  \n",
       " 1694  [1, 770, 5, 18, 54, 4, 16, 206, 250, 118, 9, 3...  \n",
       " 1695  [6, 40, 76, 81, 4, 226, 7, 1, 850, 384, 81, 9,...  \n",
       " 1696  [40, 76, 4527, 1779, 1, 123, 2233, 1, 45, 29, ...  \n",
       " 1697  [5, 239, 249, 27, 1, 178, 151, 2440, 40, 76, 8...  \n",
       " 1698  [6, 1, 40, 76, 81, 75, 1, 1110, 388, 53, 7, 1,...  \n",
       " 1699  [249, 27, 1, 152, 36, 151, 9, 1028, 74, 10, 37...  \n",
       " 1700  [34, 519, 249, 27, 1, 151, 8, 53, 7, 1, 196, 5...  \n",
       " 1701  [6, 249, 27, 1, 151, 34, 26, 519, 8, 40, 81, 8...  \n",
       " ...                                                 ...  \n",
       " 2920  [249, 27, 1, 178, 151, 34, 26, 519, 40, 76, 81...  \n",
       " 2921  [1351, 40, 6, 4, 1, 123, 1124, 40, 81, 19, 1, ...  \n",
       " 2922  [1, 239, 2004, 2853, 975, 249, 27, 1, 178, 151...  \n",
       " 2923  [1, 142, 8, 40, 76, 6, 128, 19, 149, 27, 1, 12...  \n",
       " 2924  [6, 249, 27, 1, 178, 151, 34, 26, 519, 8, 40, ...  \n",
       " 2925  [6, 40, 76, 81, 128, 1, 123, 28, 149, 3, 40, 7...  \n",
       " 2926  [1, 152, 45, 7, 215, 99, 54, 30, 5600, 9, 128,...  \n",
       " 2927  [6, 34, 26, 519, 8, 18, 54, 19, 37, 696, 18, 1...  \n",
       " 2928  [6, 40, 76, 81, 128, 1, 123, 13, 9, 128, 71, 1...  \n",
       " 2929  [6, 6, 239, 249, 27, 1, 152, 36, 151, 4, 40, 7...  \n",
       " 2930  [6, 249, 27, 1, 178, 151, 34, 519, 8, 40, 76, ...  \n",
       " 2931  [40, 76, 6, 19, 1, 92, 76, 7, 40, 8, 128, 1, 1...  \n",
       " 2932  [1, 178, 45, 29, 8284, 1, 54, 30, 273, 1, 142,...  \n",
       " 2933  [6, 6, 239, 249, 27, 1, 178, 151, 4, 8, 1, 113...  \n",
       " 2934  [552, 81, 4, 1, 123, 565, 40, 139, 1, 338, 1, ...  \n",
       " 2935  [249, 27, 1, 178, 151, 224, 40, 76, 241, 128, ...  \n",
       " 2936  [6, 420, 2, 445, 224, 40, 76, 81, 83, 1, 1110,...  \n",
       " 2937  [6, 5, 239, 40, 76, 81, 83, 128, 1, 123, 5, 56...  \n",
       " 2938  [8286, 40, 76, 81, 811, 2, 401, 1, 1125, 304, ...  \n",
       " 2939  [6, 239, 27, 45, 975, 4, 8, 40, 76, 81, 75, 1,...  \n",
       " 2940  [6, 249, 27, 1, 151, 40, 6, 111, 219, 6, 1738,...  \n",
       " 2941  [249, 27, 1, 178, 151, 40, 76, 6, 3, 81, 5, 16...  \n",
       " 2942  [81, 188, 49, 384, 10, 11, 40, 1209, 561, 9, 2...  \n",
       " 2943  [40, 76, 81, 19, 1, 123, 565, 9, 128, 27, 187,...  \n",
       " 2944  [6, 114, 1000, 18, 54, 1, 224, 423, 8, 40, 169...  \n",
       " 2945  [6, 188, 26, 519, 8, 40, 81, 4, 1, 123, 565, 4...  \n",
       " 2946  [6, 44, 239, 34, 29, 2340, 18, 151, 19, 8, 40,...  \n",
       " 2947  [1, 253, 165, 4, 37, 525, 1, 102, 38, 1, 120, ...  \n",
       " 2948  [6, 34, 29, 1123, 8, 249, 27, 1, 178, 151, 8, ...  \n",
       " 2949  [40, 76, 81, 565, 123, 149, 5, 205, 149, 5, 20...  \n",
       " \n",
       " [1278 rows x 8 columns],         Id  EssaySet  Score1  Score2  \\\n",
       " 2950  4918         3       1       1   \n",
       " 2951  4919         3       1       2   \n",
       " 2952  4920         3       1       1   \n",
       " 2953  4921         3       2       1   \n",
       " 2954  4922         3       0       0   \n",
       " 2955  4923         3       0       0   \n",
       " 2956  4924         3       2       1   \n",
       " 2957  4925         3       1       1   \n",
       " 2958  4926         3       1       2   \n",
       " 2959  4927         3       1       0   \n",
       " 2960  4928         3       2       1   \n",
       " 2961  4929         3       1       1   \n",
       " 2962  4930         3       1       0   \n",
       " 2963  4931         3       0       0   \n",
       " 2964  4932         3       1       1   \n",
       " 2965  4933         3       2       2   \n",
       " 2966  4934         3       1       2   \n",
       " 2967  4935         3       1       0   \n",
       " 2968  4936         3       1       1   \n",
       " 2969  4937         3       2       2   \n",
       " 2970  4938         3       1       1   \n",
       " 2971  4939         3       1       1   \n",
       " 2972  4940         3       1       1   \n",
       " 2973  4941         3       2       2   \n",
       " 2974  4942         3       0       0   \n",
       " 2975  4943         3       1       2   \n",
       " 2976  4944         3       2       1   \n",
       " 2977  4945         3       1       1   \n",
       " 2978  4946         3       2       1   \n",
       " 2979  4947         3       1       1   \n",
       " ...    ...       ...     ...     ...   \n",
       " 4728  6778         3       1       1   \n",
       " 4729  6779         3       1       1   \n",
       " 4730  6780         3       1       1   \n",
       " 4731  6781         3       0       0   \n",
       " 4732  6782         3       0       0   \n",
       " 4733  6783         3       1       1   \n",
       " 4734  6784         3       1       1   \n",
       " 4735  6785         3       0       0   \n",
       " 4736  6786         3       0       1   \n",
       " 4737  6787         3       1       1   \n",
       " 4738  6788         3       0       0   \n",
       " 4739  6789         3       1       1   \n",
       " 4740  6791         3       2       2   \n",
       " 4741  6792         3       1       1   \n",
       " 4742  6793         3       1       1   \n",
       " 4743  6794         3       1       1   \n",
       " 4744  6795         3       0       1   \n",
       " 4745  6796         3       1       1   \n",
       " 4746  6797         3       1       1   \n",
       " 4747  6798         3       1       1   \n",
       " 4748  6799         3       0       0   \n",
       " 4749  6800         3       2       2   \n",
       " 4750  6801         3       2       2   \n",
       " 4751  6802         3       1       1   \n",
       " 4752  6803         3       2       2   \n",
       " 4753  6804         3       2       2   \n",
       " 4754  6805         3       0       0   \n",
       " 4755  6806         3       1       1   \n",
       " 4756  6807         3       1       1   \n",
       " 4757  6808         3       1       1   \n",
       " \n",
       "                                               EssayText  \\\n",
       " 2950  China's panda and Australia's koala are two an...   \n",
       " 2951  Pandas and koalas are similar because they are...   \n",
       " 2952  Pandas in China and Koalas in Australia are si...   \n",
       " 2953  Pandas in China only eat bamboo and Koalas in ...   \n",
       " 2954  Pandas in China and koalas from Australia are ...   \n",
       " 2955  Panda's are similar to koala's because they ar...   \n",
       " 2956  Panda's are similar to Koala's by they are bot...   \n",
       " 2957  Pandas in china are similar to koalas in Austr...   \n",
       " 2958  Pandas and koalas are similar because they eat...   \n",
       " 2959  Pandas are similar to koalas due to their very...   \n",
       " 2960  A panda from china and a koala from Australia ...   \n",
       " 2961  Pandas and koalas are similar in that they are...   \n",
       " 2962  A China panda eats almost nothing, but bamboo ...   \n",
       " 2963  The pandas in China eat nothing but bamboo and...   \n",
       " 2964  Panda's and koalas are both similar because th...   \n",
       " 2965  A panda and a koala beer are similar because t...   \n",
       " 2966  Pandas and koalas are similar because they nee...   \n",
       " 2967  Pandas in China are similar to koalas in Austr...   \n",
       " 2968  China's panda is similar to Australia's Koala ...   \n",
       " 2969  Pandas in China and koalas in Australia are si...   \n",
       " 2970  Pandas in China and koalas in Australia are si...   \n",
       " 2971  Pandas in China are similar to koalas in Austr...   \n",
       " 2972  Pandas and koalas, eva fogh they live in diffe...   \n",
       " 2973  A panda beer is similar koala to a koala bear ...   \n",
       " 2974  Pandas in China and koalas in Australia are si...   \n",
       " 2975  China's panda and Australia's koala bear are b...   \n",
       " 2976  Pandas and koalas are similar to each other be...   \n",
       " 2977  In the article it says that both koalas and pa...   \n",
       " 2978  Koala's and panda's are both specialists, whic...   \n",
       " 2979  Pandas in China and koalas from Australia are ...   \n",
       " ...                                                 ...   \n",
       " 4728  Pandas from China are similar to koalas in Aus...   \n",
       " 4729  The pandas in China and the koala bears in Aus...   \n",
       " 4730  Pandas in China are similar to koalas in Austr...   \n",
       " 4731  Panda's in China are similar to the koalas in ...   \n",
       " 4732  Pandas and koalas like to be in trees and I th...   \n",
       " 4733  China's panda and Australia's koala's are simi...   \n",
       " 4734  Both China's panda bear and Australia's koala ...   \n",
       " 4735  Pandas and koalas are similar to each other be...   \n",
       " 4736  Pandas in China are similar to koalas in Austr...   \n",
       " 4737  Pandas in China are similar to koalas in Austr...   \n",
       " 4738  Panda's in China are similar to koalas in Aust...   \n",
       " 4739  Pandas and koalas are similar because a panda ...   \n",
       " 4740  The China's panda and Australia's Koala are bo...   \n",
       " 4741  Panda bears and koalas are similar because the...   \n",
       " 4742  Pandas in China are similar to kola bears in A...   \n",
       " 4743  Pandas and koalas are similar because they eat...   \n",
       " 4744  China's panda only eat bamboo and Australia's ...   \n",
       " 4745  Pandas and koalas are similar because they are...   \n",
       " 4746  In the article it is explained that both panda...   \n",
       " 4747  They are similar because they both only eat a ...   \n",
       " 4748  Pandas in China are similar to koalas in Austr...   \n",
       " 4749  Pandas in China and koalas in Australia are si...   \n",
       " 4750  Pandas in china are similar to koalas in Austr...   \n",
       " 4751  Koalas bears and pandas are similar because bo...   \n",
       " 4752  Pandas and Koalas are similar because they are...   \n",
       " 4753  Pandas in China must live these because they e...   \n",
       " 4754  They are similar because they are not as visio...   \n",
       " 4755  Pandas in china are similar to Australia's kol...   \n",
       " 4756  The China's panda and the Eustralia's koala ar...   \n",
       " 4757  Pandas a koalas are similar because both have ...   \n",
       " \n",
       "                                                   clean  \\\n",
       " 2950  china 's panda and australia 's koala are two ...   \n",
       " 2951  pandas and koalas are similar because they are...   \n",
       " 2952  pandas in china and koalas in australia are si...   \n",
       " 2953  pandas in china only eat bamboo and koalas in ...   \n",
       " 2954  pandas in china and koalas from australia are ...   \n",
       " 2955  panda 's are similar to koala 's because they ...   \n",
       " 2956  panda 's are similar to koala 's by they are b...   \n",
       " 2957  pandas in china are similar to koalas in austr...   \n",
       " 2958  pandas and koalas are similar because they eat...   \n",
       " 2959  pandas are similar to koalas due to their very...   \n",
       " 2960  a panda from china and a koala from australia ...   \n",
       " 2961  pandas and koalas are similar in that they are...   \n",
       " 2962  a china panda eats almost nothing , but bamboo...   \n",
       " 2963  the pandas in china eat nothing but bamboo and...   \n",
       " 2964  panda 's and koalas are both similar because t...   \n",
       " 2965  a panda and a koala beer are similar because t...   \n",
       " 2966  pandas and koalas are similar because they nee...   \n",
       " 2967  pandas in china are similar to koalas in austr...   \n",
       " 2968  china 's panda is similar to australia 's koal...   \n",
       " 2969  pandas in china and koalas in australia are si...   \n",
       " 2970  pandas in china and koalas in australia are si...   \n",
       " 2971  pandas in china are similar to koalas in austr...   \n",
       " 2972  pandas and koalas , eva fogh they live in diff...   \n",
       " 2973  a panda beer is similar koala to a koala bear ...   \n",
       " 2974  pandas in china and koalas in australia are si...   \n",
       " 2975  china 's panda and australia 's koala bear are...   \n",
       " 2976  pandas and koalas are similar to each other be...   \n",
       " 2977  in the article it says that both koalas and pa...   \n",
       " 2978  koala 's and panda 's are both specialists , w...   \n",
       " 2979  pandas in china and koalas from australia are ...   \n",
       " ...                                                 ...   \n",
       " 4728  pandas from china are similar to koalas in aus...   \n",
       " 4729  the pandas in china and the koala bears in aus...   \n",
       " 4730  pandas in china are similar to koalas in austr...   \n",
       " 4731  panda 's in china are similar to the koalas in...   \n",
       " 4732  pandas and koalas like to be in trees and i th...   \n",
       " 4733  china 's panda and australia 's koala 's are s...   \n",
       " 4734  both china 's panda bear and australia 's koal...   \n",
       " 4735  pandas and koalas are similar to each other be...   \n",
       " 4736  pandas in china are similar to koalas in austr...   \n",
       " 4737  pandas in china are similar to koalas in austr...   \n",
       " 4738  panda 's in china are similar to koalas in aus...   \n",
       " 4739  pandas and koalas are similar because a panda ...   \n",
       " 4740  the china 's panda and australia 's koala are ...   \n",
       " 4741  panda bears and koalas are similar because the...   \n",
       " 4742  pandas in china are similar to kola bears in a...   \n",
       " 4743  pandas and koalas are similar because they eat...   \n",
       " 4744  china 's panda only eat bamboo and australia '...   \n",
       " 4745  pandas and koalas are similar because they are...   \n",
       " 4746  in the article it is explained that both panda...   \n",
       " 4747  they are similar because they both only eat a ...   \n",
       " 4748  pandas in china are similar to koalas in austr...   \n",
       " 4749  pandas in china and koalas in australia are si...   \n",
       " 4750  pandas in china are similar to koalas in austr...   \n",
       " 4751  koalas bears and pandas are similar because bo...   \n",
       " 4752  pandas and koalas are similar because they are...   \n",
       " 4753  pandas in china must live these because they e...   \n",
       " 4754  they are similar because they are not as visio...   \n",
       " 4755  pandas in china are similar to australia 's ko...   \n",
       " 4756  the china 's panda and the eustralia 's koala ...   \n",
       " 4757  pandas a koalas are similar because both have ...   \n",
       " \n",
       "                                                       X  \\\n",
       " 2950  [94, 36, 124, 3, 101, 36, 126, 11, 157, 106, 8...   \n",
       " 2951  [61, 3, 64, 11, 95, 13, 10, 11, 56, 918, 38, 2...   \n",
       " 2952  [61, 5, 94, 3, 64, 5, 101, 11, 95, 13, 10, 56,...   \n",
       " 2953  [61, 5, 94, 92, 47, 125, 3, 64, 5, 101, 47, 30...   \n",
       " 2954  [61, 5, 94, 3, 64, 42, 101, 11, 95, 13, 10, 11...   \n",
       " 2955  [124, 36, 11, 95, 2, 126, 36, 13, 10, 11, 56, ...   \n",
       " 2956  [124, 36, 11, 95, 2, 126, 36, 30, 10, 11, 56, ...   \n",
       " 2957  [61, 5, 94, 11, 95, 2, 64, 5, 101, 13, 10, 56,...   \n",
       " 2958  [61, 3, 64, 11, 95, 13, 10, 47, 629, 44, 76, 7...   \n",
       " 2959  [61, 11, 95, 2, 64, 735, 2, 99, 104, 70, 6, 74...   \n",
       " 2960  [6, 124, 42, 94, 3, 6, 126, 42, 101, 11, 104, ...   \n",
       " 2961  [61, 3, 64, 11, 95, 5, 8, 10, 11, 56, 680, 3, ...   \n",
       " 2962  [6, 94, 124, 141, 138, 223, 52, 125, 64, 297, ...   \n",
       " 2963  [1, 61, 5, 94, 47, 223, 52, 125, 3, 101, 36, 1...   \n",
       " 2964  [124, 36, 3, 64, 11, 56, 95, 13, 10, 11, 246, ...   \n",
       " 2965  [6, 124, 3, 6, 126, 3310, 11, 95, 13, 10, 56, ...   \n",
       " 2966  [61, 3, 64, 11, 95, 13, 10, 49, 1057, 356, 3, ...   \n",
       " 2967  [61, 5, 94, 11, 95, 2, 64, 5, 101, 13, 10, 11,...   \n",
       " 2968  [94, 36, 124, 4, 95, 2, 101, 36, 126, 13, 5, 1...   \n",
       " 2969  [61, 5, 94, 3, 64, 5, 101, 11, 95, 13, 10, 11,...   \n",
       " 2970  [61, 5, 94, 3, 64, 5, 101, 11, 95, 13, 10, 56,...   \n",
       " 2971  [61, 5, 94, 11, 95, 2, 64, 5, 101, 13, 10, 47,...   \n",
       " 2972  [61, 3, 64, 8301, 8302, 10, 160, 5, 57, 7, 1, ...   \n",
       " 2973  [6, 124, 3310, 4, 95, 126, 2, 6, 126, 297, 13,...   \n",
       " 2974  [61, 5, 94, 3, 64, 5, 101, 11, 95, 13, 10, 56,...   \n",
       " 2975  [94, 36, 124, 3, 101, 36, 126, 297, 11, 56, 22...   \n",
       " 2976  [61, 3, 64, 11, 95, 2, 73, 84, 13, 10, 56, 92,...   \n",
       " 2977  [5, 1, 35, 9, 162, 8, 56, 64, 3, 61, 11, 246, ...   \n",
       " 2978  [126, 36, 3, 124, 36, 11, 56, 220, 77, 190, 8,...   \n",
       " 2979  [61, 5, 94, 3, 64, 42, 101, 11, 95, 13, 10, 11...   \n",
       " ...                                                 ...   \n",
       " 4728  [61, 42, 94, 11, 95, 2, 64, 5, 101, 13, 1316, ...   \n",
       " 4729  [1, 61, 5, 94, 3, 1, 126, 395, 5, 101, 11, 56,...   \n",
       " 4730  [61, 5, 94, 11, 95, 2, 64, 5, 101, 13, 10, 47,...   \n",
       " 4731  [124, 36, 5, 94, 11, 95, 2, 1, 64, 5, 101, 13,...   \n",
       " 4732  [61, 3, 64, 74, 2, 14, 5, 1294, 3, 34, 166, 11...   \n",
       " 4733  [94, 36, 124, 3, 101, 36, 126, 36, 11, 95, 42,...   \n",
       " 4734  [56, 94, 36, 124, 297, 3, 101, 36, 126, 297, 5...   \n",
       " 4735  [61, 3, 64, 11, 95, 2, 73, 84, 13, 10, 56, 47,...   \n",
       " 4736  [61, 5, 94, 11, 95, 2, 64, 5, 101, 13, 10, 11,...   \n",
       " 4737  [61, 5, 94, 11, 95, 2, 64, 5, 101, 10, 92, 47,...   \n",
       " 4738  [124, 36, 5, 94, 11, 95, 2, 64, 5, 101, 13, 10...   \n",
       " 4739  [61, 3, 64, 11, 95, 13, 6, 124, 3335, 27, 125,...   \n",
       " 4740  [1, 94, 36, 124, 3, 101, 36, 126, 11, 56, 844,...   \n",
       " 4741  [124, 395, 3, 64, 11, 95, 13, 10, 11, 56, 1792...   \n",
       " 4742  [61, 5, 94, 11, 95, 2, 3629, 395, 5, 101, 13, ...   \n",
       " 4743  [61, 3, 64, 11, 95, 13, 10, 47, 138, 223, 52, ...   \n",
       " 4744  [94, 36, 124, 92, 47, 125, 3, 101, 36, 126, 92...   \n",
       " 4745  [61, 3, 64, 11, 95, 13, 10, 11, 220, 10, 29, 2...   \n",
       " 4746  [5, 1, 35, 9, 4, 852, 8, 56, 61, 3, 64, 11, 22...   \n",
       " 4747  [10, 11, 95, 13, 10, 56, 92, 47, 6, 270, 76, 7...   \n",
       " 4748  [61, 5, 94, 11, 95, 2, 64, 5, 101, 336, 10, 11...   \n",
       " 4749  [61, 5, 94, 3, 64, 5, 101, 11, 95, 13, 10, 11,...   \n",
       " 4750  [61, 5, 94, 11, 95, 2, 64, 5, 101, 13, 56, 11,...   \n",
       " 4751  [64, 395, 3, 61, 11, 95, 13, 56, 7, 158, 106, ...   \n",
       " 4752  [61, 3, 64, 11, 95, 13, 10, 11, 56, 220, 10, 4...   \n",
       " 4753  [61, 5, 94, 489, 160, 158, 13, 10, 47, 125, 64...   \n",
       " 4754  [10, 11, 95, 13, 10, 11, 37, 38, 8901, 38, 254...   \n",
       " 4755  [61, 5, 94, 11, 95, 2, 101, 36, 8902, 297, 13,...   \n",
       " 4756  [1, 94, 36, 124, 3, 1, 8903, 36, 126, 11, 8904...   \n",
       " 4757  [61, 6, 64, 11, 95, 13, 56, 29, 6, 827, 8, 244...   \n",
       " \n",
       "                                                     X_1  \n",
       " 2950  [94, 36, 124, 3, 101, 36, 126, 11, 157, 106, 8...  \n",
       " 2951  [61, 3, 64, 11, 95, 13, 10, 11, 56, 918, 38, 2...  \n",
       " 2952  [61, 5, 94, 3, 64, 5, 101, 11, 95, 13, 10, 56,...  \n",
       " 2953  [61, 5, 94, 92, 47, 125, 3, 64, 5, 101, 47, 30...  \n",
       " 2954  [61, 5, 94, 3, 64, 42, 101, 11, 95, 13, 10, 11...  \n",
       " 2955  [124, 36, 11, 95, 2, 126, 36, 13, 10, 11, 56, ...  \n",
       " 2956  [124, 36, 11, 95, 2, 126, 36, 30, 10, 11, 56, ...  \n",
       " 2957  [61, 5, 94, 11, 95, 2, 64, 5, 101, 13, 10, 56,...  \n",
       " 2958  [61, 3, 64, 11, 95, 13, 10, 47, 629, 44, 76, 7...  \n",
       " 2959  [61, 11, 95, 2, 64, 735, 2, 99, 104, 70, 6, 74...  \n",
       " 2960  [6, 124, 42, 94, 3, 6, 126, 42, 101, 11, 104, ...  \n",
       " 2961  [61, 3, 64, 11, 95, 5, 8, 10, 11, 56, 680, 3, ...  \n",
       " 2962  [6, 94, 124, 141, 138, 223, 52, 125, 64, 297, ...  \n",
       " 2963  [1, 61, 5, 94, 47, 223, 52, 125, 3, 101, 36, 1...  \n",
       " 2964  [124, 36, 3, 64, 11, 56, 95, 13, 10, 11, 246, ...  \n",
       " 2965  [6, 124, 3, 6, 126, 3310, 11, 95, 13, 10, 56, ...  \n",
       " 2966  [61, 3, 64, 11, 95, 13, 10, 49, 1057, 356, 3, ...  \n",
       " 2967  [61, 5, 94, 11, 95, 2, 64, 5, 101, 13, 10, 11,...  \n",
       " 2968  [94, 36, 124, 4, 95, 2, 101, 36, 126, 13, 5, 1...  \n",
       " 2969  [61, 5, 94, 3, 64, 5, 101, 11, 95, 13, 10, 11,...  \n",
       " 2970  [61, 5, 94, 3, 64, 5, 101, 11, 95, 13, 10, 56,...  \n",
       " 2971  [61, 5, 94, 11, 95, 2, 64, 5, 101, 13, 10, 47,...  \n",
       " 2972  [61, 3, 64, 8301, 8302, 10, 160, 5, 57, 7, 1, ...  \n",
       " 2973  [6, 124, 3310, 4, 95, 126, 2, 6, 126, 297, 13,...  \n",
       " 2974  [61, 5, 94, 3, 64, 5, 101, 11, 95, 13, 10, 56,...  \n",
       " 2975  [94, 36, 124, 3, 101, 36, 126, 297, 11, 56, 22...  \n",
       " 2976  [61, 3, 64, 11, 95, 2, 73, 84, 13, 10, 56, 92,...  \n",
       " 2977  [5, 1, 35, 9, 162, 8, 56, 64, 3, 61, 11, 246, ...  \n",
       " 2978  [126, 36, 3, 124, 36, 11, 56, 220, 77, 190, 8,...  \n",
       " 2979  [61, 5, 94, 3, 64, 42, 101, 11, 95, 13, 10, 11...  \n",
       " ...                                                 ...  \n",
       " 4728  [61, 42, 94, 11, 95, 2, 64, 5, 101, 13, 1316, ...  \n",
       " 4729  [1, 61, 5, 94, 3, 1, 126, 395, 5, 101, 11, 56,...  \n",
       " 4730  [61, 5, 94, 11, 95, 2, 64, 5, 101, 13, 10, 47,...  \n",
       " 4731  [124, 36, 5, 94, 11, 95, 2, 1, 64, 5, 101, 13,...  \n",
       " 4732  [61, 3, 64, 74, 2, 14, 5, 1294, 3, 34, 166, 11...  \n",
       " 4733  [94, 36, 124, 3, 101, 36, 126, 36, 11, 95, 42,...  \n",
       " 4734  [56, 94, 36, 124, 297, 3, 101, 36, 126, 297, 5...  \n",
       " 4735  [61, 3, 64, 11, 95, 2, 73, 84, 13, 10, 56, 47,...  \n",
       " 4736  [61, 5, 94, 11, 95, 2, 64, 5, 101, 13, 10, 11,...  \n",
       " 4737  [61, 5, 94, 11, 95, 2, 64, 5, 101, 10, 92, 47,...  \n",
       " 4738  [124, 36, 5, 94, 11, 95, 2, 64, 5, 101, 13, 10...  \n",
       " 4739  [61, 3, 64, 11, 95, 13, 6, 124, 3335, 27, 125,...  \n",
       " 4740  [1, 94, 36, 124, 3, 101, 36, 126, 11, 56, 844,...  \n",
       " 4741  [124, 395, 3, 64, 11, 95, 13, 10, 11, 56, 1792...  \n",
       " 4742  [61, 5, 94, 11, 95, 2, 3629, 395, 5, 101, 13, ...  \n",
       " 4743  [61, 3, 64, 11, 95, 13, 10, 47, 138, 223, 52, ...  \n",
       " 4744  [94, 36, 124, 92, 47, 125, 3, 101, 36, 126, 92...  \n",
       " 4745  [61, 3, 64, 11, 95, 13, 10, 11, 220, 10, 29, 2...  \n",
       " 4746  [5, 1, 35, 9, 4, 852, 8, 56, 61, 3, 64, 11, 22...  \n",
       " 4747  [10, 11, 95, 13, 10, 56, 92, 47, 6, 270, 76, 7...  \n",
       " 4748  [61, 5, 94, 11, 95, 2, 64, 5, 101, 336, 10, 11...  \n",
       " 4749  [61, 5, 94, 3, 64, 5, 101, 11, 95, 13, 10, 11,...  \n",
       " 4750  [61, 5, 94, 11, 95, 2, 64, 5, 101, 13, 56, 11,...  \n",
       " 4751  [64, 395, 3, 61, 11, 95, 13, 56, 7, 158, 106, ...  \n",
       " 4752  [61, 3, 64, 11, 95, 13, 10, 11, 56, 220, 10, 4...  \n",
       " 4753  [61, 5, 94, 489, 160, 158, 13, 10, 47, 125, 64...  \n",
       " 4754  [10, 11, 95, 13, 10, 11, 37, 38, 8901, 38, 254...  \n",
       " 4755  [61, 5, 94, 11, 95, 2, 101, 36, 8902, 297, 13,...  \n",
       " 4756  [1, 94, 36, 124, 3, 1, 8903, 36, 126, 11, 8904...  \n",
       " 4757  [61, 6, 64, 11, 95, 13, 56, 29, 6, 827, 8, 244...  \n",
       " \n",
       " [1808 rows x 8 columns],         Id  EssaySet  Score1  Score2  \\\n",
       " 4758  8070         4       0       0   \n",
       " 4759  8071         4       1       1   \n",
       " 4760  8072         4       0       0   \n",
       " 4761  8073         4       1       1   \n",
       " 4762  8074         4       1       1   \n",
       " 4763  8075         4       0       0   \n",
       " 4764  8076         4       1       1   \n",
       " 4765  8077         4       2       1   \n",
       " 4766  8078         4       1       1   \n",
       " 4767  8079         4       1       1   \n",
       " 4768  8080         4       1       2   \n",
       " 4769  8081         4       1       1   \n",
       " 4770  8082         4       0       1   \n",
       " 4771  8083         4       0       0   \n",
       " 4772  8084         4       1       1   \n",
       " 4773  8085         4       1       1   \n",
       " 4774  8086         4       0       1   \n",
       " 4775  8087         4       2       2   \n",
       " 4776  8088         4       1       0   \n",
       " 4777  8089         4       0       0   \n",
       " 4778  8090         4       1       1   \n",
       " 4779  8091         4       0       0   \n",
       " 4780  8092         4       1       1   \n",
       " 4781  8093         4       2       2   \n",
       " 4782  8094         4       2       2   \n",
       " 4783  8095         4       1       1   \n",
       " 4784  8096         4       1       1   \n",
       " 4785  8097         4       0       0   \n",
       " 4786  8098         4       1       0   \n",
       " 4787  8099         4       0       0   \n",
       " ...    ...       ...     ...     ...   \n",
       " 6385  9775         4       0       0   \n",
       " 6386  9777         4       1       1   \n",
       " 6387  9778         4       1       1   \n",
       " 6388  9779         4       0       0   \n",
       " 6389  9780         4       1       1   \n",
       " 6390  9781         4       1       1   \n",
       " 6391  9782         4       1       1   \n",
       " 6392  9783         4       1       1   \n",
       " 6393  9784         4       1       1   \n",
       " 6394  9785         4       0       0   \n",
       " 6395  9786         4       0       1   \n",
       " 6396  9787         4       0       0   \n",
       " 6397  9788         4       0       0   \n",
       " 6398  9789         4       0       0   \n",
       " 6399  9790         4       2       2   \n",
       " 6400  9791         4       0       0   \n",
       " 6401  9792         4       1       0   \n",
       " 6402  9793         4       1       1   \n",
       " 6403  9794         4       2       2   \n",
       " 6404  9795         4       0       0   \n",
       " 6405  9796         4       0       0   \n",
       " 6406  9797         4       1       1   \n",
       " 6407  9798         4       0       0   \n",
       " 6408  9799         4       1       1   \n",
       " 6409  9800         4       2       2   \n",
       " 6410  9801         4       1       2   \n",
       " 6411  9803         4       2       2   \n",
       " 6412  9805         4       1       1   \n",
       " 6413  9806         4       1       1   \n",
       " 6414  9807         4       1       1   \n",
       " \n",
       "                                               EssayText  \\\n",
       " 4758            I dont no what the would invasive mean.   \n",
       " 4759  The word invasive is significant to this artic...   \n",
       " 4760  The significance of the word invasive is meani...   \n",
       " 4761  The word \"invasive\"  in the article is used in...   \n",
       " 4762  The significance of the word ''invasive'' to t...   \n",
       " 4763  The word invasive like says in the article mea...   \n",
       " 4764  The article focuses on the introduction of the...   \n",
       " 4765  The word invasive is very important to the art...   \n",
       " 4766  I believe in this article \"invasive\" means hid...   \n",
       " 4767  Invasive, meaning a foreign species that has m...   \n",
       " 4768  The significance of the word invasive is that ...   \n",
       " 4769  Invasive means something that invades an area....   \n",
       " 4770  The word \"invasive\" is very appropriate for al...   \n",
       " 4771  The significance of the word invasive is passi...   \n",
       " 4772  In the article, \" one mans pet, another' invas...   \n",
       " 4773  The significance of \"invasive\" is that there i...   \n",
       " 4774  The significance of the word \"invasive\" to the...   \n",
       " 4775  The word \"invasive\" means intruding. Invasive ...   \n",
       " 4776  Invassive as to almost coming right up too. In...   \n",
       " 4777  The significance of the word invasive is it ca...   \n",
       " 4778  The significance of the word invasive is to ex...   \n",
       " 4779  I think that the word invasive is like saying ...   \n",
       " 4780  The significance of the word \"Invasive\" is tha...   \n",
       " 4781  The word, \"invasive\" plays a very important ro...   \n",
       " 4782  The significance of the word \"invasive\" in thi...   \n",
       " 4783  The significance of the word \"Invasive\" is lik...   \n",
       " 4784  The word invasive is significant to the articl...   \n",
       " 4785  The significane of the wod \"invasive\" is truel...   \n",
       " 4786  The word invasive is significant because they ...   \n",
       " 4787  Invasive means that the species kind of was in...   \n",
       " ...                                                 ...   \n",
       " 6385  I believe the word \"invasive\" was incorporated...   \n",
       " 6386  The word \"invasive\" reffers to the term \"invas...   \n",
       " 6387  The word invasive is a good way to describe th...   \n",
       " 6388  The word \"invasive\" is very significant in the...   \n",
       " 6389  Invasive in this article is exactly what the s...   \n",
       " 6390  The word invasive is significant to this artic...   \n",
       " 6391  Invasive is a repedetive word. We in this arti...   \n",
       " 6392  Invasive in this article is significant becaus...   \n",
       " 6393  Invasive species are species that have no pred...   \n",
       " 6394  The word invasive means that the reptiles can ...   \n",
       " 6395  The word invasive means that the animal or thi...   \n",
       " 6396  Invasive to the article means that other speci...   \n",
       " 6397  The significance of the word \"invasive\" to the...   \n",
       " 6398  Invasive is described as not being introduced ...   \n",
       " 6399  Invasive means that a species, not native to t...   \n",
       " 6400  Invasive is significant because it implies tha...   \n",
       " 6401  The significance of the word invasive is to de...   \n",
       " 6402  Invasive is like when there is a lot of one th...   \n",
       " 6403  The significant of the word invasive refers to...   \n",
       " 6404  The word invasive means \"unnatural\" in the art...   \n",
       " 6405  Invasive means farced. Because there saying ho...   \n",
       " 6406  The significance of the word \"Invasive\" is tha...   \n",
       " 6407  The significance of the word invasive is to sh...   \n",
       " 6408  Invasiveis significant to the rest of this art...   \n",
       " 6409  The term invasive means or species of plants o...   \n",
       " 6410  In the article \"invasive refers to a species t...   \n",
       " 6411  The word \"invasive\" is significance to the art...   \n",
       " 6412  The word invasive shows that pythons, rattlesn...   \n",
       " 6413  The word invasive is significant to the rest o...   \n",
       " 6414  The significance of \"Invasive\" is passing judg...   \n",
       " \n",
       "                                                   clean  \\\n",
       " 4758             i dont no what the would invasive mean   \n",
       " 4759  the word invasive is significant to this artic...   \n",
       " 4760  the significance of the word invasive is meani...   \n",
       " 4761  the word invasive in the article is used in th...   \n",
       " 4762  the significance of the word ''invasive'' to t...   \n",
       " 4763  the word invasive like says in the article mea...   \n",
       " 4764  the article focuses on the introduction of the...   \n",
       " 4765  the word invasive is very important to the art...   \n",
       " 4766  i believe in this article invasive means hidde...   \n",
       " 4767  invasive , meaning a foreign species that has ...   \n",
       " 4768  the significance of the word invasive is that ...   \n",
       " 4769  invasive means something that invades an area ...   \n",
       " 4770  the word invasive is very appropriate for almo...   \n",
       " 4771  the significance of the word invasive is passi...   \n",
       " 4772  in the article , one mans pet , another' invas...   \n",
       " 4773  the significance of invasive is that there is ...   \n",
       " 4774  the significance of the word invasive to the r...   \n",
       " 4775  the word invasive means intruding invasive spe...   \n",
       " 4776  invassive as to almost coming right up too in ...   \n",
       " 4777  the significance of the word invasive is it ca...   \n",
       " 4778  the significance of the word invasive is to ex...   \n",
       " 4779  i think that the word invasive is like saying ...   \n",
       " 4780  the significance of the word invasive is that ...   \n",
       " 4781  the word , invasive plays a very important rol...   \n",
       " 4782  the significance of the word invasive in this ...   \n",
       " 4783  the significance of the word invasive is like ...   \n",
       " 4784  the word invasive is significant to the articl...   \n",
       " 4785  the significane of the wod invasive is truely ...   \n",
       " 4786  the word invasive is significant because they ...   \n",
       " 4787  invasive means that the species kind of was in...   \n",
       " ...                                                 ...   \n",
       " 6385  i believe the word invasive was incorporated i...   \n",
       " 6386  the word invasive reffers to the term invasion...   \n",
       " 6387  the word invasive is a good way to describe th...   \n",
       " 6388  the word invasive is very significant in the a...   \n",
       " 6389  invasive in this article is exactly what the s...   \n",
       " 6390  the word invasive is significant to this artic...   \n",
       " 6391  invasive is a repedetive word we in this artic...   \n",
       " 6392  invasive in this article is significant becaus...   \n",
       " 6393  invasive species are species that have no pred...   \n",
       " 6394  the word invasive means that the reptiles can ...   \n",
       " 6395  the word invasive means that the animal or thi...   \n",
       " 6396  invasive to the article means that other speci...   \n",
       " 6397  the significance of the word invasive to the r...   \n",
       " 6398  invasive is described as not being introduced ...   \n",
       " 6399  invasive means that a species , not native to ...   \n",
       " 6400  invasive is significant because it implies tha...   \n",
       " 6401  the significance of the word invasive is to de...   \n",
       " 6402  invasive is like when there is a lot of one th...   \n",
       " 6403  the significant of the word invasive refers to...   \n",
       " 6404  the word invasive means unnatural in the artic...   \n",
       " 6405  invasive means farced because there saying how...   \n",
       " 6406  the significance of the word invasive is that ...   \n",
       " 6407  the significance of the word invasive is to sh...   \n",
       " 6408  invasiveis significant to the rest of this art...   \n",
       " 6409  the term invasive means or species of plants o...   \n",
       " 6410  in the article invasive refers to a species th...   \n",
       " 6411  the word invasive is significance to the artic...   \n",
       " 6412  the word invasive shows that pythons , rattles...   \n",
       " 6413  the word invasive is significant to the rest o...   \n",
       " 6414  the significance of invasive is passing judgem...   \n",
       " \n",
       "                                                       X  \\\n",
       " 4758  [34, 603, 277, 23, 1, 15, 39, 649, 0, 0, 0, 0,...   \n",
       " 4759  [1, 91, 39, 4, 370, 2, 18, 35, 1, 58, 330, 1, ...   \n",
       " 4760  [1, 255, 7, 1, 91, 39, 4, 511, 1, 511, 7, 1, 9...   \n",
       " 4761  [1, 91, 39, 5, 1, 35, 4, 105, 5, 1, 2723, 7, 3...   \n",
       " 4762  [1, 255, 7, 1, 91, 808, 2, 1, 353, 7, 1, 35, 4...   \n",
       " 4763  [1, 91, 39, 74, 162, 5, 1, 35, 649, 6, 106, 8,...   \n",
       " 4764  [1, 35, 3107, 27, 1, 315, 7, 1, 485, 80, 7, 25...   \n",
       " 4765  [1, 91, 39, 4, 104, 303, 2, 1, 35, 3, 41, 238,...   \n",
       " 4766  [34, 499, 5, 18, 35, 39, 190, 3957, 645, 62, 3...   \n",
       " 4767  [39, 511, 6, 1319, 80, 8, 75, 226, 6, 505, 121...   \n",
       " 4768  [1, 255, 7, 1, 91, 39, 4, 8, 1, 112, 3, 84, 25...   \n",
       " 4769  [39, 190, 204, 8, 2103, 71, 345, 18, 4, 303, 5...   \n",
       " 4770  [1, 91, 39, 4, 104, 2317, 20, 138, 87, 7, 1, 3...   \n",
       " 4771  [1, 255, 7, 1, 91, 39, 4, 380, 416, 13, 5, 1, ...   \n",
       " 4772  [5, 1, 35, 44, 4683, 664, 8914, 39, 80, 30, 23...   \n",
       " 4773  [1, 255, 7, 39, 4, 8, 103, 4, 3340, 27, 865, 6...   \n",
       " 4774  [1, 255, 7, 1, 91, 39, 2, 1, 353, 7, 1, 35, 19...   \n",
       " 4775  [1, 91, 39, 190, 2105, 39, 80, 74, 1, 112, 5, ...   \n",
       " 4776  [5746, 38, 2, 138, 756, 482, 127, 191, 5, 18, ...   \n",
       " 4777  [1, 255, 7, 1, 91, 39, 4, 9, 26, 14, 105, 38, ...   \n",
       " 4778  [1, 255, 7, 1, 91, 39, 4, 2, 462, 8, 1, 480, 4...   \n",
       " 4779  [34, 166, 8, 1, 91, 39, 4, 74, 385, 194, 11, 3...   \n",
       " 4780  [1, 255, 7, 1, 91, 39, 4, 8, 48, 194, 1864, 9,...   \n",
       " 4781  [1, 91, 39, 2227, 6, 104, 303, 1663, 5, 1, 35,...   \n",
       " 4782  [1, 255, 7, 1, 91, 39, 5, 18, 35, 4, 71, 480, ...   \n",
       " 4783  [1, 255, 7, 1, 91, 39, 4, 74, 10, 132, 774, 61...   \n",
       " 4784  [1, 91, 39, 4, 370, 2, 1, 35, 13, 5, 44, 387, ...   \n",
       " 4785  [1, 5747, 7, 1, 5748, 39, 4, 2699, 511, 6, 892...   \n",
       " 4786  [1, 91, 39, 4, 370, 13, 10, 1614, 32, 8, 158, ...   \n",
       " 4787  [39, 190, 8, 1, 80, 329, 7, 19, 8924, 2, 71, 3...   \n",
       " ...                                                 ...   \n",
       " 6385  [34, 499, 1, 91, 39, 19, 5945, 5, 2, 1, 35, 2,...   \n",
       " 6386  [1, 91, 39, 9837, 2, 1, 403, 1143, 1, 35, 4, 8...   \n",
       " 6387  [1, 91, 39, 4, 6, 134, 108, 2, 375, 1, 106, 5,...   \n",
       " 6388  [1, 91, 39, 4, 104, 370, 5, 1, 35, 18, 91, 4, ...   \n",
       " 6389  [39, 5, 18, 35, 4, 525, 23, 1, 390, 11, 99, 44...   \n",
       " 6390  [1, 91, 39, 4, 370, 2, 18, 35, 9, 4, 13, 7, 18...   \n",
       " 6391  [39, 4, 6, 9838, 91, 188, 5, 18, 35, 9, 1263, ...   \n",
       " 6392  [39, 5, 18, 35, 4, 370, 13, 9, 238, 16, 1568, ...   \n",
       " 6393  [39, 80, 11, 80, 8, 29, 277, 2027, 59, 1, 4162...   \n",
       " 6394  [1, 91, 39, 190, 8, 1, 254, 26, 14, 800, 106, ...   \n",
       " 6395  [1, 91, 39, 190, 8, 1, 480, 62, 170, 4, 321, 9...   \n",
       " 6396  [39, 2, 1, 35, 190, 8, 84, 80, 597, 172, 6, 17...   \n",
       " 6397  [1, 255, 7, 1, 91, 39, 2, 1, 353, 7, 1, 35, 4,...   \n",
       " 6398  [39, 4, 918, 38, 37, 148, 322, 3, 8, 9, 4, 191...   \n",
       " 6399  [39, 190, 8, 6, 80, 37, 485, 2, 1, 1382, 345, ...   \n",
       " 6400  [39, 4, 370, 13, 9, 1480, 8, 254, 633, 288, 50...   \n",
       " 6401  [1, 255, 7, 1, 91, 39, 4, 2, 375, 1, 112, 5, 1...   \n",
       " 6402  [39, 4, 74, 48, 103, 4, 6, 366, 7, 44, 170, 13...   \n",
       " 6403  [1, 370, 7, 1, 91, 39, 1088, 2, 71, 4106, 2, 1...   \n",
       " 6404  [1, 91, 39, 190, 3122, 5, 1, 35, 13, 1, 35, 22...   \n",
       " 6405  [39, 190, 9847, 13, 103, 385, 16, 194, 584, 58...   \n",
       " 6406  [1, 255, 7, 1, 91, 39, 4, 8, 18, 190, 80, 11, ...   \n",
       " 6407  [1, 255, 7, 1, 91, 39, 4, 2, 423, 234, 1, 189,...   \n",
       " 6408  [9848, 370, 2, 1, 353, 7, 18, 35, 13, 9, 4, 16...   \n",
       " 6409  [1, 403, 39, 190, 62, 80, 7, 484, 62, 106, 8, ...   \n",
       " 6410  [5, 1, 35, 39, 1088, 2, 6, 80, 8, 391, 2, 14, ...   \n",
       " 6411  [1, 91, 39, 4, 255, 2, 1, 35, 13, 9, 4, 1, 35,...   \n",
       " 6412  [1, 91, 39, 219, 8, 51, 5819, 3, 84, 254, 132,...   \n",
       " 6413  [1, 91, 39, 4, 370, 2, 1, 353, 7, 1, 35, 13, 9...   \n",
       " 6414  [1, 255, 7, 39, 4, 380, 416, 5, 211, 1321, 236...   \n",
       " \n",
       "                                                     X_1  \n",
       " 4758  [34, 603, 277, 23, 1, 15, 39, 649, 0, 0, 0, 0,...  \n",
       " 4759  [1, 91, 39, 4, 370, 2, 18, 35, 1, 58, 330, 1, ...  \n",
       " 4760  [1, 255, 7, 1, 91, 39, 4, 511, 1, 511, 7, 1, 9...  \n",
       " 4761  [1, 91, 39, 5, 1, 35, 4, 105, 5, 1, 2723, 7, 3...  \n",
       " 4762  [1, 255, 7, 1, 91, 808, 2, 1, 353, 7, 1, 35, 4...  \n",
       " 4763  [1, 91, 39, 74, 162, 5, 1, 35, 649, 6, 106, 8,...  \n",
       " 4764  [1, 35, 3107, 27, 1, 315, 7, 1, 485, 80, 7, 25...  \n",
       " 4765  [1, 91, 39, 4, 104, 303, 2, 1, 35, 3, 41, 238,...  \n",
       " 4766  [34, 499, 5, 18, 35, 39, 190, 3957, 645, 62, 3...  \n",
       " 4767  [39, 511, 6, 1319, 80, 8, 75, 226, 6, 505, 121...  \n",
       " 4768  [1, 255, 7, 1, 91, 39, 4, 8, 1, 112, 3, 84, 25...  \n",
       " 4769  [39, 190, 204, 8, 2103, 71, 345, 18, 4, 303, 5...  \n",
       " 4770  [1, 91, 39, 4, 104, 2317, 20, 138, 87, 7, 1, 3...  \n",
       " 4771  [1, 255, 7, 1, 91, 39, 4, 380, 416, 13, 5, 1, ...  \n",
       " 4772  [5, 1, 35, 44, 4683, 664, 8914, 39, 80, 30, 23...  \n",
       " 4773  [1, 255, 7, 39, 4, 8, 103, 4, 3340, 27, 865, 6...  \n",
       " 4774  [1, 255, 7, 1, 91, 39, 2, 1, 353, 7, 1, 35, 19...  \n",
       " 4775  [1, 91, 39, 190, 2105, 39, 80, 74, 1, 112, 5, ...  \n",
       " 4776  [5746, 38, 2, 138, 756, 482, 127, 191, 5, 18, ...  \n",
       " 4777  [1, 255, 7, 1, 91, 39, 4, 9, 26, 14, 105, 38, ...  \n",
       " 4778  [1, 255, 7, 1, 91, 39, 4, 2, 462, 8, 1, 480, 4...  \n",
       " 4779  [34, 166, 8, 1, 91, 39, 4, 74, 385, 194, 11, 3...  \n",
       " 4780  [1, 255, 7, 1, 91, 39, 4, 8, 48, 194, 1864, 9,...  \n",
       " 4781  [1, 91, 39, 2227, 6, 104, 303, 1663, 5, 1, 35,...  \n",
       " 4782  [1, 255, 7, 1, 91, 39, 5, 18, 35, 4, 71, 480, ...  \n",
       " 4783  [1, 255, 7, 1, 91, 39, 4, 74, 10, 132, 774, 61...  \n",
       " 4784  [1, 91, 39, 4, 370, 2, 1, 35, 13, 5, 44, 387, ...  \n",
       " 4785  [1, 5747, 7, 1, 5748, 39, 4, 2699, 511, 6, 892...  \n",
       " 4786  [1, 91, 39, 4, 370, 13, 10, 1614, 32, 8, 158, ...  \n",
       " 4787  [39, 190, 8, 1, 80, 329, 7, 19, 8924, 2, 71, 3...  \n",
       " ...                                                 ...  \n",
       " 6385  [34, 499, 1, 91, 39, 19, 5945, 5, 2, 1, 35, 2,...  \n",
       " 6386  [1, 91, 39, 9837, 2, 1, 403, 1143, 1, 35, 4, 8...  \n",
       " 6387  [1, 91, 39, 4, 6, 134, 108, 2, 375, 1, 106, 5,...  \n",
       " 6388  [1, 91, 39, 4, 104, 370, 5, 1, 35, 18, 91, 4, ...  \n",
       " 6389  [39, 5, 18, 35, 4, 525, 23, 1, 390, 11, 99, 44...  \n",
       " 6390  [1, 91, 39, 4, 370, 2, 18, 35, 9, 4, 13, 7, 18...  \n",
       " 6391  [39, 4, 6, 9838, 91, 188, 5, 18, 35, 9, 1263, ...  \n",
       " 6392  [39, 5, 18, 35, 4, 370, 13, 9, 238, 16, 1568, ...  \n",
       " 6393  [39, 80, 11, 80, 8, 29, 277, 2027, 59, 1, 4162...  \n",
       " 6394  [1, 91, 39, 190, 8, 1, 254, 26, 14, 800, 106, ...  \n",
       " 6395  [1, 91, 39, 190, 8, 1, 480, 62, 170, 4, 321, 9...  \n",
       " 6396  [39, 2, 1, 35, 190, 8, 84, 80, 597, 172, 6, 17...  \n",
       " 6397  [1, 255, 7, 1, 91, 39, 2, 1, 353, 7, 1, 35, 4,...  \n",
       " 6398  [39, 4, 918, 38, 37, 148, 322, 3, 8, 9, 4, 191...  \n",
       " 6399  [39, 190, 8, 6, 80, 37, 485, 2, 1, 1382, 345, ...  \n",
       " 6400  [39, 4, 370, 13, 9, 1480, 8, 254, 633, 288, 50...  \n",
       " 6401  [1, 255, 7, 1, 91, 39, 4, 2, 375, 1, 112, 5, 1...  \n",
       " 6402  [39, 4, 74, 48, 103, 4, 6, 366, 7, 44, 170, 13...  \n",
       " 6403  [1, 370, 7, 1, 91, 39, 1088, 2, 71, 4106, 2, 1...  \n",
       " 6404  [1, 91, 39, 190, 3122, 5, 1, 35, 13, 1, 35, 22...  \n",
       " 6405  [39, 190, 9847, 13, 103, 385, 16, 194, 584, 58...  \n",
       " 6406  [1, 255, 7, 1, 91, 39, 4, 8, 18, 190, 80, 11, ...  \n",
       " 6407  [1, 255, 7, 1, 91, 39, 4, 2, 423, 234, 1, 189,...  \n",
       " 6408  [9848, 370, 2, 1, 353, 7, 18, 35, 13, 9, 4, 16...  \n",
       " 6409  [1, 403, 39, 190, 62, 80, 7, 484, 62, 106, 8, ...  \n",
       " 6410  [5, 1, 35, 39, 1088, 2, 6, 80, 8, 391, 2, 14, ...  \n",
       " 6411  [1, 91, 39, 4, 255, 2, 1, 35, 13, 9, 4, 1, 35,...  \n",
       " 6412  [1, 91, 39, 219, 8, 51, 5819, 3, 84, 254, 132,...  \n",
       " 6413  [1, 91, 39, 4, 370, 2, 1, 353, 7, 1, 35, 13, 9...  \n",
       " 6414  [1, 255, 7, 39, 4, 380, 416, 5, 211, 1321, 236...  \n",
       " \n",
       " [1657 rows x 8 columns],          Id  EssaySet  Score1  Score2  \\\n",
       " 6415  10967         5       1       1   \n",
       " 6416  10968         5       0       0   \n",
       " 6417  10969         5       0       0   \n",
       " 6418  10970         5       0       0   \n",
       " 6419  10971         5       2       2   \n",
       " 6420  10972         5       1       1   \n",
       " 6421  10973         5       0       0   \n",
       " 6422  10974         5       0       0   \n",
       " 6423  10975         5       0       1   \n",
       " 6424  10976         5       1       1   \n",
       " 6425  10977         5       0       0   \n",
       " 6426  10978         5       1       1   \n",
       " 6427  10979         5       1       1   \n",
       " 6428  10980         5       0       0   \n",
       " 6429  10981         5       3       3   \n",
       " 6430  10982         5       1       1   \n",
       " 6431  10983         5       0       0   \n",
       " 6432  10984         5       0       0   \n",
       " 6433  10985         5       1       1   \n",
       " 6434  10986         5       0       0   \n",
       " 6435  10987         5       0       0   \n",
       " 6436  10988         5       0       0   \n",
       " 6437  10989         5       0       0   \n",
       " 6438  10990         5       0       0   \n",
       " 6439  10991         5       0       0   \n",
       " 6440  10992         5       3       3   \n",
       " 6441  10993         5       0       0   \n",
       " 6442  10994         5       0       0   \n",
       " 6443  10995         5       1       1   \n",
       " 6444  10996         5       2       2   \n",
       " ...     ...       ...     ...     ...   \n",
       " 8180  12732         5       0       0   \n",
       " 8181  12733         5       0       0   \n",
       " 8182  12734         5       1       1   \n",
       " 8183  12735         5       1       1   \n",
       " 8184  12736         5       1       1   \n",
       " 8185  12737         5       0       0   \n",
       " 8186  12738         5       0       0   \n",
       " 8187  12739         5       1       1   \n",
       " 8188  12740         5       0       0   \n",
       " 8189  12741         5       1       1   \n",
       " 8190  12742         5       1       1   \n",
       " 8191  12743         5       0       0   \n",
       " 8192  12744         5       0       0   \n",
       " 8193  12745         5       0       0   \n",
       " 8194  12746         5       0       0   \n",
       " 8195  12747         5       0       0   \n",
       " 8196  12748         5       0       0   \n",
       " 8197  12749         5       0       0   \n",
       " 8198  12750         5       0       0   \n",
       " 8199  12751         5       0       0   \n",
       " 8200  12752         5       0       0   \n",
       " 8201  12753         5       0       1   \n",
       " 8202  12754         5       0       0   \n",
       " 8203  12755         5       1       1   \n",
       " 8204  12756         5       0       0   \n",
       " 8205  12757         5       0       0   \n",
       " 8206  12758         5       0       0   \n",
       " 8207  12759         5       0       0   \n",
       " 8208  12760         5       0       0   \n",
       " 8209  12761         5       1       0   \n",
       " \n",
       "                                               EssayText  \\\n",
       " 6415  The mRNA travels to the ribosomes. At the ribo...   \n",
       " 6416  ATP is created and broken down.It is broken do...   \n",
       " 6417  The mRNA first gets on an electron transport c...   \n",
       " 6418  The mRNA then travels to the mitochondria, whe...   \n",
       " 6419  First thing the mRNA does is go to the ribosom...   \n",
       " 6420                    goes to the er then golgi bodys   \n",
       " 6421  messenger RNA collect  the protein particals i...   \n",
       " 6422  Translation- The amino acid code is translated...   \n",
       " 6423  1. The mRNA takes the copied information out i...   \n",
       " 6424  first the mRNA leaves the cell nucleus and mov...   \n",
       " 6425  The mRNA brings the protein to another place i...   \n",
       " 6426  mRNA leaves the  nucleus. Then it enters the c...   \n",
       " 6427  1. The mRNA leaves the nucleus.2. The mRNA tra...   \n",
       " 6428           anaphase, metaphase, prophase, telophase   \n",
       " 6429  Four major steps involved in protein synthesis...   \n",
       " 6430  1.The mRNA sends a message to ribosomes to sen...   \n",
       " 6431           Prophase, Metaphase, Anaphase, Telophase   \n",
       " 6432  It sends instructions for the amino acid seque...   \n",
       " 6433  The mRNA trvales through the cytoplasm, then e...   \n",
       " 6434  MRNA- MESSENGER RNA WHERE IT SEND MESSAGES THR...   \n",
       " 6435                ribsomesnucleusanaphasemoonchadrion   \n",
       " 6436  Protein synthesis is needed in your body becau...   \n",
       " 6437  Once mRNA leaves the nucleus, it travels to th...   \n",
       " 6438  the dna then begins to reproduce,  then mitosi...   \n",
       " 6439  The mRNA exits the nucleus and enters the cyto...   \n",
       " 6440  When the mRNA leaves the nucleus, it fits thro...   \n",
       " 6441  1. mitosis2. meiosis3. dna extraction4. blood ...   \n",
       " 6442  1) mRNA leaves the nucleus on a DNA strand.2) ...   \n",
       " 6443  Once the mRNA leaves the nucleus it has to mak...   \n",
       " 6444  1. The mRNA attaches to a ribosome, to begin p...   \n",
       " ...                                                 ...   \n",
       " 8180  mRNA leaves the nucleus, the amino acids form ...   \n",
       " 8181  living in the envorment, to survle the envorme...   \n",
       " 8182  The mRNA goes to the  ribosomes to make tRNA s...   \n",
       " 8183  1. The mRNA bind to the tRNA2. The anticodons ...   \n",
       " 8184  The mRNA finds an rRNA and the mRNA unzipps th...   \n",
       " 8185                       -enzymes-protein-amino acids   \n",
       " 8186  Four major steps in protein synthesis are to t...   \n",
       " 8187  First, the protein makes a copy of its self, w...   \n",
       " 8188  mRNA- it sends the message to the cells statin...   \n",
       " 8189  After mRNA leaves the nucleus, it is moved to ...   \n",
       " 8190  mRNA leaves the nucleustravels to the ribosome...   \n",
       " 8191            Translation, Transcription, Replication   \n",
       " 8192                         transcription, translation   \n",
       " 8193  sending messages to the whole cell, transfurin...   \n",
       " 8194  The cells getting Rna to make it.The building ...   \n",
       " 8195                      -The mRNA leaves the nucleus.   \n",
       " 8196                             mRNA is messenger RNA.   \n",
       " 8197  1.Copies the mRNA first2.Then transfers it3.It...   \n",
       " 8198                        tRNA mitosisrRNAand meiosis   \n",
       " 8199  1. The mRNA reaches the tRNA.2. tRNA gives the...   \n",
       " 8200  Mitocondria regulates the making of protiensth...   \n",
       " 8201  After the mRNA leaves the nucleus, it takes th...   \n",
       " 8202  1)takes instruction to the nucleus2)the nucleu...   \n",
       " 8203  After the mRNA leaves the nucleus it takes the...   \n",
       " 8204  mRNA leaves the nucleus. Then it goes to a str...   \n",
       " 8205  protein synthesis give the body protein so it ...   \n",
       " 8206                            the code has to change.   \n",
       " 8207  it takes it to whatever place it goes then tak...   \n",
       " 8208  mRNA goes to the cytoplasm.There, it makes cop...   \n",
       " 8209  When mRNA leaves the nucleus after transcripti...   \n",
       " \n",
       "                                                   clean  \\\n",
       " 6415  the mrna travels to the ribosomes at the ribos...   \n",
       " 6416  atp is created and broken down it is broken do...   \n",
       " 6417  the mrna first gets on an electron transport c...   \n",
       " 6418  the mrna then travels to the mitochondria , wh...   \n",
       " 6419  first thing the mrna does is go to the ribosom...   \n",
       " 6420                    goes to the er then golgi bodys   \n",
       " 6421  messenger rna collect the protein particals in...   \n",
       " 6422  translation the amino acid code is translatedt...   \n",
       " 6423  the mrna takes the copied information out into...   \n",
       " 6424  first the mrna leaves the cell nucleus and mov...   \n",
       " 6425  the mrna brings the protein to another place i...   \n",
       " 6426  mrna leaves the nucleus then it enters the cyt...   \n",
       " 6427  the mrna leaves the nucleus  the mrna transpor...   \n",
       " 6428        anaphase , metaphase , prophase , telophase   \n",
       " 6429  four major steps involved in protein synthesis...   \n",
       " 6430  the mrna sends a message to ribosomes to send ...   \n",
       " 6431        prophase , metaphase , anaphase , telophase   \n",
       " 6432  it sends instructions for the amino acid seque...   \n",
       " 6433  the mrna trvales through the cytoplasm , then ...   \n",
       " 6434  mrna messenger rna where it send messages thro...   \n",
       " 6435                ribsomesnucleusanaphasemoonchadrion   \n",
       " 6436  protein synthesis is needed in your body becau...   \n",
       " 6437  once mrna leaves the nucleus , it travels to t...   \n",
       " 6438  the dna then begins to reproduce , then mitosi...   \n",
       " 6439  the mrna exits the nucleus and enters the cyto...   \n",
       " 6440  when the mrna leaves the nucleus , it fits thr...   \n",
       " 6441      mitosis meiosis dna extraction blood transfer   \n",
       " 6442  ) mrna leaves the nucleus on a dna strand  ) r...   \n",
       " 6443  once the mrna leaves the nucleus it has to mak...   \n",
       " 6444  the mrna attaches to a ribosome , to begin pro...   \n",
       " ...                                                 ...   \n",
       " 8180  mrna leaves the nucleus , the amino acids form...   \n",
       " 8181  living in the envorment , to survle the envorm...   \n",
       " 8182  the mrna goes to the ribosomes to make trna st...   \n",
       " 8183  the mrna bind to the trna the anticodons attac...   \n",
       " 8184  the mrna finds an rrna and the mrna unzipps th...   \n",
       " 8185                        enzymes protein amino acids   \n",
       " 8186  four major steps in protein synthesis are to t...   \n",
       " 8187  first , the protein makes a copy of its self ,...   \n",
       " 8188  mrna it sends the message to the cells stating...   \n",
       " 8189  after mrna leaves the nucleus , it is moved to...   \n",
       " 8190  mrna leaves the nucleustravels to the ribosome...   \n",
       " 8191          translation , transcription , replication   \n",
       " 8192                        transcription , translation   \n",
       " 8193  sending messages to the whole cell , transfuri...   \n",
       " 8194  the cells getting rna to make it the building ...   \n",
       " 8195                        the mrna leaves the nucleus   \n",
       " 8196                              mrna is messenger rna   \n",
       " 8197  copies the mrna first then transfers it it the...   \n",
       " 8198                        trna mitosisrrnaand meiosis   \n",
       " 8199  the mrna reaches the trna  trna gives the dna ...   \n",
       " 8200  mitocondria regulates the making of protiensth...   \n",
       " 8201  after the mrna leaves the nucleus , it takes t...   \n",
       " 8202  ) takes instruction to the nucleus ) the nucle...   \n",
       " 8203  after the mrna leaves the nucleus it takes the...   \n",
       " 8204  mrna leaves the nucleus then it goes to a stra...   \n",
       " 8205  protein synthesis give the body protein so it ...   \n",
       " 8206                             the code has to change   \n",
       " 8207  it takes it to whatever place it goes then tak...   \n",
       " 8208  mrna goes to the cytoplasm there , it makes co...   \n",
       " 8209  when mrna leaves the nucleus after transcripti...   \n",
       " \n",
       "                                                       X  \\\n",
       " 6415  [1, 85, 681, 2, 1, 382, 78, 1, 382, 1, 85, 4, ...   \n",
       " 6416  [1220, 4, 754, 3, 1545, 248, 9, 4, 1545, 248, ...   \n",
       " 6417  [1, 85, 120, 266, 27, 71, 2373, 262, 641, 0, 0...   \n",
       " 6418  [1, 85, 31, 681, 2, 1, 781, 155, 9, 4, 31, 867...   \n",
       " 6419  [120, 170, 1, 85, 130, 4, 172, 2, 1, 355, 314,...   \n",
       " 6420  [154, 2, 1, 909, 31, 746, 4163, 0, 0, 0, 0, 0,...   \n",
       " 6421  [1183, 280, 1700, 1, 150, 9853, 197, 1, 180, 7...   \n",
       " 6422  [650, 1, 212, 337, 700, 4, 9856, 1, 212, 337, ...   \n",
       " 6423  [1, 85, 392, 1, 867, 67, 53, 72, 1, 578, 103, ...   \n",
       " 6424  [120, 1, 85, 93, 1, 46, 180, 3, 443, 2, 1, 355...   \n",
       " 6425  [1, 85, 720, 1, 150, 2, 164, 310, 5, 1, 46, 33...   \n",
       " 6426  [85, 93, 1, 180, 31, 9, 860, 1, 5948, 3, 1976,...   \n",
       " 6427  [1, 85, 93, 1, 180, 1, 85, 1145, 2, 1, 966, 1,...   \n",
       " 6428  [634, 713, 666, 956, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       " 6429  [196, 343, 490, 882, 5, 150, 441, 11, 38, 1563...   \n",
       " 6430  [1, 85, 670, 6, 613, 2, 382, 2, 895, 1166, 1, ...   \n",
       " 6431  [666, 713, 634, 956, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       " 6432  [9, 670, 786, 20, 1, 212, 337, 512, 79, 6, 150...   \n",
       " 6433  [1, 85, 9860, 133, 1, 578, 31, 860, 1, 355, 31...   \n",
       " 6434  [85, 1183, 280, 155, 9, 895, 1046, 509, 1, 594...   \n",
       " 6435  [9862, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       " 6436  [150, 441, 4, 269, 5, 288, 381, 13, 7, 1, 243,...   \n",
       " 6437  [434, 85, 93, 1, 180, 9, 681, 2, 1, 84, 181, 5...   \n",
       " 6438  [1, 260, 31, 583, 2, 1341, 31, 840, 392, 310, ...   \n",
       " 6439  [1, 85, 2186, 1, 180, 3, 860, 1, 578, 7, 1, 46...   \n",
       " 6440  [48, 1, 85, 93, 1, 180, 9, 1961, 133, 1, 1516,...   \n",
       " 6441  [840, 1104, 260, 9868, 933, 806, 0, 0, 0, 0, 0...   \n",
       " 6442  [85, 93, 1, 180, 27, 6, 260, 383, 382, 1087, 2...   \n",
       " 6443  [434, 1, 85, 93, 1, 180, 9, 75, 2, 97, 164, 32...   \n",
       " 6444  [1, 85, 768, 2, 6, 355, 2, 888, 150, 441, 1, 8...   \n",
       " ...                                                 ...   \n",
       " 8180  [85, 93, 1, 180, 1, 212, 311, 549, 6, 150, 146...   \n",
       " 8181  [683, 5, 1, 4685, 2, 10860, 1, 4685, 2, 29, 6,...   \n",
       " 8182  [1, 85, 154, 2, 1, 382, 2, 97, 146, 2255, 18, ...   \n",
       " 8183  [1, 85, 2492, 2, 1, 146, 1, 1166, 1072, 2, 1, ...   \n",
       " 8184  [1, 85, 335, 71, 406, 3, 1, 85, 6043, 1, 406, ...   \n",
       " 8185  [1184, 150, 212, 311, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       " 8186  [196, 343, 490, 5, 150, 441, 11, 2, 806, 1, 15...   \n",
       " 8187  [120, 1, 150, 213, 6, 880, 7, 174, 988, 77, 4,...   \n",
       " 8188  [85, 9, 670, 1, 613, 2, 1, 181, 845, 1, 10864,...   \n",
       " 8189  [114, 85, 93, 1, 180, 9, 4, 745, 2, 71, 212, 3...   \n",
       " 8190  [85, 93, 1, 10869, 2, 1, 10870, 259, 4848, 355...   \n",
       " 8191  [650, 726, 1335, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       " 8192  [726, 650, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       " 8193  [1642, 1046, 2, 1, 543, 46, 10871, 6145, 3, 29...   \n",
       " 8194  [1, 181, 481, 280, 2, 97, 9, 1, 2162, 150, 1, ...   \n",
       " 8195  [1, 85, 93, 1, 180, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       " 8196  [85, 4, 1183, 280, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       " 8197  [981, 1, 85, 120, 31, 809, 9, 9, 31, 1105, 621...   \n",
       " 8198  [146, 10872, 1104, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       " 8199  [1, 85, 1372, 1, 146, 146, 286, 1, 260, 3744, ...   \n",
       " 8200  [2047, 2618, 1, 298, 7, 10874, 914, 966, 213, ...   \n",
       " 8201  [114, 1, 85, 93, 1, 180, 9, 392, 1, 10875, 260...   \n",
       " 8202  [392, 2820, 2, 1, 180, 1, 180, 2268, 1, 260, 1...   \n",
       " 8203  [114, 1, 85, 93, 1, 180, 9, 392, 1, 1528, 9, 3...   \n",
       " 8204  [85, 93, 1, 180, 31, 9, 154, 2, 6, 383, 7, 260...   \n",
       " 8205  [150, 441, 372, 1, 381, 150, 59, 9, 26, 1979, ...   \n",
       " 8206  [1, 700, 75, 2, 365, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       " 8207  [9, 392, 9, 2, 1223, 310, 9, 154, 31, 392, 9, ...   \n",
       " 8208  [85, 154, 2, 1, 578, 103, 9, 213, 981, 7, 798,...   \n",
       " 8209  [48, 85, 93, 1, 180, 114, 726, 9, 31, 768, 2, ...   \n",
       " \n",
       "                                                     X_1  \n",
       " 6415  [1, 85, 681, 2, 1, 382, 78, 1, 382, 1, 85, 4, ...  \n",
       " 6416  [1220, 4, 754, 3, 1545, 248, 9, 4, 1545, 248, ...  \n",
       " 6417  [1, 85, 120, 266, 27, 71, 2373, 262, 641, 0, 0...  \n",
       " 6418  [1, 85, 31, 681, 2, 1, 781, 155, 9, 4, 31, 867...  \n",
       " 6419  [120, 170, 1, 85, 130, 4, 172, 2, 1, 355, 314,...  \n",
       " 6420  [154, 2, 1, 909, 31, 746, 4163, 0, 0, 0, 0, 0,...  \n",
       " 6421  [1183, 280, 1700, 1, 150, 9853, 197, 1, 180, 7...  \n",
       " 6422  [650, 1, 212, 337, 700, 4, 9856, 1, 212, 337, ...  \n",
       " 6423  [1, 85, 392, 1, 867, 67, 53, 72, 1, 578, 103, ...  \n",
       " 6424  [120, 1, 85, 93, 1, 46, 180, 3, 443, 2, 1, 355...  \n",
       " 6425  [1, 85, 720, 1, 150, 2, 164, 310, 5, 1, 46, 33...  \n",
       " 6426  [85, 93, 1, 180, 31, 9, 860, 1, 5948, 3, 1976,...  \n",
       " 6427  [1, 85, 93, 1, 180, 1, 85, 1145, 2, 1, 966, 1,...  \n",
       " 6428  [634, 713, 666, 956, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       " 6429  [196, 343, 490, 882, 5, 150, 441, 11, 38, 1563...  \n",
       " 6430  [1, 85, 670, 6, 613, 2, 382, 2, 895, 1166, 1, ...  \n",
       " 6431  [666, 713, 634, 956, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       " 6432  [9, 670, 786, 20, 1, 212, 337, 512, 79, 6, 150...  \n",
       " 6433  [1, 85, 9860, 133, 1, 578, 31, 860, 1, 355, 31...  \n",
       " 6434  [85, 1183, 280, 155, 9, 895, 1046, 509, 1, 594...  \n",
       " 6435  [9862, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       " 6436  [150, 441, 4, 269, 5, 288, 381, 13, 7, 1, 243,...  \n",
       " 6437  [434, 85, 93, 1, 180, 9, 681, 2, 1, 84, 181, 5...  \n",
       " 6438  [1, 260, 31, 583, 2, 1341, 31, 840, 392, 310, ...  \n",
       " 6439  [1, 85, 2186, 1, 180, 3, 860, 1, 578, 7, 1, 46...  \n",
       " 6440  [48, 1, 85, 93, 1, 180, 9, 1961, 133, 1, 1516,...  \n",
       " 6441  [840, 1104, 260, 9868, 933, 806, 0, 0, 0, 0, 0...  \n",
       " 6442  [85, 93, 1, 180, 27, 6, 260, 383, 382, 1087, 2...  \n",
       " 6443  [434, 1, 85, 93, 1, 180, 9, 75, 2, 97, 164, 32...  \n",
       " 6444  [1, 85, 768, 2, 6, 355, 2, 888, 150, 441, 1, 8...  \n",
       " ...                                                 ...  \n",
       " 8180  [85, 93, 1, 180, 1, 212, 311, 549, 6, 150, 146...  \n",
       " 8181  [683, 5, 1, 4685, 2, 10860, 1, 4685, 2, 29, 6,...  \n",
       " 8182  [1, 85, 154, 2, 1, 382, 2, 97, 146, 2255, 18, ...  \n",
       " 8183  [1, 85, 2492, 2, 1, 146, 1, 1166, 1072, 2, 1, ...  \n",
       " 8184  [1, 85, 335, 71, 406, 3, 1, 85, 6043, 1, 406, ...  \n",
       " 8185  [1184, 150, 212, 311, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       " 8186  [196, 343, 490, 5, 150, 441, 11, 2, 806, 1, 15...  \n",
       " 8187  [120, 1, 150, 213, 6, 880, 7, 174, 988, 77, 4,...  \n",
       " 8188  [85, 9, 670, 1, 613, 2, 1, 181, 845, 1, 10864,...  \n",
       " 8189  [114, 85, 93, 1, 180, 9, 4, 745, 2, 71, 212, 3...  \n",
       " 8190  [85, 93, 1, 10869, 2, 1, 10870, 259, 4848, 355...  \n",
       " 8191  [650, 726, 1335, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       " 8192  [726, 650, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       " 8193  [1642, 1046, 2, 1, 543, 46, 10871, 6145, 3, 29...  \n",
       " 8194  [1, 181, 481, 280, 2, 97, 9, 1, 2162, 150, 1, ...  \n",
       " 8195  [1, 85, 93, 1, 180, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       " 8196  [85, 4, 1183, 280, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       " 8197  [981, 1, 85, 120, 31, 809, 9, 9, 31, 1105, 621...  \n",
       " 8198  [146, 10872, 1104, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       " 8199  [1, 85, 1372, 1, 146, 146, 286, 1, 260, 3744, ...  \n",
       " 8200  [2047, 2618, 1, 298, 7, 10874, 914, 966, 213, ...  \n",
       " 8201  [114, 1, 85, 93, 1, 180, 9, 392, 1, 10875, 260...  \n",
       " 8202  [392, 2820, 2, 1, 180, 1, 180, 2268, 1, 260, 1...  \n",
       " 8203  [114, 1, 85, 93, 1, 180, 9, 392, 1, 1528, 9, 3...  \n",
       " 8204  [85, 93, 1, 180, 31, 9, 154, 2, 6, 383, 7, 260...  \n",
       " 8205  [150, 441, 372, 1, 381, 150, 59, 9, 26, 1979, ...  \n",
       " 8206  [1, 700, 75, 2, 365, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       " 8207  [9, 392, 9, 2, 1223, 310, 9, 154, 31, 392, 9, ...  \n",
       " 8208  [85, 154, 2, 1, 578, 103, 9, 213, 981, 7, 798,...  \n",
       " 8209  [48, 85, 93, 1, 180, 114, 726, 9, 31, 768, 2, ...  \n",
       " \n",
       " [1795 rows x 8 columns],           Id  EssaySet  Score1  Score2  \\\n",
       " 8210   13959         6       0       0   \n",
       " 8211   13960         6       0       0   \n",
       " 8212   13961         6       0       0   \n",
       " 8213   13962         6       0       0   \n",
       " 8214   13963         6       0       0   \n",
       " 8215   13964         6       0       0   \n",
       " 8216   13965         6       0       0   \n",
       " 8217   13966         6       0       0   \n",
       " 8218   13967         6       0       0   \n",
       " 8219   13968         6       0       0   \n",
       " 8220   13969         6       0       0   \n",
       " 8221   13970         6       0       0   \n",
       " 8222   13971         6       1       1   \n",
       " 8223   13972         6       0       0   \n",
       " 8224   13973         6       0       0   \n",
       " 8225   13974         6       0       0   \n",
       " 8226   13975         6       0       0   \n",
       " 8227   13976         6       0       0   \n",
       " 8228   13977         6       0       0   \n",
       " 8229   13978         6       0       0   \n",
       " 8230   13979         6       0       0   \n",
       " 8231   13980         6       0       0   \n",
       " 8232   13981         6       0       0   \n",
       " 8233   13982         6       0       0   \n",
       " 8234   13983         6       0       0   \n",
       " 8235   13984         6       0       0   \n",
       " 8236   13985         6       0       0   \n",
       " 8237   13986         6       0       0   \n",
       " 8238   13987         6       0       0   \n",
       " 8239   13988         6       0       0   \n",
       " ...      ...       ...     ...     ...   \n",
       " 9977   15726         6       1       1   \n",
       " 9978   15727         6       0       0   \n",
       " 9979   15728         6       3       3   \n",
       " 9980   15729         6       0       0   \n",
       " 9981   15730         6       0       0   \n",
       " 9982   15731         6       0       0   \n",
       " 9983   15732         6       0       0   \n",
       " 9984   15733         6       0       0   \n",
       " 9985   15734         6       0       0   \n",
       " 9986   15735         6       0       0   \n",
       " 9987   15736         6       0       0   \n",
       " 9988   15737         6       0       1   \n",
       " 9989   15738         6       3       3   \n",
       " 9990   15739         6       0       0   \n",
       " 9991   15740         6       0       0   \n",
       " 9992   15741         6       0       0   \n",
       " 9993   15742         6       0       0   \n",
       " 9994   15743         6       0       0   \n",
       " 9995   15744         6       3       2   \n",
       " 9996   15745         6       0       0   \n",
       " 9997   15746         6       0       0   \n",
       " 9998   15747         6       3       3   \n",
       " 9999   15748         6       1       1   \n",
       " 10000  15749         6       0       0   \n",
       " 10001  15750         6       0       0   \n",
       " 10002  15751         6       0       0   \n",
       " 10003  15752         6       0       0   \n",
       " 10004  15753         6       0       0   \n",
       " 10005  15754         6       0       0   \n",
       " 10006  15755         6       0       0   \n",
       " \n",
       "                                                EssayText  \\\n",
       " 8210   1.they do mitosis so they can stay alive if on...   \n",
       " 8211                          MITOSIS MEOSIS TRANSLATION   \n",
       " 8212   1. The plasma membrane itself helps to control...   \n",
       " 8213   The cell can tell the substances to move acros...   \n",
       " 8214   The membrane recognizes and lets needed protie...   \n",
       " 8215   1.The structure og the cell's is the same2.liv...   \n",
       " 8216   respirationmovement of foods and protiens in a...   \n",
       " 8217   1. The postion of the cell sometimes wont let ...   \n",
       " 8218                                       cellular wall   \n",
       " 8219   First they decide if the substances is good or...   \n",
       " 8220   Proteins are carried by ribosomes to the golgi...   \n",
       " 8221                      Osmosis, Mitosis, and Cytosis.   \n",
       " 8222   Osmosious controls the movement of water acros...   \n",
       " 8223   All cells are living.Some cells make other cells.   \n",
       " 8224   Transportation so the cell can move particles ...   \n",
       " 8225   The membrane is permeable which allows smaller...   \n",
       " 8226   anaphase: Cells splittelophase: Cells start to...   \n",
       " 8227   Cell Wall- controls what goes in and outCell M...   \n",
       " 8228                     The cell membrane uses enzymes.   \n",
       " 8229   PHILANGES AND TINY HAIRLIKE STRUCTURES ON THE ...   \n",
       " 8230                                    cell respiration   \n",
       " 8231   Osmosis is used to move molecules across the c...   \n",
       " 8232   equilbrium of substances from higt to low conc...   \n",
       " 8233   Three processes used by cells to control the m...   \n",
       " 8234                            into and out of the cell   \n",
       " 8235   Trna is a movement of a type of cell across th...   \n",
       " 8236   Three processes are when the cell usese other ...   \n",
       " 8237   The cells use active transport, osmosis, and d...   \n",
       " 8238                                        Replication-   \n",
       " 8239   Cells use the process of diffusion to take in ...   \n",
       " ...                                                  ...   \n",
       " 9977   Cells can use the process of osmosis which has...   \n",
       " 9978                             PHOTOSYNTHESISFLAGELLUM   \n",
       " 9979   Passive Transport-where no energy is needed fo...   \n",
       " 9980   Protien moves aross the cell membrane when som...   \n",
       " 9981   Sun,Water, and SoilIt makes the plant stay ali...   \n",
       " 9982                                        mRNAtRNArRNA   \n",
       " 9983   The cells use prophase. Then they use metaphas...   \n",
       " 9984                      mitochondriagolgibodiesnucleus   \n",
       " 9985   1)mRNA     2)protein synthesis     3)photosynt...   \n",
       " 9986   Cell wall- allows certain things to enter and ...   \n",
       " 9987   They use translation, transcription, and repli...   \n",
       " 9988   When a substance tries to move into a cell, th...   \n",
       " 9989   Osmosis is where water helps move things in an...   \n",
       " 9990                            MITOSISMEIOSISINTERPHASE   \n",
       " 9991                         membrane, chloraph, nuecles   \n",
       " 9992   The three processes that are used by the cells...   \n",
       " 9993   The three processes that the cell uses to conr...   \n",
       " 9994   cytoplasm is used to move particles throughout...   \n",
       " 9995   Three processes used by cells to control the m...   \n",
       " 9996   Flagella are used for movement by the cell. Ce...   \n",
       " 9997                                nucleuscell membrane   \n",
       " 9998   The plasma membrane is selectivly permeable.  ...   \n",
       " 9999   1) Osmosis, which is the movement of water int...   \n",
       " 10000  use there tails to whip around and transfer in...   \n",
       " 10001                  MOVEMENNREPRODUCINGGATHERING FOOD   \n",
       " 10002  1.kill the white cells2.clean throughout your ...   \n",
       " 10003  first they have to be a substance that the cel...   \n",
       " 10004                                   OSMISOSDIFFUSION   \n",
       " 10005         Krebs CycleCellular respirationCell Theory   \n",
       " 10006  they let things in and out. They have instruct...   \n",
       " \n",
       "                                                    clean  \\\n",
       " 8210   they do mitosis so they can stay alive if one ...   \n",
       " 8211                          mitosis meosis translation   \n",
       " 8212   the plasma membrane itself helps to control th...   \n",
       " 8213   the cell can tell the substances to move acros...   \n",
       " 8214   the membrane recognizes and lets needed protie...   \n",
       " 8215   the structure og the cell 's is the same livin...   \n",
       " 8216   respirationmovement of foods and protiens in a...   \n",
       " 8217   the postion of the cell sometimes wont let the...   \n",
       " 8218                                       cellular wall   \n",
       " 8219   first they decide if the substances is good or...   \n",
       " 8220   proteins are carried by ribosomes to the golgi...   \n",
       " 8221                     osmosis , mitosis , and cytosis   \n",
       " 8222   osmosious controls the movement of water acros...   \n",
       " 8223    all cells are living some cells make other cells   \n",
       " 8224   transportation so the cell can move particles ...   \n",
       " 8225   the membrane is permeable which allows smaller...   \n",
       " 8226   anaphase cells splittelophase cells start to s...   \n",
       " 8227   cell wall controls what goes in and outcell me...   \n",
       " 8228                      the cell membrane uses enzymes   \n",
       " 8229   philanges and tiny hairlike structures on the ...   \n",
       " 8230                                    cell respiration   \n",
       " 8231   osmosis is used to move molecules across the c...   \n",
       " 8232   equilbrium of substances from higt to low conc...   \n",
       " 8233   three processes used by cells to control the m...   \n",
       " 8234                            into and out of the cell   \n",
       " 8235   trna is a movement of a type of cell across th...   \n",
       " 8236   three processes are when the cell usese other ...   \n",
       " 8237   the cells use active transport , osmosis , and...   \n",
       " 8238                                         replication   \n",
       " 8239   cells use the process of diffusion to take in ...   \n",
       " ...                                                  ...   \n",
       " 9977   cells can use the process of osmosis which has...   \n",
       " 9978                             photosynthesisflagellum   \n",
       " 9979   passive transport where no energy is needed fo...   \n",
       " 9980   protien moves aross the cell membrane when som...   \n",
       " 9981   sun , water , and soilit makes the plant stay ...   \n",
       " 9982                                        mrnatrnarrna   \n",
       " 9983   the cells use prophase then they use metaphase...   \n",
       " 9984                      mitochondriagolgibodiesnucleus   \n",
       " 9985       ) mrna  ) protein synthesis  ) photosynthesis   \n",
       " 9986   cell wall allows certain things to enter and e...   \n",
       " 9987   they use translation , transcription , and rep...   \n",
       " 9988   when a substance tries to move into a cell , t...   \n",
       " 9989   osmosis is where water helps move things in an...   \n",
       " 9990                            mitosismeiosisinterphase   \n",
       " 9991                       membrane , chloraph , nuecles   \n",
       " 9992   the three processes that are used by the cells...   \n",
       " 9993   the three processes that the cell uses to conr...   \n",
       " 9994   cytoplasm is used to move particles throughout...   \n",
       " 9995   three processes used by cells to control the m...   \n",
       " 9996   flagella are used for movement by the cell cel...   \n",
       " 9997                                nucleuscell membrane   \n",
       " 9998   the plasma membrane is selectivly permeable it...   \n",
       " 9999   ) osmosis , which is the movement of water int...   \n",
       " 10000  use there tails to whip around and transfer in...   \n",
       " 10001                  movemennreproducinggathering food   \n",
       " 10002  kill the white cells clean throughout your bod...   \n",
       " 10003  first they have to be a substance that the cel...   \n",
       " 10004                                   osmisosdiffusion   \n",
       " 10005         krebs cyclecellular respirationcell theory   \n",
       " 10006  they let things in and out they have instructi...   \n",
       " \n",
       "                                                        X  \\\n",
       " 8210   [10, 69, 840, 59, 10, 26, 421, 2031, 79, 44, 4...   \n",
       " 8211   [840, 2116, 650, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       " 8212   [1, 1397, 115, 798, 333, 2, 237, 1, 258, 7, 16...   \n",
       " 8213   [1, 46, 26, 175, 1, 217, 2, 210, 245, 1, 115, ...   \n",
       " 8214   [1, 115, 4211, 3, 711, 269, 617, 172, 72, 1, 4...   \n",
       " 8215   [1, 1383, 1804, 1, 46, 36, 4, 1, 102, 683, 46,...   \n",
       " 8216   [10883, 7, 723, 3, 617, 5, 3, 53, 7, 1, 46, 0,...   \n",
       " 8217   [1, 4223, 7, 1, 46, 1214, 604, 408, 1, 46, 210...   \n",
       " 8218   [1120, 517, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       " 8219   [120, 10, 2111, 79, 1, 217, 4, 134, 62, 357, 1...   \n",
       " 8220   [243, 11, 1566, 30, 382, 2, 1, 746, 1025, 2193...   \n",
       " 8221   [320, 840, 3, 10890, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       " 8222   [10891, 653, 1, 258, 7, 221, 245, 1, 46, 271, ...   \n",
       " 8223   [87, 181, 11, 683, 163, 181, 97, 84, 181, 0, 0...   \n",
       " 8224   [1619, 59, 1, 46, 26, 210, 743, 245, 1, 1397, ...   \n",
       " 8225   [1, 115, 4, 982, 77, 642, 974, 181, 2, 172, 13...   \n",
       " 8226   [634, 181, 10893, 181, 371, 2, 10894, 1588, 13...   \n",
       " 8227   [46, 517, 653, 23, 154, 5, 3, 10896, 115, 41, ...   \n",
       " 8228   [1, 46, 115, 330, 1184, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       " 8229   [10899, 3, 923, 3406, 1647, 27, 1, 544, 7, 6, ...   \n",
       " 8230   [46, 997, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       " 8231   [320, 4, 105, 2, 210, 1014, 245, 1, 46, 36, 21...   \n",
       " 8232   [10900, 7, 217, 42, 10901, 2, 640, 10902, 271,...   \n",
       " 8233   [242, 503, 105, 30, 181, 2, 237, 1, 258, 7, 21...   \n",
       " 8234   [72, 3, 53, 7, 1, 46, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       " 8235   [146, 4, 6, 258, 7, 6, 76, 7, 46, 245, 1, 46, ...   \n",
       " 8236   [242, 503, 11, 48, 1, 46, 10904, 84, 181, 1, 1...   \n",
       " 8237   [1, 181, 117, 444, 262, 320, 3, 271, 2, 237, 1...   \n",
       " 8238   [1335, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       " 8239   [181, 117, 1, 301, 7, 271, 2, 332, 5, 1, 217, ...   \n",
       " ...                                                  ...   \n",
       " 9977   [181, 26, 117, 1, 301, 7, 320, 77, 75, 221, 60...   \n",
       " 9978   [12006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       " 9979   [581, 262, 155, 277, 230, 4, 269, 20, 1, 1619,...   \n",
       " 9980   [555, 443, 12008, 1, 46, 115, 48, 204, 4, 49, ...   \n",
       " 9981   [526, 221, 3, 12009, 213, 1, 702, 421, 2031, 5...   \n",
       " 9982   [6078, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       " 9983   [1, 181, 117, 666, 31, 10, 117, 713, 2, 391, 1...   \n",
       " 9984   [12011, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       " 9985   [85, 150, 441, 1679, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       " 9986   [46, 517, 642, 270, 119, 2, 719, 3, 1980, 1, 4...   \n",
       " 9987   [10, 117, 650, 726, 3, 1335, 0, 0, 0, 0, 0, 0,...   \n",
       " 9988   [48, 6, 502, 520, 2, 210, 72, 6, 46, 1, 46, 29...   \n",
       " 9989   [320, 4, 155, 221, 333, 210, 119, 5, 3, 53, 7,...   \n",
       " 9990   [12013, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       " 9991   [115, 12014, 12015, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       " 9992   [1, 242, 503, 8, 11, 105, 30, 1, 181, 2, 210, ...   \n",
       " 9993   [1, 242, 503, 8, 1, 46, 330, 2, 6313, 1, 258, ...   \n",
       " 9994   [578, 4, 105, 2, 210, 743, 509, 1, 46, 1, 1128...   \n",
       " 9995   [242, 503, 105, 30, 181, 2, 237, 1, 258, 7, 21...   \n",
       " 9996   [1128, 11, 105, 20, 258, 30, 1, 46, 1120, 997,...   \n",
       " 9997   [6006, 115, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       " 9998   [1, 1397, 115, 4, 4948, 982, 9, 711, 6342, 5, ...   \n",
       " 9999   [320, 77, 4, 1, 258, 7, 221, 72, 3, 53, 7, 1, ...   \n",
       " 10000  [117, 103, 2620, 2, 2386, 256, 3, 806, 67, 10,...   \n",
       " 10001  [12018, 159, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       " 10002  [1218, 1, 63, 181, 1943, 509, 288, 381, 12019,...   \n",
       " 10003  [120, 10, 29, 2, 14, 6, 502, 8, 1, 46, 1056, 3...   \n",
       " 10004  [12021, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       " 10005  [4879, 12022, 6269, 3135, 0, 0, 0, 0, 0, 0, 0,...   \n",
       " 10006  [10, 408, 119, 5, 3, 53, 10, 29, 786, 20, 1202...   \n",
       " \n",
       "                                                      X_1  \n",
       " 8210   [10, 69, 840, 59, 10, 26, 421, 2031, 79, 44, 4...  \n",
       " 8211   [840, 2116, 650, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       " 8212   [1, 1397, 115, 798, 333, 2, 237, 1, 258, 7, 16...  \n",
       " 8213   [1, 46, 26, 175, 1, 217, 2, 210, 245, 1, 115, ...  \n",
       " 8214   [1, 115, 4211, 3, 711, 269, 617, 172, 72, 1, 4...  \n",
       " 8215   [1, 1383, 1804, 1, 46, 36, 4, 1, 102, 683, 46,...  \n",
       " 8216   [10883, 7, 723, 3, 617, 5, 3, 53, 7, 1, 46, 0,...  \n",
       " 8217   [1, 4223, 7, 1, 46, 1214, 604, 408, 1, 46, 210...  \n",
       " 8218   [1120, 517, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       " 8219   [120, 10, 2111, 79, 1, 217, 4, 134, 62, 357, 1...  \n",
       " 8220   [243, 11, 1566, 30, 382, 2, 1, 746, 1025, 2193...  \n",
       " 8221   [320, 840, 3, 10890, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       " 8222   [10891, 653, 1, 258, 7, 221, 245, 1, 46, 271, ...  \n",
       " 8223   [87, 181, 11, 683, 163, 181, 97, 84, 181, 0, 0...  \n",
       " 8224   [1619, 59, 1, 46, 26, 210, 743, 245, 1, 1397, ...  \n",
       " 8225   [1, 115, 4, 982, 77, 642, 974, 181, 2, 172, 13...  \n",
       " 8226   [634, 181, 10893, 181, 371, 2, 10894, 1588, 13...  \n",
       " 8227   [46, 517, 653, 23, 154, 5, 3, 10896, 115, 41, ...  \n",
       " 8228   [1, 46, 115, 330, 1184, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       " 8229   [10899, 3, 923, 3406, 1647, 27, 1, 544, 7, 6, ...  \n",
       " 8230   [46, 997, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       " 8231   [320, 4, 105, 2, 210, 1014, 245, 1, 46, 36, 21...  \n",
       " 8232   [10900, 7, 217, 42, 10901, 2, 640, 10902, 271,...  \n",
       " 8233   [242, 503, 105, 30, 181, 2, 237, 1, 258, 7, 21...  \n",
       " 8234   [72, 3, 53, 7, 1, 46, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       " 8235   [146, 4, 6, 258, 7, 6, 76, 7, 46, 245, 1, 46, ...  \n",
       " 8236   [242, 503, 11, 48, 1, 46, 10904, 84, 181, 1, 1...  \n",
       " 8237   [1, 181, 117, 444, 262, 320, 3, 271, 2, 237, 1...  \n",
       " 8238   [1335, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       " 8239   [181, 117, 1, 301, 7, 271, 2, 332, 5, 1, 217, ...  \n",
       " ...                                                  ...  \n",
       " 9977   [181, 26, 117, 1, 301, 7, 320, 77, 75, 221, 60...  \n",
       " 9978   [12006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       " 9979   [581, 262, 155, 277, 230, 4, 269, 20, 1, 1619,...  \n",
       " 9980   [555, 443, 12008, 1, 46, 115, 48, 204, 4, 49, ...  \n",
       " 9981   [526, 221, 3, 12009, 213, 1, 702, 421, 2031, 5...  \n",
       " 9982   [6078, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       " 9983   [1, 181, 117, 666, 31, 10, 117, 713, 2, 391, 1...  \n",
       " 9984   [12011, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       " 9985   [85, 150, 441, 1679, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       " 9986   [46, 517, 642, 270, 119, 2, 719, 3, 1980, 1, 4...  \n",
       " 9987   [10, 117, 650, 726, 3, 1335, 0, 0, 0, 0, 0, 0,...  \n",
       " 9988   [48, 6, 502, 520, 2, 210, 72, 6, 46, 1, 46, 29...  \n",
       " 9989   [320, 4, 155, 221, 333, 210, 119, 5, 3, 53, 7,...  \n",
       " 9990   [12013, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       " 9991   [115, 12014, 12015, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       " 9992   [1, 242, 503, 8, 11, 105, 30, 1, 181, 2, 210, ...  \n",
       " 9993   [1, 242, 503, 8, 1, 46, 330, 2, 6313, 1, 258, ...  \n",
       " 9994   [578, 4, 105, 2, 210, 743, 509, 1, 46, 1, 1128...  \n",
       " 9995   [242, 503, 105, 30, 181, 2, 237, 1, 258, 7, 21...  \n",
       " 9996   [1128, 11, 105, 20, 258, 30, 1, 46, 1120, 997,...  \n",
       " 9997   [6006, 115, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       " 9998   [1, 1397, 115, 4, 4948, 982, 9, 711, 6342, 5, ...  \n",
       " 9999   [320, 77, 4, 1, 258, 7, 221, 72, 3, 53, 7, 1, ...  \n",
       " 10000  [117, 103, 2620, 2, 2386, 256, 3, 806, 67, 10,...  \n",
       " 10001  [12018, 159, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       " 10002  [1218, 1, 63, 181, 1943, 509, 288, 381, 12019,...  \n",
       " 10003  [120, 10, 29, 2, 14, 6, 502, 8, 1, 46, 1056, 3...  \n",
       " 10004  [12021, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       " 10005  [4879, 12022, 6269, 3135, 0, 0, 0, 0, 0, 0, 0,...  \n",
       " 10006  [10, 408, 119, 5, 3, 53, 10, 29, 786, 20, 1202...  \n",
       " \n",
       " [1797 rows x 8 columns],           Id  EssaySet  Score1  Score2  \\\n",
       " 10007  16954         7       1       1   \n",
       " 10008  16955         7       1       1   \n",
       " 10009  16956         7       2       2   \n",
       " 10010  16957         7       2       2   \n",
       " 10011  16958         7       1       2   \n",
       " 10012  16959         7       0       1   \n",
       " 10013  16960         7       2       2   \n",
       " 10014  16961         7       0       0   \n",
       " 10015  16962         7       0       0   \n",
       " 10016  16963         7       1       1   \n",
       " 10017  16964         7       0       0   \n",
       " 10018  16965         7       2       2   \n",
       " 10019  16966         7       2       2   \n",
       " 10020  16967         7       0       0   \n",
       " 10021  16968         7       0       0   \n",
       " 10022  16969         7       0       0   \n",
       " 10023  16970         7       0       0   \n",
       " 10024  16971         7       0       0   \n",
       " 10025  16972         7       0       0   \n",
       " 10026  16973         7       1       1   \n",
       " 10027  16974         7       1       1   \n",
       " 10028  16975         7       1       1   \n",
       " 10029  16976         7       0       0   \n",
       " 10030  16977         7       2       2   \n",
       " 10031  16978         7       1       1   \n",
       " 10032  16979         7       2       2   \n",
       " 10033  16980         7       1       1   \n",
       " 10034  16981         7       1       1   \n",
       " 10035  16982         7       2       2   \n",
       " 10036  16983         7       0       0   \n",
       " ...      ...       ...     ...     ...   \n",
       " 11776  18723         7       1       1   \n",
       " 11777  18724         7       1       1   \n",
       " 11778  18725         7       0       0   \n",
       " 11779  18726         7       1       1   \n",
       " 11780  18727         7       2       2   \n",
       " 11781  18728         7       1       1   \n",
       " 11782  18729         7       0       1   \n",
       " 11783  18730         7       2       2   \n",
       " 11784  18731         7       0       0   \n",
       " 11785  18732         7       0       0   \n",
       " 11786  18733         7       2       2   \n",
       " 11787  18734         7       1       1   \n",
       " 11788  18735         7       2       2   \n",
       " 11789  18736         7       1       1   \n",
       " 11790  18737         7       2       2   \n",
       " 11791  18738         7       1       1   \n",
       " 11792  18739         7       0       0   \n",
       " 11793  18740         7       0       0   \n",
       " 11794  18741         7       0       0   \n",
       " 11795  18742         7       0       0   \n",
       " 11796  18743         7       1       1   \n",
       " 11797  18744         7       1       1   \n",
       " 11798  18745         7       0       0   \n",
       " 11799  18746         7       0       0   \n",
       " 11800  18747         7       1       1   \n",
       " 11801  18748         7       0       0   \n",
       " 11802  18749         7       2       2   \n",
       " 11803  18750         7       0       0   \n",
       " 11804  18751         7       1       1   \n",
       " 11805  18752         7       1       1   \n",
       " \n",
       "                                                EssayText  \\\n",
       " 10007  Rose wants to help the family but feels weighe...   \n",
       " 10008  Rose can be described by the trait of responsa...   \n",
       " 10009  Rose is thoughtful and caring.  She has plenty...   \n",
       " 10010  One trait that describes Rose is understanding...   \n",
       " 10011  She feels weighed down by helping the family a...   \n",
       " 10012  Rose is a tired hard working girl. When she ta...   \n",
       " 10013  Based on her conversation with Anna, Rose come...   \n",
       " 10014  Rose conversations were based on there trait t...   \n",
       " 10015  One trait that can describe Rose is that she i...   \n",
       " 10016  One trait would be that she is smart and is a ...   \n",
       " 10017  The time Rose was talking to Anna about why th...   \n",
       " 10018  Rose is a very optimistic person. In her conve...   \n",
       " 10019  she is caring she doesnt want to tell her aunt...   \n",
       " 10020  Rose is pressured.  She reveals this trait as ...   \n",
       " 10021  Rose is sweat and wouldn't want to hurt anyone...   \n",
       " 10022  Anna was trying to help sounding like their mo...   \n",
       " 10023  'You sound just like Mama.' it was when Anna a...   \n",
       " 10024  One trait that Rose has is that she is underst...   \n",
       " 10025  Anna glared at Rose.'Money isn't everything.' ...   \n",
       " 10026  One trait that can descirbe Rose is responsibl...   \n",
       " 10027  One trait that Rose has is selflessness. I kno...   \n",
       " 10028  Based on Rose's attitude from the story, she s...   \n",
       " 10029  I think shes just feelin alot of pressure with...   \n",
       " 10030  From her conversations, Rose seems to be a car...   \n",
       " 10031  Rose is grateful. She is willing to help the f...   \n",
       " 10032  In the story Rose is a very giving person. She...   \n",
       " 10033  She is steadfast in her loyalty to her family....   \n",
       " 10034  One trait that can describe Rose is that she i...   \n",
       " 10035  One trait that can describe Rose is thoughtful...   \n",
       " 10036  Rose and Anna both want their mom and dad to s...   \n",
       " ...                                                  ...   \n",
       " 11776  Rose can be described as selfless. She puts th...   \n",
       " 11777  One trait that I can see in Rose is that she i...   \n",
       " 11778  Rose can quickly see the better things that ca...   \n",
       " 11779  I think Rose is very caring.  It seems like sh...   \n",
       " 11780  She is considerate. Paragraph 19- Rose didn't ...   \n",
       " 11781                              Shes a caring person.   \n",
       " 11782  Rose doesn't want to hurt anyone's feelings by...   \n",
       " 11783  Rose seems like she is a caring and providing ...   \n",
       " 11784      thinks she knows everything.she ignores anna.   \n",
       " 11785  Rose has this trait that seems to show that sh...   \n",
       " 11786  Rose cares very deeply about others. She tries...   \n",
       " 11787  She is hard working,' From school she'd gone s...   \n",
       " 11788  Rose is dedicated to her family.  This is prov...   \n",
       " 11789  One trait that Rose has that stands out his he...   \n",
       " 11790  Rose displays an outward optimism in her conve...   \n",
       " 11791  One trait that describes Rose based on her con...   \n",
       " 11792  Rose thought she is to young to have all the r...   \n",
       " 11793  one trait of Rose is that she hides her feelin...   \n",
       " 11794  Rose is a strong person. She wants to be a roc...   \n",
       " 11795  Rose hides her emotions for the sake of others...   \n",
       " 11796  Rose seems to be very responsible. She had a j...   \n",
       " 11797  Rose is very helpfull, which is a lovely trait...   \n",
       " 11798  happy cause she trys to cheer her sister up.  ...   \n",
       " 11799  she is scared and dont know what to do, she fe...   \n",
       " 11800                          That she is a hardworker.   \n",
       " 11801  A person that trying to be sucsecfule and dowh...   \n",
       " 11802  A trait that Rose haswould be hope. We can see...   \n",
       " 11803  She is from Cambodia. 'In Cambodia, our first ...   \n",
       " 11804  I think that Rose stands her grounds. She is v...   \n",
       " 11805  Rose is very responsible. She is involved with...   \n",
       " \n",
       "                                                    clean  \\\n",
       " 10007  rose wants to help the family but feels weighe...   \n",
       " 10008  rose can be described by the trait of responsa...   \n",
       " 10009  rose is thoughtful and caring she has plenty o...   \n",
       " 10010  one trait that describes rose is understanding...   \n",
       " 10011  she feels weighed down by helping the family a...   \n",
       " 10012  rose is a tired hard working girl when she tal...   \n",
       " 10013  based on her conversation with anna , rose com...   \n",
       " 10014  rose conversations were based on there trait t...   \n",
       " 10015  one trait that can describe rose is that she i...   \n",
       " 10016  one trait would be that she is smart and is a ...   \n",
       " 10017  the time rose was talking to anna about why th...   \n",
       " 10018  rose is a very optimistic person in her conver...   \n",
       " 10019  she is caring she doesnt want to tell her aunt...   \n",
       " 10020  rose is pressured she reveals this trait as sh...   \n",
       " 10021  rose is sweat and would n't want to hurt anyon...   \n",
       " 10022  anna was trying to help sounding like their mo...   \n",
       " 10023  'you sound just like mama ' it was when anna a...   \n",
       " 10024  one trait that rose has is that she is underst...   \n",
       " 10025  anna glared at rose 'money is n't everything '...   \n",
       " 10026  one trait that can descirbe rose is responsibl...   \n",
       " 10027  one trait that rose has is selflessness i know...   \n",
       " 10028  based on rose 's attitude from the story , she...   \n",
       " 10029  i think shes just feelin alot of pressure with...   \n",
       " 10030  from her conversations , rose seems to be a ca...   \n",
       " 10031  rose is grateful she is willing to help the fa...   \n",
       " 10032  in the story rose is a very giving person she ...   \n",
       " 10033  she is steadfast in her loyalty to her family ...   \n",
       " 10034  one trait that can describe rose is that she i...   \n",
       " 10035  one trait that can describe rose is thoughtful...   \n",
       " 10036  rose and anna both want their mom and dad to s...   \n",
       " ...                                                  ...   \n",
       " 11776  rose can be described as selfless she puts the...   \n",
       " 11777  one trait that i can see in rose is that she i...   \n",
       " 11778  rose can quickly see the better things that ca...   \n",
       " 11779  i think rose is very caring it seems like she ...   \n",
       " 11780  she is considerate paragraph  rose did n't ans...   \n",
       " 11781                               shes a caring person   \n",
       " 11782  rose does n't want to hurt anyone 's feelings ...   \n",
       " 11783  rose seems like she is a caring and providing ...   \n",
       " 11784       thinks she knows everything she ignores anna   \n",
       " 11785  rose has this trait that seems to show that sh...   \n",
       " 11786  rose cares very deeply about others she tries ...   \n",
       " 11787  she is hard working , ' from school she 'd gon...   \n",
       " 11788  rose is dedicated to her family this is proven...   \n",
       " 11789  one trait that rose has that stands out his he...   \n",
       " 11790  rose displays an outward optimism in her conve...   \n",
       " 11791  one trait that describes rose based on her con...   \n",
       " 11792  rose thought she is to young to have all the r...   \n",
       " 11793  one trait of rose is that she hides her feelin...   \n",
       " 11794  rose is a strong person she wants to be a rock...   \n",
       " 11795  rose hides her emotions for the sake of others...   \n",
       " 11796  rose seems to be very responsible she had a jo...   \n",
       " 11797  rose is very helpfull , which is a lovely trai...   \n",
       " 11798  happy cause she trys to cheer her sister up de...   \n",
       " 11799  she is scared and dont know what to do , she f...   \n",
       " 11800                           that she is a hardworker   \n",
       " 11801  a person that trying to be sucsecfule and dowh...   \n",
       " 11802  a trait that rose haswould be hope we can see ...   \n",
       " 11803  she is from cambodia 'in cambodia , our first ...   \n",
       " 11804  i think that rose stands her grounds she is ve...   \n",
       " 11805  rose is very responsible she is involved with ...   \n",
       " \n",
       "                                                        X  \\\n",
       " 10007  [60, 279, 2, 86, 1, 177, 52, 218, 497, 248, 8,...   \n",
       " 10008  [60, 26, 14, 918, 30, 1, 295, 7, 3774, 18, 4, ...   \n",
       " 10009  [60, 4, 1982, 3, 469, 17, 75, 4777, 7, 769, 62...   \n",
       " 10010  [44, 295, 8, 428, 60, 4, 772, 48, 17, 19, 294,...   \n",
       " 10011  [17, 218, 497, 248, 30, 488, 1, 177, 3, 130, 5...   \n",
       " 10012  [60, 4, 6, 1489, 283, 456, 937, 48, 17, 289, 2...   \n",
       " 10013  [249, 27, 25, 449, 28, 161, 60, 463, 245, 38, ...   \n",
       " 10014  [60, 893, 132, 249, 27, 103, 295, 103, 11, 87,...   \n",
       " 10015  [44, 295, 8, 26, 375, 60, 4, 8, 17, 4, 795, 32...   \n",
       " 10016  [44, 295, 15, 14, 8, 17, 4, 1548, 3, 4, 6, 283...   \n",
       " 10017  [1, 145, 60, 19, 294, 2, 161, 33, 171, 1832, 8...   \n",
       " 10018  [60, 4, 6, 104, 1462, 346, 5, 25, 449, 28, 161...   \n",
       " 10019  [17, 4, 469, 17, 639, 193, 2, 175, 25, 143, 13...   \n",
       " 10020  [60, 4, 2124, 17, 1964, 18, 295, 38, 17, 289, ...   \n",
       " 10021  [60, 4, 4231, 3, 15, 50, 193, 2, 398, 948, 74,...   \n",
       " 10022  [161, 19, 319, 2, 86, 5848, 74, 99, 619, 13, 9...   \n",
       " 10023  [1590, 1605, 137, 74, 2196, 122, 9, 19, 48, 16...   \n",
       " 10024  [44, 295, 8, 60, 75, 4, 8, 17, 4, 772, 48, 161...   \n",
       " 10025  [161, 3777, 78, 60, 1519, 4, 50, 306, 122, 154...   \n",
       " 10026  [44, 295, 8, 26, 12032, 60, 4, 910, 9, 426, 74...   \n",
       " 10027  [44, 295, 8, 60, 75, 4, 4254, 34, 68, 18, 13, ...   \n",
       " 10028  [249, 27, 60, 36, 1758, 42, 1, 182, 17, 426, 7...   \n",
       " 10029  [34, 166, 1364, 137, 12033, 612, 7, 1230, 28, ...   \n",
       " 10030  [42, 25, 893, 60, 426, 2, 14, 6, 469, 1693, 5,...   \n",
       " 10031  [60, 4, 1984, 17, 4, 979, 2, 86, 1, 177, 2497,...   \n",
       " 10032  [5, 1, 182, 60, 4, 6, 104, 551, 346, 17, 520, ...   \n",
       " 10033  [17, 4, 12037, 5, 25, 3778, 2, 25, 177, 186, 3...   \n",
       " 10034  [44, 295, 8, 26, 375, 60, 4, 8, 17, 4, 910, 95...   \n",
       " 10035  [44, 295, 8, 26, 375, 60, 4, 1982, 60, 4, 1982...   \n",
       " 10036  [60, 3, 161, 56, 193, 99, 1017, 3, 814, 2, 628...   \n",
       " ...                                                  ...   \n",
       " 11776  [60, 26, 14, 918, 38, 2506, 17, 609, 1, 328, 7...   \n",
       " 11777  [44, 295, 8, 34, 26, 265, 5, 60, 4, 8, 17, 4, ...   \n",
       " 11778  [60, 26, 1260, 265, 1, 208, 119, 8, 26, 391, 5...   \n",
       " 11779  [34, 166, 60, 4, 104, 469, 9, 426, 74, 17, 4, ...   \n",
       " 11780  [17, 4, 1925, 211, 60, 118, 50, 438, 79, 17, 3...   \n",
       " 11781  [1364, 6, 469, 346, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       " 11782  [60, 130, 50, 193, 2, 398, 948, 36, 412, 30, 3...   \n",
       " 11783  [60, 426, 74, 17, 4, 6, 469, 3, 2057, 3131, 5,...   \n",
       " 11784  [586, 17, 377, 306, 17, 3789, 161, 0, 0, 0, 0,...   \n",
       " 11785  [60, 75, 18, 295, 8, 426, 2, 423, 8, 17, 678, ...   \n",
       " 11786  [60, 661, 104, 4260, 33, 369, 17, 520, 2, 1422...   \n",
       " 11787  [17, 4, 283, 456, 122, 42, 183, 17, 837, 665, ...   \n",
       " 11788  [60, 4, 2106, 2, 25, 177, 18, 4, 2017, 30, 25,...   \n",
       " 11789  [44, 295, 8, 60, 75, 8, 2255, 53, 96, 25, 2599...   \n",
       " 11790  [60, 2689, 71, 12928, 4308, 5, 25, 449, 28, 25...   \n",
       " 11791  [44, 295, 8, 428, 60, 249, 27, 25, 449, 28, 16...   \n",
       " 11792  [60, 697, 17, 4, 2, 902, 2, 29, 87, 1, 3774, 8...   \n",
       " 11793  [44, 295, 7, 60, 4, 8, 17, 2762, 25, 412, 114,...   \n",
       " 11794  [60, 4, 6, 820, 346, 17, 279, 2, 14, 6, 1366, ...   \n",
       " 11795  [60, 2762, 25, 762, 20, 1, 2504, 7, 369, 17, 2...   \n",
       " 11796  [60, 426, 2, 14, 104, 910, 17, 83, 6, 354, 2, ...   \n",
       " 11797  [60, 4, 104, 12935, 77, 4, 6, 12936, 295, 2, 2...   \n",
       " 11798  [884, 336, 17, 1247, 2, 3455, 25, 275, 127, 16...   \n",
       " 11799  [17, 4, 1711, 3, 603, 68, 23, 2, 69, 17, 218, ...   \n",
       " 11800  [8, 17, 4, 6, 3431, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       " 11801  [6, 346, 8, 319, 2, 14, 12937, 3, 12938, 25, 1...   \n",
       " 11802  [6, 295, 8, 60, 12940, 14, 1098, 188, 26, 265,...   \n",
       " 11803  [17, 4, 42, 1326, 1727, 1326, 379, 120, 757, 2...   \n",
       " 11804  [34, 166, 8, 60, 2255, 25, 12942, 17, 4, 104, ...   \n",
       " 11805  [60, 4, 104, 910, 17, 4, 882, 28, 183, 228, 23...   \n",
       " \n",
       "                                                      X_1  \n",
       " 10007  [60, 279, 2, 86, 1, 177, 52, 218, 497, 248, 8,...  \n",
       " 10008  [60, 26, 14, 918, 30, 1, 295, 7, 3774, 18, 4, ...  \n",
       " 10009  [60, 4, 1982, 3, 469, 17, 75, 4777, 7, 769, 62...  \n",
       " 10010  [44, 295, 8, 428, 60, 4, 772, 48, 17, 19, 294,...  \n",
       " 10011  [17, 218, 497, 248, 30, 488, 1, 177, 3, 130, 5...  \n",
       " 10012  [60, 4, 6, 1489, 283, 456, 937, 48, 17, 289, 2...  \n",
       " 10013  [249, 27, 25, 449, 28, 161, 60, 463, 245, 38, ...  \n",
       " 10014  [60, 893, 132, 249, 27, 103, 295, 103, 11, 87,...  \n",
       " 10015  [44, 295, 8, 26, 375, 60, 4, 8, 17, 4, 795, 32...  \n",
       " 10016  [44, 295, 15, 14, 8, 17, 4, 1548, 3, 4, 6, 283...  \n",
       " 10017  [1, 145, 60, 19, 294, 2, 161, 33, 171, 1832, 8...  \n",
       " 10018  [60, 4, 6, 104, 1462, 346, 5, 25, 449, 28, 161...  \n",
       " 10019  [17, 4, 469, 17, 639, 193, 2, 175, 25, 143, 13...  \n",
       " 10020  [60, 4, 2124, 17, 1964, 18, 295, 38, 17, 289, ...  \n",
       " 10021  [60, 4, 4231, 3, 15, 50, 193, 2, 398, 948, 74,...  \n",
       " 10022  [161, 19, 319, 2, 86, 5848, 74, 99, 619, 13, 9...  \n",
       " 10023  [1590, 1605, 137, 74, 2196, 122, 9, 19, 48, 16...  \n",
       " 10024  [44, 295, 8, 60, 75, 4, 8, 17, 4, 772, 48, 161...  \n",
       " 10025  [161, 3777, 78, 60, 1519, 4, 50, 306, 122, 154...  \n",
       " 10026  [44, 295, 8, 26, 12032, 60, 4, 910, 9, 426, 74...  \n",
       " 10027  [44, 295, 8, 60, 75, 4, 4254, 34, 68, 18, 13, ...  \n",
       " 10028  [249, 27, 60, 36, 1758, 42, 1, 182, 17, 426, 7...  \n",
       " 10029  [34, 166, 1364, 137, 12033, 612, 7, 1230, 28, ...  \n",
       " 10030  [42, 25, 893, 60, 426, 2, 14, 6, 469, 1693, 5,...  \n",
       " 10031  [60, 4, 1984, 17, 4, 979, 2, 86, 1, 177, 2497,...  \n",
       " 10032  [5, 1, 182, 60, 4, 6, 104, 551, 346, 17, 520, ...  \n",
       " 10033  [17, 4, 12037, 5, 25, 3778, 2, 25, 177, 186, 3...  \n",
       " 10034  [44, 295, 8, 26, 375, 60, 4, 8, 17, 4, 910, 95...  \n",
       " 10035  [44, 295, 8, 26, 375, 60, 4, 1982, 60, 4, 1982...  \n",
       " 10036  [60, 3, 161, 56, 193, 99, 1017, 3, 814, 2, 628...  \n",
       " ...                                                  ...  \n",
       " 11776  [60, 26, 14, 918, 38, 2506, 17, 609, 1, 328, 7...  \n",
       " 11777  [44, 295, 8, 34, 26, 265, 5, 60, 4, 8, 17, 4, ...  \n",
       " 11778  [60, 26, 1260, 265, 1, 208, 119, 8, 26, 391, 5...  \n",
       " 11779  [34, 166, 60, 4, 104, 469, 9, 426, 74, 17, 4, ...  \n",
       " 11780  [17, 4, 1925, 211, 60, 118, 50, 438, 79, 17, 3...  \n",
       " 11781  [1364, 6, 469, 346, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       " 11782  [60, 130, 50, 193, 2, 398, 948, 36, 412, 30, 3...  \n",
       " 11783  [60, 426, 74, 17, 4, 6, 469, 3, 2057, 3131, 5,...  \n",
       " 11784  [586, 17, 377, 306, 17, 3789, 161, 0, 0, 0, 0,...  \n",
       " 11785  [60, 75, 18, 295, 8, 426, 2, 423, 8, 17, 678, ...  \n",
       " 11786  [60, 661, 104, 4260, 33, 369, 17, 520, 2, 1422...  \n",
       " 11787  [17, 4, 283, 456, 122, 42, 183, 17, 837, 665, ...  \n",
       " 11788  [60, 4, 2106, 2, 25, 177, 18, 4, 2017, 30, 25,...  \n",
       " 11789  [44, 295, 8, 60, 75, 8, 2255, 53, 96, 25, 2599...  \n",
       " 11790  [60, 2689, 71, 12928, 4308, 5, 25, 449, 28, 25...  \n",
       " 11791  [44, 295, 8, 428, 60, 249, 27, 25, 449, 28, 16...  \n",
       " 11792  [60, 697, 17, 4, 2, 902, 2, 29, 87, 1, 3774, 8...  \n",
       " 11793  [44, 295, 7, 60, 4, 8, 17, 2762, 25, 412, 114,...  \n",
       " 11794  [60, 4, 6, 820, 346, 17, 279, 2, 14, 6, 1366, ...  \n",
       " 11795  [60, 2762, 25, 762, 20, 1, 2504, 7, 369, 17, 2...  \n",
       " 11796  [60, 426, 2, 14, 104, 910, 17, 83, 6, 354, 2, ...  \n",
       " 11797  [60, 4, 104, 12935, 77, 4, 6, 12936, 295, 2, 2...  \n",
       " 11798  [884, 336, 17, 1247, 2, 3455, 25, 275, 127, 16...  \n",
       " 11799  [17, 4, 1711, 3, 603, 68, 23, 2, 69, 17, 218, ...  \n",
       " 11800  [8, 17, 4, 6, 3431, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       " 11801  [6, 346, 8, 319, 2, 14, 12937, 3, 12938, 25, 1...  \n",
       " 11802  [6, 295, 8, 60, 12940, 14, 1098, 188, 26, 265,...  \n",
       " 11803  [17, 4, 42, 1326, 1727, 1326, 379, 120, 757, 2...  \n",
       " 11804  [34, 166, 8, 60, 2255, 25, 12942, 17, 4, 104, ...  \n",
       " 11805  [60, 4, 104, 910, 17, 4, 882, 28, 183, 228, 23...  \n",
       " \n",
       " [1799 rows x 8 columns],           Id  EssaySet  Score1  Score2  \\\n",
       " 11806  19953         8       2       2   \n",
       " 11807  19954         8       2       1   \n",
       " 11808  19955         8       1       1   \n",
       " 11809  19956         8       0       0   \n",
       " 11810  19957         8       1       1   \n",
       " 11811  19958         8       2       2   \n",
       " 11812  19959         8       0       0   \n",
       " 11813  19960         8       2       2   \n",
       " 11814  19961         8       1       1   \n",
       " 11815  19962         8       0       0   \n",
       " 11816  19963         8       2       2   \n",
       " 11817  19964         8       2       2   \n",
       " 11818  19965         8       2       2   \n",
       " 11819  19966         8       2       2   \n",
       " 11820  19967         8       2       2   \n",
       " 11821  19968         8       0       0   \n",
       " 11822  19969         8       0       0   \n",
       " 11823  19970         8       1       1   \n",
       " 11824  19971         8       2       2   \n",
       " 11825  19972         8       0       0   \n",
       " 11826  19973         8       2       2   \n",
       " 11827  19974         8       1       1   \n",
       " 11828  19975         8       0       0   \n",
       " 11829  19976         8       0       0   \n",
       " 11830  19977         8       2       2   \n",
       " 11831  19978         8       2       2   \n",
       " 11832  19979         8       1       1   \n",
       " 11833  19980         8       0       2   \n",
       " 11834  19981         8       1       0   \n",
       " 11835  19982         8       1       1   \n",
       " ...      ...       ...     ...     ...   \n",
       " 13575  21722         8       0       0   \n",
       " 13576  21723         8       0       2   \n",
       " 13577  21724         8       2       2   \n",
       " 13578  21725         8       2       2   \n",
       " 13579  21726         8       1       1   \n",
       " 13580  21727         8       1       1   \n",
       " 13581  21728         8       0       0   \n",
       " 13582  21729         8       0       0   \n",
       " 13583  21730         8       1       1   \n",
       " 13584  21731         8       2       2   \n",
       " 13585  21732         8       0       0   \n",
       " 13586  21733         8       1       2   \n",
       " 13587  21734         8       2       2   \n",
       " 13588  21735         8       0       0   \n",
       " 13589  21736         8       1       1   \n",
       " 13590  21737         8       0       0   \n",
       " 13591  21738         8       0       0   \n",
       " 13592  21739         8       0       0   \n",
       " 13593  21740         8       0       0   \n",
       " 13594  21741         8       2       2   \n",
       " 13595  21742         8       0       0   \n",
       " 13596  21743         8       2       2   \n",
       " 13597  21744         8       1       1   \n",
       " 13598  21745         8       1       1   \n",
       " 13599  21746         8       2       1   \n",
       " 13600  21747         8       0       0   \n",
       " 13601  21748         8       2       2   \n",
       " 13602  21749         8       1       2   \n",
       " 13603  21750         8       1       1   \n",
       " 13604  21751         8       2       2   \n",
       " \n",
       "                                                EssayText  \\\n",
       " 11806  Paul finds out that Mr. Leonard was a track st...   \n",
       " 11807  The background information about Mr. Leonard r...   \n",
       " 11808  It motivated him, He knew what Mr. leonard mea...   \n",
       " 11809  The information that Paul gets about Mr.Leonar...   \n",
       " 11810  The information has an effect on Paul because ...   \n",
       " 11811  As Paul learns more about Mr. Leonard, he is a...   \n",
       " 11812  The effect on Paul about Mr. Leonard was that ...   \n",
       " 11813  It shows that even if you cant read, just like...   \n",
       " 11814   He relates to Mr. Leonard. So feels bad for him.   \n",
       " 11815  He starts to realize that Mr. Leonard is tryin...   \n",
       " 11816  When Paul recieved the background information ...   \n",
       " 11817  Mr. Leonard isnt a good reader and so he tells...   \n",
       " 11818  In knowing Mr. Leonard's background informatio...   \n",
       " 11819  The background information that the reader rec...   \n",
       " 11820  The backrground information that Paul finds ou...   \n",
       " 11821  He is excited because he will learn how to and...   \n",
       " 11822  Paul thinks that if he got to know Mr. Leonard...   \n",
       " 11823  The effect the backround information has on Pa...   \n",
       " 11824  The background information that Paul gets abou...   \n",
       " 11825  He lost his scholarship and flunked out.No ath...   \n",
       " 11826  The backgroung information made a similar emot...   \n",
       " 11827  The effection the data has on Paul is a lot. P...   \n",
       " 11828  When Paul gets some background information on ...   \n",
       " 11829  Mr. Leonard has poor reading skills and therfo...   \n",
       " 11830  The information Paul finds about Mr. Leonard o...   \n",
       " 11831  It shows Paul that Mr. Leonard has gone throug...   \n",
       " 11832  At first Paul felt a little big left out becau...   \n",
       " 11833  The backgound information that PAul had gotten...   \n",
       " 11834  The effect of the background information had a...   \n",
       " 11835  Paul is surprised by what he finds out about M...   \n",
       " ...                                                  ...   \n",
       " 13575  Paul learns that Mr. Leonard is not vary smart...   \n",
       " 13576  The effect that the background information has...   \n",
       " 13577  The background information we learn about Mr. ...   \n",
       " 13578  When Mr. Leonard told Paul about his backgroun...   \n",
       " 13579  When Paul finds out about Mr. Leonard's past i...   \n",
       " 13580  Paul ggets excited to hear about his past, but...   \n",
       " 13581  During the story, Paul gets background informa...   \n",
       " 13582  Paul doesn't like on how they are getting a ba...   \n",
       " 13583  The background information not only inspires P...   \n",
       " 13584  The background information has a positive effe...   \n",
       " 13585  I think it means, just because you cant do one...   \n",
       " 13586  When Mr. Leonard was in high school, he was ju...   \n",
       " 13587  He understands the Mr. Leonard and paul weren'...   \n",
       " 13588  Mr. Leonard feels that Paul will not be able t...   \n",
       " 13589  When Paul get the background information fro m...   \n",
       " 13590  Mr. Leonard didn't do well in college.Evidence...   \n",
       " 13591       The story talks about his background because   \n",
       " 13592  It tells Paul that he can do something that is...   \n",
       " 13593  In this story, Mr. Leonard helps Paul to overc...   \n",
       " 13594  When Paul learns Mr. Leonard's background info...   \n",
       " 13595  The information on Mr. Leonard is about his re...   \n",
       " 13596  That background information about Mr. Leonard ...   \n",
       " 13597  The effect that the backgroung information had...   \n",
       " 13598  The backround information about Mr. Leonard is...   \n",
       " 13599  He wants to help Mr. Leonard because Mr. Leona...   \n",
       " 13600  The short story is a pome.You can tell by the ...   \n",
       " 13601  In Gifts the effect the background has on Paul...   \n",
       " 13602  He realizes that Mr. Leonard is a lot by him w...   \n",
       " 13603  The backround information makes Mr. leonard an...   \n",
       " 13604  Towards the end of the story, Paul learns more...   \n",
       " \n",
       "                                                    clean  \\\n",
       " 11806  paul finds out that mr leonard was a track sta...   \n",
       " 11807  the background information about mr leonard re...   \n",
       " 11808  it motivated him , he knew what mr leonard mea...   \n",
       " 11809  the information that paul gets about mr leonar...   \n",
       " 11810  the information has an effect on paul because ...   \n",
       " 11811  as paul learns more about mr leonard , he is a...   \n",
       " 11812  the effect on paul about mr leonard was that i...   \n",
       " 11813  it shows that even if you cant read , just lik...   \n",
       " 11814      he relates to mr leonard so feels bad for him   \n",
       " 11815  he starts to realize that mr leonard is trying...   \n",
       " 11816  when paul recieved the background information ...   \n",
       " 11817  mr leonard isnt a good reader and so he tells ...   \n",
       " 11818  in knowing mr leonard 's background informatio...   \n",
       " 11819  the background information that the reader rec...   \n",
       " 11820  the backrground information that paul finds ou...   \n",
       " 11821  he is excited because he will learn how to and...   \n",
       " 11822  paul thinks that if he got to know mr leonard ...   \n",
       " 11823  the effect the backround information has on pa...   \n",
       " 11824  the background information that paul gets abou...   \n",
       " 11825  he lost his scholarship and flunked out no ath...   \n",
       " 11826  the backgroung information made a similar emot...   \n",
       " 11827  the effection the data has on paul is a lot pa...   \n",
       " 11828  when paul gets some background information on ...   \n",
       " 11829  mr leonard has poor reading skills and therfor...   \n",
       " 11830  the information paul finds about mr leonard on...   \n",
       " 11831  it shows paul that mr leonard has gone through...   \n",
       " 11832  at first paul felt a little big left out becau...   \n",
       " 11833  the backgound information that paul had gotten...   \n",
       " 11834  the effect of the background information had a...   \n",
       " 11835  paul is surprised by what he finds out about m...   \n",
       " ...                                                  ...   \n",
       " 13575  paul learns that mr leonard is not vary smart ...   \n",
       " 13576  the effect that the background information has...   \n",
       " 13577  the background information we learn about mr l...   \n",
       " 13578  when mr leonard told paul about his background...   \n",
       " 13579  when paul finds out about mr leonard 's past i...   \n",
       " 13580  paul ggets excited to hear about his past , bu...   \n",
       " 13581  during the story , paul gets background inform...   \n",
       " 13582  paul does n't like on how they are getting a b...   \n",
       " 13583  the background information not only inspires p...   \n",
       " 13584  the background information has a positive effe...   \n",
       " 13585  i think it means , just because you cant do on...   \n",
       " 13586  when mr leonard was in high school , he was ju...   \n",
       " 13587  he understands the mr leonard and paul were n'...   \n",
       " 13588  mr leonard feels that paul will not be able to...   \n",
       " 13589  when paul get the background information fro m...   \n",
       " 13590  mr leonard did n't do well in college evidence...   \n",
       " 13591       the story talks about his background because   \n",
       " 13592  it tells paul that he can do something that is...   \n",
       " 13593  in this story , mr leonard helps paul to overc...   \n",
       " 13594  when paul learns mr leonard 's background info...   \n",
       " 13595  the information on mr leonard is about his rec...   \n",
       " 13596  that background information about mr leonard s...   \n",
       " 13597  the effect that the backgroung information had...   \n",
       " 13598  the backround information about mr leonard is ...   \n",
       " 13599  he wants to help mr leonard because mr leonard...   \n",
       " 13600  the short story is a pome you can tell by the ...   \n",
       " 13601  in gifts the effect the background has on paul...   \n",
       " 13602  he realizes that mr leonard is a lot by him wi...   \n",
       " 13603  the backround information makes mr leonard and...   \n",
       " 13604  towards the end of the story , paul learns mor...   \n",
       " \n",
       "                                                        X  \\\n",
       " 11806  [21, 335, 53, 8, 22, 24, 19, 6, 185, 455, 52, ...   \n",
       " 11807  [1, 198, 67, 33, 22, 24, 1204, 2, 21, 13, 10, ...   \n",
       " 11808  [9, 983, 65, 12, 446, 23, 22, 24, 1408, 3, 8, ...   \n",
       " 11809  [1, 67, 8, 21, 266, 33, 22, 24, 19, 8, 12, 19,...   \n",
       " 11810  [1, 67, 75, 71, 282, 27, 21, 13, 21, 3, 22, 24...   \n",
       " 11811  [38, 21, 531, 55, 33, 22, 24, 12, 4, 290, 2, 2...   \n",
       " 11812  [1, 282, 27, 21, 33, 22, 24, 19, 8, 9, 69, 50,...   \n",
       " 11813  [9, 219, 8, 186, 79, 32, 799, 100, 137, 74, 22...   \n",
       " 11814  [12, 1204, 2, 22, 24, 59, 218, 357, 20, 65, 0,...   \n",
       " 11815  [12, 278, 2, 747, 8, 22, 24, 4, 319, 2, 748, 6...   \n",
       " 11816  [48, 21, 2076, 1, 198, 67, 27, 22, 24, 12, 113...   \n",
       " 11817  [22, 24, 1683, 6, 134, 189, 3, 59, 12, 203, 21...   \n",
       " 11818  [5, 731, 22, 24, 36, 198, 67, 21, 218, 74, 1, ...   \n",
       " 11819  [1, 198, 67, 8, 1, 189, 1435, 33, 22, 24, 4, 8...   \n",
       " 11820  [1, 6574, 67, 8, 21, 335, 53, 33, 22, 24, 75, ...   \n",
       " 11821  [12, 4, 1989, 13, 12, 66, 466, 16, 2, 3, 14, 1...   \n",
       " 11822  [21, 586, 8, 79, 12, 351, 2, 68, 22, 24, 31, 6...   \n",
       " 11823  [1, 282, 1, 1097, 67, 75, 27, 21, 4, 9, 219, 6...   \n",
       " 11824  [1, 198, 67, 8, 21, 266, 33, 22, 24, 3465, 65,...   \n",
       " 11825  [12, 620, 96, 790, 3, 476, 53, 277, 12950, 346...   \n",
       " 11826  [1, 2637, 67, 226, 6, 95, 2633, 643, 391, 344,...   \n",
       " 11827  [1, 12953, 1, 151, 75, 27, 21, 4, 6, 366, 21, ...   \n",
       " 11828  [48, 21, 266, 163, 198, 67, 27, 22, 24, 12, 33...   \n",
       " 11829  [22, 24, 75, 1109, 225, 1510, 3, 4324, 83, 2, ...   \n",
       " 11830  [1, 67, 21, 335, 33, 22, 24, 27, 1, 1191, 4, 2...   \n",
       " 11831  [9, 219, 21, 8, 22, 24, 75, 665, 133, 612, 7, ...   \n",
       " 11832  [78, 120, 21, 318, 6, 244, 214, 686, 53, 13, 5...   \n",
       " 11833  [1, 12955, 67, 8, 21, 83, 1950, 282, 65, 13, 2...   \n",
       " 11834  [1, 282, 7, 1, 198, 67, 83, 6, 389, 282, 27, 2...   \n",
       " 11835  [21, 4, 857, 30, 23, 12, 335, 53, 33, 22, 24, ...   \n",
       " ...                                                  ...   \n",
       " 13575  [21, 531, 8, 22, 24, 4, 37, 1602, 1548, 1, 938...   \n",
       " 13576  [1, 282, 8, 1, 198, 67, 75, 27, 21, 4, 9, 2619...   \n",
       " 13577  [1, 198, 67, 188, 466, 33, 22, 24, 4, 6, 1198,...   \n",
       " 13578  [48, 22, 24, 300, 21, 33, 96, 198, 8, 12, 476,...   \n",
       " 13579  [48, 21, 335, 53, 33, 22, 24, 36, 410, 9, 213,...   \n",
       " 13580  [21, 13887, 1989, 2, 1864, 33, 96, 410, 52, 31...   \n",
       " 13581  [451, 1, 182, 21, 266, 198, 67, 33, 22, 24, 48...   \n",
       " 13582  [21, 130, 50, 74, 27, 16, 10, 11, 481, 6, 198,...   \n",
       " 13583  [1, 198, 67, 37, 92, 1553, 21, 2, 69, 96, 231,...   \n",
       " 13584  [1, 198, 67, 75, 6, 1076, 282, 27, 21, 13, 9, ...   \n",
       " 13585  [34, 166, 9, 190, 137, 13, 32, 799, 69, 44, 17...   \n",
       " 13586  [48, 22, 24, 19, 5, 393, 183, 12, 19, 137, 74,...   \n",
       " 13587  [12, 654, 1, 22, 24, 3, 21, 132, 50, 59, 57, 1...   \n",
       " 13588  [22, 24, 218, 8, 21, 66, 37, 14, 290, 2, 109, ...   \n",
       " 13589  [48, 21, 109, 1, 198, 67, 2340, 13893, 5123, 1...   \n",
       " 13590  [22, 24, 118, 50, 69, 200, 5, 135, 13894, 119,...   \n",
       " 13591  [1, 182, 289, 33, 96, 198, 13, 0, 0, 0, 0, 0, ...   \n",
       " 13592  [9, 203, 21, 8, 12, 26, 69, 204, 8, 4, 3329, 9...   \n",
       " 13593  [5, 18, 182, 22, 24, 333, 21, 2, 1081, 13897, ...   \n",
       " 13594  [48, 21, 531, 22, 24, 36, 198, 67, 12, 218, 16...   \n",
       " 13595  [1, 67, 27, 22, 24, 4, 33, 96, 576, 5, 1048, 3...   \n",
       " 13596  [8, 198, 67, 33, 22, 24, 3465, 21, 21, 31, 279...   \n",
       " 13597  [1, 282, 8, 1, 2637, 67, 83, 27, 21, 19, 284, ...   \n",
       " 13598  [1, 1097, 67, 33, 22, 24, 4, 2657, 5, 18, 182,...   \n",
       " 13599  [12, 279, 2, 86, 22, 24, 13, 22, 24, 453, 65, ...   \n",
       " 13600  [1, 1448, 182, 4, 6, 13902, 32, 26, 175, 30, 1...   \n",
       " 13601  [5, 4359, 1, 282, 1, 198, 75, 27, 21, 4, 8, 48...   \n",
       " 13602  [12, 541, 8, 22, 24, 4, 6, 366, 30, 65, 28, 96...   \n",
       " 13603  [1, 1097, 67, 213, 22, 24, 3, 21, 400, 186, 12...   \n",
       " 13604  [826, 1, 360, 7, 1, 182, 21, 531, 55, 33, 22, ...   \n",
       " \n",
       "                                                      X_1  \n",
       " 11806  [21, 335, 53, 8, 22, 24, 19, 6, 185, 455, 52, ...  \n",
       " 11807  [1, 198, 67, 33, 22, 24, 1204, 2, 21, 13, 10, ...  \n",
       " 11808  [9, 983, 65, 12, 446, 23, 22, 24, 1408, 3, 8, ...  \n",
       " 11809  [1, 67, 8, 21, 266, 33, 22, 24, 19, 8, 12, 19,...  \n",
       " 11810  [1, 67, 75, 71, 282, 27, 21, 13, 21, 3, 22, 24...  \n",
       " 11811  [38, 21, 531, 55, 33, 22, 24, 12, 4, 290, 2, 2...  \n",
       " 11812  [1, 282, 27, 21, 33, 22, 24, 19, 8, 9, 69, 50,...  \n",
       " 11813  [9, 219, 8, 186, 79, 32, 799, 100, 137, 74, 22...  \n",
       " 11814  [12, 1204, 2, 22, 24, 59, 218, 357, 20, 65, 0,...  \n",
       " 11815  [12, 278, 2, 747, 8, 22, 24, 4, 319, 2, 748, 6...  \n",
       " 11816  [48, 21, 2076, 1, 198, 67, 27, 22, 24, 12, 113...  \n",
       " 11817  [22, 24, 1683, 6, 134, 189, 3, 59, 12, 203, 21...  \n",
       " 11818  [5, 731, 22, 24, 36, 198, 67, 21, 218, 74, 1, ...  \n",
       " 11819  [1, 198, 67, 8, 1, 189, 1435, 33, 22, 24, 4, 8...  \n",
       " 11820  [1, 6574, 67, 8, 21, 335, 53, 33, 22, 24, 75, ...  \n",
       " 11821  [12, 4, 1989, 13, 12, 66, 466, 16, 2, 3, 14, 1...  \n",
       " 11822  [21, 586, 8, 79, 12, 351, 2, 68, 22, 24, 31, 6...  \n",
       " 11823  [1, 282, 1, 1097, 67, 75, 27, 21, 4, 9, 219, 6...  \n",
       " 11824  [1, 198, 67, 8, 21, 266, 33, 22, 24, 3465, 65,...  \n",
       " 11825  [12, 620, 96, 790, 3, 476, 53, 277, 12950, 346...  \n",
       " 11826  [1, 2637, 67, 226, 6, 95, 2633, 643, 391, 344,...  \n",
       " 11827  [1, 12953, 1, 151, 75, 27, 21, 4, 6, 366, 21, ...  \n",
       " 11828  [48, 21, 266, 163, 198, 67, 27, 22, 24, 12, 33...  \n",
       " 11829  [22, 24, 75, 1109, 225, 1510, 3, 4324, 83, 2, ...  \n",
       " 11830  [1, 67, 21, 335, 33, 22, 24, 27, 1, 1191, 4, 2...  \n",
       " 11831  [9, 219, 21, 8, 22, 24, 75, 665, 133, 612, 7, ...  \n",
       " 11832  [78, 120, 21, 318, 6, 244, 214, 686, 53, 13, 5...  \n",
       " 11833  [1, 12955, 67, 8, 21, 83, 1950, 282, 65, 13, 2...  \n",
       " 11834  [1, 282, 7, 1, 198, 67, 83, 6, 389, 282, 27, 2...  \n",
       " 11835  [21, 4, 857, 30, 23, 12, 335, 53, 33, 22, 24, ...  \n",
       " ...                                                  ...  \n",
       " 13575  [21, 531, 8, 22, 24, 4, 37, 1602, 1548, 1, 938...  \n",
       " 13576  [1, 282, 8, 1, 198, 67, 75, 27, 21, 4, 9, 2619...  \n",
       " 13577  [1, 198, 67, 188, 466, 33, 22, 24, 4, 6, 1198,...  \n",
       " 13578  [48, 22, 24, 300, 21, 33, 96, 198, 8, 12, 476,...  \n",
       " 13579  [48, 21, 335, 53, 33, 22, 24, 36, 410, 9, 213,...  \n",
       " 13580  [21, 13887, 1989, 2, 1864, 33, 96, 410, 52, 31...  \n",
       " 13581  [451, 1, 182, 21, 266, 198, 67, 33, 22, 24, 48...  \n",
       " 13582  [21, 130, 50, 74, 27, 16, 10, 11, 481, 6, 198,...  \n",
       " 13583  [1, 198, 67, 37, 92, 1553, 21, 2, 69, 96, 231,...  \n",
       " 13584  [1, 198, 67, 75, 6, 1076, 282, 27, 21, 13, 9, ...  \n",
       " 13585  [34, 166, 9, 190, 137, 13, 32, 799, 69, 44, 17...  \n",
       " 13586  [48, 22, 24, 19, 5, 393, 183, 12, 19, 137, 74,...  \n",
       " 13587  [12, 654, 1, 22, 24, 3, 21, 132, 50, 59, 57, 1...  \n",
       " 13588  [22, 24, 218, 8, 21, 66, 37, 14, 290, 2, 109, ...  \n",
       " 13589  [48, 21, 109, 1, 198, 67, 2340, 13893, 5123, 1...  \n",
       " 13590  [22, 24, 118, 50, 69, 200, 5, 135, 13894, 119,...  \n",
       " 13591  [1, 182, 289, 33, 96, 198, 13, 0, 0, 0, 0, 0, ...  \n",
       " 13592  [9, 203, 21, 8, 12, 26, 69, 204, 8, 4, 3329, 9...  \n",
       " 13593  [5, 18, 182, 22, 24, 333, 21, 2, 1081, 13897, ...  \n",
       " 13594  [48, 21, 531, 22, 24, 36, 198, 67, 12, 218, 16...  \n",
       " 13595  [1, 67, 27, 22, 24, 4, 33, 96, 576, 5, 1048, 3...  \n",
       " 13596  [8, 198, 67, 33, 22, 24, 3465, 21, 21, 31, 279...  \n",
       " 13597  [1, 282, 8, 1, 2637, 67, 83, 27, 21, 19, 284, ...  \n",
       " 13598  [1, 1097, 67, 33, 22, 24, 4, 2657, 5, 18, 182,...  \n",
       " 13599  [12, 279, 2, 86, 22, 24, 13, 22, 24, 453, 65, ...  \n",
       " 13600  [1, 1448, 182, 4, 6, 13902, 32, 26, 175, 30, 1...  \n",
       " 13601  [5, 4359, 1, 282, 1, 198, 75, 27, 21, 4, 8, 48...  \n",
       " 13602  [12, 541, 8, 22, 24, 4, 6, 366, 30, 65, 28, 96...  \n",
       " 13603  [1, 1097, 67, 213, 22, 24, 3, 21, 400, 186, 12...  \n",
       " 13604  [826, 1, 360, 7, 1, 182, 21, 531, 55, 33, 22, ...  \n",
       " \n",
       " [1799 rows x 8 columns],           Id  EssaySet  Score1  Score2  \\\n",
       " 13605  22952         9       1       0   \n",
       " 13606  22953         9       0       0   \n",
       " 13607  22954         9       0       0   \n",
       " 13608  22955         9       2       2   \n",
       " 13609  22956         9       1       1   \n",
       " 13610  22957         9       2       2   \n",
       " 13611  22958         9       1       1   \n",
       " 13612  22959         9       2       1   \n",
       " 13613  22960         9       0       0   \n",
       " 13614  22961         9       2       2   \n",
       " 13615  22962         9       2       2   \n",
       " 13616  22963         9       1       1   \n",
       " 13617  22964         9       1       1   \n",
       " 13618  22965         9       0       0   \n",
       " 13619  22966         9       1       0   \n",
       " 13620  22967         9       2       2   \n",
       " 13621  22968         9       2       2   \n",
       " 13622  22969         9       2       2   \n",
       " 13623  22970         9       2       2   \n",
       " 13624  22971         9       2       1   \n",
       " 13625  22972         9       2       2   \n",
       " 13626  22973         9       0       0   \n",
       " 13627  22974         9       2       2   \n",
       " 13628  22975         9       1       1   \n",
       " 13629  22976         9       1       1   \n",
       " 13630  22977         9       1       1   \n",
       " 13631  22978         9       1       0   \n",
       " 13632  22979         9       1       1   \n",
       " 13633  22980         9       2       2   \n",
       " 13634  22981         9       2       2   \n",
       " ...      ...       ...     ...     ...   \n",
       " 15373  24720         9       2       2   \n",
       " 15374  24721         9       1       1   \n",
       " 15375  24722         9       2       1   \n",
       " 15376  24723         9       1       1   \n",
       " 15377  24724         9       2       2   \n",
       " 15378  24725         9       1       0   \n",
       " 15379  24726         9       2       2   \n",
       " 15380  24727         9       1       1   \n",
       " 15381  24728         9       2       2   \n",
       " 15382  24729         9       2       2   \n",
       " 15383  24730         9       1       0   \n",
       " 15384  24731         9       1       1   \n",
       " 15385  24732         9       1       1   \n",
       " 15386  24733         9       0       0   \n",
       " 15387  24734         9       2       2   \n",
       " 15388  24735         9       1       0   \n",
       " 15389  24736         9       1       1   \n",
       " 15390  24737         9       2       2   \n",
       " 15391  24738         9       2       2   \n",
       " 15392  24739         9       1       1   \n",
       " 15393  24740         9       0       0   \n",
       " 15394  24741         9       0       0   \n",
       " 15395  24742         9       2       2   \n",
       " 15396  24743         9       2       1   \n",
       " 15397  24744         9       0       0   \n",
       " 15398  24745         9       1       1   \n",
       " 15399  24746         9       0       0   \n",
       " 15400  24747         9       1       1   \n",
       " 15401  24748         9       0       0   \n",
       " 15402  24749         9       2       2   \n",
       " \n",
       "                                                EssayText  \\\n",
       " 13605  The author orginizes this article by putting i...   \n",
       " 13606  By starting back in the 1950's also by their t...   \n",
       " 13607  He says what it is, and the first year they st...   \n",
       " 13608  The author organizes the article by catching y...   \n",
       " 13609  She organizes the article by useing different ...   \n",
       " 13610  The author organizes the article by first lett...   \n",
       " 13611  It was very well put together, becuase the aut...   \n",
       " 13612  The Auther organizes the article by first info...   \n",
       " 13613  The author does step by step in order to infor...   \n",
       " 13614  The author organizes the article by breaking i...   \n",
       " 13615  The author organizes this article by giving us...   \n",
       " 13616  The author organizes the article by opening wi...   \n",
       " 13617  The author organizes the article by telling ab...   \n",
       " 13618        By telling about the history of space junk.   \n",
       " 13619  It talks about what space junk is. Then it tal...   \n",
       " 13620  The author gives us a clear introduction at th...   \n",
       " 13621  The author organizes his/her writing into diff...   \n",
       " 13622  The author breaks the article up into smaller ...   \n",
       " 13623  First he talks and informs us about the genera...   \n",
       " 13624  This organized in a way that tells us about or...   \n",
       " 13625  First, the author gives a quick explanation of...   \n",
       " 13626  The author organize the article to help reader...   \n",
       " 13627  The author organizes the artical well. He puts...   \n",
       " 13628  The author organizes the article in parts. He ...   \n",
       " 13629  he starts with a joke, then moves to the histo...   \n",
       " 13630  The author puts the titles on them which help ...   \n",
       " 13631  He organzie how it begins to get worst through...   \n",
       " 13632  First he talks about what space junk is and ho...   \n",
       " 13633  The author organizes the articles by producing...   \n",
       " 13634  The author starts off trying to lure the reade...   \n",
       " ...                                                  ...   \n",
       " 15373  First, the author talks about Space junk and t...   \n",
       " 15374  The author puts this article in a weird way, H...   \n",
       " 15375  At first the author uses a suprising statement...   \n",
       " 15376  The author organizes the article but making si...   \n",
       " 15377  The organization of the article is information...   \n",
       " 15378  the author organizes the artical but putting s...   \n",
       " 15379  The author seperates the article into differen...   \n",
       " 15380  The author organized this to where he could dr...   \n",
       " 15381  The author seems to organize the article with ...   \n",
       " 15382  The introduction of this article grabs your at...   \n",
       " 15383  The author begins the article with fun interes...   \n",
       " 15384  The author organizes the article in a way were...   \n",
       " 15385  The author organizes the article by putting th...   \n",
       " 15386  FROM FUN NEWS TO REALLY BAD INFORMATION ABOUT ...   \n",
       " 15387  The author starts off the passage with a comic...   \n",
       " 15388  He first explains what is beyond the Earth's a...   \n",
       " 15389  He organizes it by getting the readers attenti...   \n",
       " 15390  The author does an excellent job at ordering t...   \n",
       " 15391  The author of 'Orbiting Junk' organizes the ar...   \n",
       " 15392  They organize it by using ancadotes as the int...   \n",
       " 15393  to make it sound like its a big deal that spac...   \n",
       " 15394  The author organizes the article by lining up ...   \n",
       " 15395  The author organizes the article in multiple s...   \n",
       " 15396  The author seperates the article into differen...   \n",
       " 15397  By giving insight on the problem, then giving ...   \n",
       " 15398  He talks about the history of Space and how we...   \n",
       " 15399  The author starts out gentle and says the fact...   \n",
       " 15400  He starts of with a attention getter and goes ...   \n",
       " 15401                             He puts it impportanc.   \n",
       " 15402  The author organizes the article into several ...   \n",
       " \n",
       "                                                    clean  \\\n",
       " 13605  the author orginizes this article by putting i...   \n",
       " 13606  by starting back in the  's also by their thou...   \n",
       " 13607  he says what it is , and the first year they s...   \n",
       " 13608  the author organizes the article by catching y...   \n",
       " 13609  she organizes the article by useing different ...   \n",
       " 13610  the author organizes the article by first lett...   \n",
       " 13611  it was very well put together , becuase the au...   \n",
       " 13612  the auther organizes the article by first info...   \n",
       " 13613  the author does step by step in order to infor...   \n",
       " 13614  the author organizes the article by breaking i...   \n",
       " 13615  the author organizes this article by giving us...   \n",
       " 13616  the author organizes the article by opening wi...   \n",
       " 13617  the author organizes the article by telling ab...   \n",
       " 13618         by telling about the history of space junk   \n",
       " 13619  it talks about what space junk is then it talk...   \n",
       " 13620  the author gives us a clear introduction at th...   \n",
       " 13621  the author organizes his her writing into diff...   \n",
       " 13622  the author breaks the article up into smaller ...   \n",
       " 13623  first he talks and informs us about the genera...   \n",
       " 13624  this organized in a way that tells us about or...   \n",
       " 13625  first , the author gives a quick explanation o...   \n",
       " 13626  the author organize the article to help reader...   \n",
       " 13627  the author organizes the artical well he puts ...   \n",
       " 13628  the author organizes the article in parts he s...   \n",
       " 13629  he starts with a joke , then moves to the hist...   \n",
       " 13630  the author puts the titles on them which help ...   \n",
       " 13631  he organzie how it begins to get worst through...   \n",
       " 13632  first he talks about what space junk is and ho...   \n",
       " 13633  the author organizes the articles by producing...   \n",
       " 13634  the author starts off trying to lure the reade...   \n",
       " ...                                                  ...   \n",
       " 15373  first , the author talks about space junk and ...   \n",
       " 15374  the author puts this article in a weird way , ...   \n",
       " 15375  at first the author uses a suprising statement...   \n",
       " 15376  the author organizes the article but making si...   \n",
       " 15377  the organization of the article is information...   \n",
       " 15378  the author organizes the artical but putting s...   \n",
       " 15379  the author seperates the article into differen...   \n",
       " 15380  the author organized this to where he could dr...   \n",
       " 15381  the author seems to organize the article with ...   \n",
       " 15382  the introduction of this article grabs your at...   \n",
       " 15383  the author begins the article with fun interes...   \n",
       " 15384  the author organizes the article in a way were...   \n",
       " 15385  the author organizes the article by putting th...   \n",
       " 15386  from fun news to really bad information about ...   \n",
       " 15387  the author starts off the passage with a comic...   \n",
       " 15388  he first explains what is beyond the earth 's ...   \n",
       " 15389  he organizes it by getting the readers attenti...   \n",
       " 15390  the author does an excellent job at ordering t...   \n",
       " 15391  the author of 'orbiting junk' organizes the ar...   \n",
       " 15392  they organize it by using ancadotes as the int...   \n",
       " 15393  to make it sound like its a big deal that spac...   \n",
       " 15394  the author organizes the article by lining up ...   \n",
       " 15395  the author organizes the article in multiple s...   \n",
       " 15396  the author seperates the article into differen...   \n",
       " 15397  by giving insight on the problem , then giving...   \n",
       " 15398  he talks about the history of space and how we...   \n",
       " 15399  the author starts out gentle and says the fact...   \n",
       " 15400  he starts of with a attention getter and goes ...   \n",
       " 15401                              he puts it impportanc   \n",
       " 15402  the author organizes the article into several ...   \n",
       " \n",
       "                                                        X  \\\n",
       " 13605  [1, 58, 3483, 18, 35, 30, 522, 72, 6, 1472, 55...   \n",
       " 13606  [30, 363, 344, 5, 1, 36, 41, 30, 99, 1866, 71,...   \n",
       " 13607  [12, 162, 23, 9, 4, 3, 1, 120, 1374, 10, 584, ...   \n",
       " 13608  [1, 58, 173, 1, 35, 30, 1730, 288, 334, 28, 11...   \n",
       " 13609  [17, 173, 1, 35, 30, 2442, 57, 984, 48, 17, 29...   \n",
       " 13610  [1, 58, 173, 1, 35, 30, 120, 1007, 1, 500, 68,...   \n",
       " 13611  [9, 19, 104, 200, 176, 486, 1078, 1, 58, 507, ...   \n",
       " 13612  [1, 1465, 173, 1, 35, 30, 120, 1671, 32, 33, 2...   \n",
       " 13613  [1, 58, 130, 272, 30, 272, 5, 168, 2, 1390, 23...   \n",
       " 13614  [1, 58, 173, 1, 35, 30, 1071, 9, 127, 72, 323,...   \n",
       " 13615  [1, 58, 173, 18, 35, 30, 551, 234, 163, 67, 33...   \n",
       " 13616  [1, 58, 173, 1, 35, 30, 1398, 28, 71, 315, 493...   \n",
       " 13617  [1, 58, 173, 1, 35, 30, 368, 33, 3855, 3, 16, ...   \n",
       " 13618  [30, 368, 33, 1, 468, 7, 43, 88, 0, 0, 0, 0, 0...   \n",
       " 13619  [9, 289, 33, 23, 43, 88, 4, 31, 9, 289, 33, 54...   \n",
       " 13620  [1, 58, 286, 234, 6, 778, 315, 78, 1, 13908, 1...   \n",
       " 13621  [1, 58, 173, 96, 25, 898, 72, 57, 323, 249, 27...   \n",
       " 13622  [1, 58, 1030, 1, 35, 127, 72, 974, 323, 30, 27...   \n",
       " 13623  [120, 12, 289, 3, 1614, 234, 33, 1, 1312, 616,...   \n",
       " 13624  [18, 507, 5, 6, 108, 8, 203, 234, 33, 763, 139...   \n",
       " 13625  [120, 1, 58, 286, 6, 1144, 1309, 7, 16, 43, 88...   \n",
       " 13626  [1, 58, 733, 1, 35, 2, 86, 189, 2, 192, 225, 1...   \n",
       " 13627  [1, 58, 173, 1, 935, 200, 12, 609, 119, 248, 3...   \n",
       " 13628  [1, 58, 173, 1, 35, 5, 582, 12, 278, 28, 71, 3...   \n",
       " 13629  [12, 278, 28, 6, 2398, 31, 443, 2, 1, 468, 7, ...   \n",
       " 13630  [1, 58, 609, 1, 782, 27, 116, 77, 86, 13, 10, ...   \n",
       " 13631  [12, 13915, 16, 9, 583, 2, 109, 3062, 509, 1, ...   \n",
       " 13632  [120, 12, 289, 33, 23, 43, 88, 4, 3, 16, 9, 4,...   \n",
       " 13633  [1, 58, 173, 1, 934, 30, 2483, 1067, 57, 782, ...   \n",
       " 13634  [1, 58, 278, 259, 319, 2, 5150, 1, 189, 1296, ...   \n",
       " ...                                                  ...   \n",
       " 15373  [120, 1, 58, 289, 33, 43, 88, 3, 203, 15099, 1...   \n",
       " 15374  [1, 58, 609, 18, 35, 5, 6, 2861, 108, 12, 62, ...   \n",
       " 15375  [78, 120, 1, 58, 330, 6, 3848, 829, 2, 1040, 1...   \n",
       " 15376  [1, 58, 173, 1, 35, 52, 298, 659, 984, 7, 1, 5...   \n",
       " 15377  [1, 1459, 7, 1, 35, 4, 67, 5, 73, 252, 201, 15...   \n",
       " 15378  [1, 58, 173, 1, 935, 52, 522, 15110, 5, 120, 3...   \n",
       " 15379  [1, 58, 1517, 1, 35, 72, 57, 323, 8, 29, 67, 8...   \n",
       " 15380  [1, 58, 507, 18, 2, 155, 12, 45, 975, 334, 2, ...   \n",
       " 15381  [1, 58, 426, 2, 733, 1, 35, 28, 6, 1065, 2475,...   \n",
       " 15382  [1, 315, 7, 18, 35, 1061, 288, 334, 3, 213, 32...   \n",
       " 15383  [1, 58, 583, 1, 35, 28, 2135, 851, 627, 8, 104...   \n",
       " 15384  [1, 58, 173, 1, 35, 5, 6, 108, 132, 1, 189, 12...   \n",
       " 15385  [1, 58, 173, 1, 35, 30, 522, 1, 102, 2134, 486...   \n",
       " 15386  [42, 2135, 2887, 2, 247, 357, 67, 33, 1861, 29...   \n",
       " 15387  [1, 58, 278, 259, 1, 673, 28, 6, 3868, 986, 16...   \n",
       " 15388  [12, 120, 238, 23, 4, 2185, 1, 437, 36, 5214, ...   \n",
       " 15389  [12, 173, 9, 30, 481, 1, 500, 334, 30, 363, 9,...   \n",
       " 15390  [1, 58, 130, 71, 2769, 354, 78, 15120, 1, 67, ...   \n",
       " 15391  [1, 58, 7, 1254, 414, 173, 1, 35, 72, 196, 323...   \n",
       " 15392  [10, 733, 9, 30, 291, 15123, 38, 1, 4486, 31, ...   \n",
       " 15393  [2, 97, 9, 1605, 74, 174, 6, 214, 427, 8, 43, ...   \n",
       " 15394  [1, 58, 173, 1, 35, 30, 4237, 127, 1, 1275, 20...   \n",
       " 15395  [1, 58, 173, 1, 35, 5, 843, 323, 10, 29, 6, 25...   \n",
       " 15396  [1, 58, 1517, 1, 35, 72, 57, 582, 103, 11, 196...   \n",
       " 15397  [30, 551, 4329, 27, 1, 233, 31, 551, 1737, 0, ...   \n",
       " 15398  [12, 289, 33, 1, 468, 7, 43, 3, 16, 188, 29, 2...   \n",
       " 15399  [1, 58, 278, 53, 1749, 3, 162, 1, 492, 838, 53...   \n",
       " 15400  [12, 278, 7, 28, 6, 334, 1820, 3, 154, 72, 1, ...   \n",
       " 15401  [12, 609, 9, 15128, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       " 15402  [1, 58, 173, 1, 35, 72, 1067, 323, 1, 2211, 43...   \n",
       " \n",
       "                                                      X_1  \n",
       " 13605  [1, 58, 3483, 18, 35, 30, 522, 72, 6, 1472, 55...  \n",
       " 13606  [30, 363, 344, 5, 1, 36, 41, 30, 99, 1866, 71,...  \n",
       " 13607  [12, 162, 23, 9, 4, 3, 1, 120, 1374, 10, 584, ...  \n",
       " 13608  [1, 58, 173, 1, 35, 30, 1730, 288, 334, 28, 11...  \n",
       " 13609  [17, 173, 1, 35, 30, 2442, 57, 984, 48, 17, 29...  \n",
       " 13610  [1, 58, 173, 1, 35, 30, 120, 1007, 1, 500, 68,...  \n",
       " 13611  [9, 19, 104, 200, 176, 486, 1078, 1, 58, 507, ...  \n",
       " 13612  [1, 1465, 173, 1, 35, 30, 120, 1671, 32, 33, 2...  \n",
       " 13613  [1, 58, 130, 272, 30, 272, 5, 168, 2, 1390, 23...  \n",
       " 13614  [1, 58, 173, 1, 35, 30, 1071, 9, 127, 72, 323,...  \n",
       " 13615  [1, 58, 173, 18, 35, 30, 551, 234, 163, 67, 33...  \n",
       " 13616  [1, 58, 173, 1, 35, 30, 1398, 28, 71, 315, 493...  \n",
       " 13617  [1, 58, 173, 1, 35, 30, 368, 33, 3855, 3, 16, ...  \n",
       " 13618  [30, 368, 33, 1, 468, 7, 43, 88, 0, 0, 0, 0, 0...  \n",
       " 13619  [9, 289, 33, 23, 43, 88, 4, 31, 9, 289, 33, 54...  \n",
       " 13620  [1, 58, 286, 234, 6, 778, 315, 78, 1, 13908, 1...  \n",
       " 13621  [1, 58, 173, 96, 25, 898, 72, 57, 323, 249, 27...  \n",
       " 13622  [1, 58, 1030, 1, 35, 127, 72, 974, 323, 30, 27...  \n",
       " 13623  [120, 12, 289, 3, 1614, 234, 33, 1, 1312, 616,...  \n",
       " 13624  [18, 507, 5, 6, 108, 8, 203, 234, 33, 763, 139...  \n",
       " 13625  [120, 1, 58, 286, 6, 1144, 1309, 7, 16, 43, 88...  \n",
       " 13626  [1, 58, 733, 1, 35, 2, 86, 189, 2, 192, 225, 1...  \n",
       " 13627  [1, 58, 173, 1, 935, 200, 12, 609, 119, 248, 3...  \n",
       " 13628  [1, 58, 173, 1, 35, 5, 582, 12, 278, 28, 71, 3...  \n",
       " 13629  [12, 278, 28, 6, 2398, 31, 443, 2, 1, 468, 7, ...  \n",
       " 13630  [1, 58, 609, 1, 782, 27, 116, 77, 86, 13, 10, ...  \n",
       " 13631  [12, 13915, 16, 9, 583, 2, 109, 3062, 509, 1, ...  \n",
       " 13632  [120, 12, 289, 33, 23, 43, 88, 4, 3, 16, 9, 4,...  \n",
       " 13633  [1, 58, 173, 1, 934, 30, 2483, 1067, 57, 782, ...  \n",
       " 13634  [1, 58, 278, 259, 319, 2, 5150, 1, 189, 1296, ...  \n",
       " ...                                                  ...  \n",
       " 15373  [120, 1, 58, 289, 33, 43, 88, 3, 203, 15099, 1...  \n",
       " 15374  [1, 58, 609, 18, 35, 5, 6, 2861, 108, 12, 62, ...  \n",
       " 15375  [78, 120, 1, 58, 330, 6, 3848, 829, 2, 1040, 1...  \n",
       " 15376  [1, 58, 173, 1, 35, 52, 298, 659, 984, 7, 1, 5...  \n",
       " 15377  [1, 1459, 7, 1, 35, 4, 67, 5, 73, 252, 201, 15...  \n",
       " 15378  [1, 58, 173, 1, 935, 52, 522, 15110, 5, 120, 3...  \n",
       " 15379  [1, 58, 1517, 1, 35, 72, 57, 323, 8, 29, 67, 8...  \n",
       " 15380  [1, 58, 507, 18, 2, 155, 12, 45, 975, 334, 2, ...  \n",
       " 15381  [1, 58, 426, 2, 733, 1, 35, 28, 6, 1065, 2475,...  \n",
       " 15382  [1, 315, 7, 18, 35, 1061, 288, 334, 3, 213, 32...  \n",
       " 15383  [1, 58, 583, 1, 35, 28, 2135, 851, 627, 8, 104...  \n",
       " 15384  [1, 58, 173, 1, 35, 5, 6, 108, 132, 1, 189, 12...  \n",
       " 15385  [1, 58, 173, 1, 35, 30, 522, 1, 102, 2134, 486...  \n",
       " 15386  [42, 2135, 2887, 2, 247, 357, 67, 33, 1861, 29...  \n",
       " 15387  [1, 58, 278, 259, 1, 673, 28, 6, 3868, 986, 16...  \n",
       " 15388  [12, 120, 238, 23, 4, 2185, 1, 437, 36, 5214, ...  \n",
       " 15389  [12, 173, 9, 30, 481, 1, 500, 334, 30, 363, 9,...  \n",
       " 15390  [1, 58, 130, 71, 2769, 354, 78, 15120, 1, 67, ...  \n",
       " 15391  [1, 58, 7, 1254, 414, 173, 1, 35, 72, 196, 323...  \n",
       " 15392  [10, 733, 9, 30, 291, 15123, 38, 1, 4486, 31, ...  \n",
       " 15393  [2, 97, 9, 1605, 74, 174, 6, 214, 427, 8, 43, ...  \n",
       " 15394  [1, 58, 173, 1, 35, 30, 4237, 127, 1, 1275, 20...  \n",
       " 15395  [1, 58, 173, 1, 35, 5, 843, 323, 10, 29, 6, 25...  \n",
       " 15396  [1, 58, 1517, 1, 35, 72, 57, 582, 103, 11, 196...  \n",
       " 15397  [30, 551, 4329, 27, 1, 233, 31, 551, 1737, 0, ...  \n",
       " 15398  [12, 289, 33, 1, 468, 7, 43, 3, 16, 188, 29, 2...  \n",
       " 15399  [1, 58, 278, 53, 1749, 3, 162, 1, 492, 838, 53...  \n",
       " 15400  [12, 278, 7, 28, 6, 334, 1820, 3, 154, 72, 1, ...  \n",
       " 15401  [12, 609, 9, 15128, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       " 15402  [1, 58, 173, 1, 35, 72, 1067, 323, 1, 2211, 43...  \n",
       " \n",
       " [1798 rows x 8 columns],           Id  EssaySet  Score1  Score2  \\\n",
       " 15403  25949        10       2       2   \n",
       " 15404  25950        10       1       1   \n",
       " 15405  25951        10       2       2   \n",
       " 15406  25952        10       1       1   \n",
       " 15407  25953        10       2       2   \n",
       " 15408  25954        10       0       1   \n",
       " 15409  25955        10       1       0   \n",
       " 15410  25956        10       1       1   \n",
       " 15411  25957        10       1       1   \n",
       " 15412  25958        10       2       2   \n",
       " 15413  25959        10       2       2   \n",
       " 15414  25960        10       0       0   \n",
       " 15415  25961        10       1       1   \n",
       " 15416  25962        10       1       1   \n",
       " 15417  25963        10       1       0   \n",
       " 15418  25964        10       0       1   \n",
       " 15419  25965        10       0       1   \n",
       " 15420  25966        10       1       1   \n",
       " 15421  25967        10       1       0   \n",
       " 15422  25968        10       2       2   \n",
       " 15423  25969        10       1       1   \n",
       " 15424  25970        10       0       0   \n",
       " 15425  25971        10       2       2   \n",
       " 15426  25972        10       1       1   \n",
       " 15427  25973        10       2       2   \n",
       " 15428  25974        10       1       1   \n",
       " 15429  25975        10       1       2   \n",
       " 15430  25976        10       2       2   \n",
       " 15431  25977        10       2       2   \n",
       " 15432  25978        10       0       0   \n",
       " ...      ...       ...     ...     ...   \n",
       " 17013  27559        10       2       2   \n",
       " 17014  27560        10       2       2   \n",
       " 17015  27561        10       2       1   \n",
       " 17016  27562        10       0       1   \n",
       " 17017  27563        10       0       0   \n",
       " 17018  27564        10       1       1   \n",
       " 17019  27565        10       1       1   \n",
       " 17020  27566        10       2       2   \n",
       " 17021  27567        10       1       1   \n",
       " 17022  27568        10       2       2   \n",
       " 17023  27569        10       2       2   \n",
       " 17024  27570        10       2       2   \n",
       " 17025  27571        10       0       0   \n",
       " 17026  27572        10       1       1   \n",
       " 17027  27573        10       1       1   \n",
       " 17028  27574        10       2       1   \n",
       " 17029  27575        10       1       1   \n",
       " 17030  27576        10       2       2   \n",
       " 17031  27577        10       2       2   \n",
       " 17032  27578        10       0       0   \n",
       " 17033  27579        10       1       1   \n",
       " 17034  27580        10       1       1   \n",
       " 17035  27581        10       1       1   \n",
       " 17036  27582        10       1       0   \n",
       " 17037  27583        10       2       2   \n",
       " 17038  27584        10       1       1   \n",
       " 17039  27585        10       1       1   \n",
       " 17040  27586        10       1       1   \n",
       " 17041  27587        10       1       0   \n",
       " 17042  27588        10       0       1   \n",
       " \n",
       "                                                EssayText  \\\n",
       " 15403  black :: black might affect the dog house beca...   \n",
       " 15404  white :: White because you wouldn't want the d...   \n",
       " 15405  white :: the color white will reflect the suns...   \n",
       " 15406  white :: this color will keep the dog house co...   \n",
       " 15407  white :: I would use a light color like White ...   \n",
       " 15408  light gray ::  The darker of color the , the h...   \n",
       " 15409  light gray :: i think it will affect it by the...   \n",
       " 15410  black ::  the color would absorb more heat fro...   \n",
       " 15411  light gray :: i chose light gray because i was...   \n",
       " 15412  white :: It will kepp the dog house cool and n...   \n",
       " 15413  black ::  The black colored doghouse would be ...   \n",
       " 15414  white :: the color white will reflect the sunl...   \n",
       " 15415  white ::  it would reflect a lot of the light ...   \n",
       " 15416  white :: it will give the dog more cool air be...   \n",
       " 15417  black :: Well the darker the lid is the more t...   \n",
       " 15418  white :: they would want to choose white for t...   \n",
       " 15419  dark gray ::  All choosable colors are waht yo...   \n",
       " 15420  black :: it will keep the inside of the dog ho...   \n",
       " 15421  light gray :: Light Gray Is The Best Color To ...   \n",
       " 15422  white ::  Brandi and Jerry should use white pa...   \n",
       " 15423  white :: The color white would affect the dogh...   \n",
       " 15424  light gray :: on cold days the light gray abso...   \n",
       " 15425  white ::  The best color to paint the dog hous...   \n",
       " 15426  light gray :: the doghouse should be light gra...   \n",
       " 15427  white :: If they colored the doghouse white th...   \n",
       " 15428  dark gray :: The air inside will be hotter tha...   \n",
       " 15429  white ::  The color might affect the dog house...   \n",
       " 15430  black :: It will effect the dog house by it be...   \n",
       " 15431  light gray :: The light gray color would refle...   \n",
       " 15432         black :: no color will affect the doghouse   \n",
       " ...                                                  ...   \n",
       " 17013  white ::  The black was 9(DEG) C warmer then t...   \n",
       " 17014  white :: painting the dog house white will kee...   \n",
       " 17015  white :: White you be the best color for Brand...   \n",
       " 17016  dark gray :: It won't be too hot or too cold i...   \n",
       " 17017  white ::  bright colers reflect the light so t...   \n",
       " 17018  black ::  the color black would keep the the d...   \n",
       " 17019  light gray :: the color wouyld affect the dog ...   \n",
       " 17020  light gray ::  A light gray paint on the dogho...   \n",
       " 17021  white :: i think it would be white because the...   \n",
       " 17022  light gray ::  it will affect the dog house by...   \n",
       " 17023  black ::  The color black will be more warmer ...   \n",
       " 17024  black :: The black paint might make the dog ho...   \n",
       " 17025   white ::  because white dose not atract anything   \n",
       " 17026  white :: white would be better because during ...   \n",
       " 17027  dark gray :: it will warm the dog house but no...   \n",
       " 17028  light gray :: Light gray would be a little coo...   \n",
       " 17029  light gray ::  In all seasons the doghouse wil...   \n",
       " 17030  white :: The white paint color would be cooler...   \n",
       " 17031  black :: White would keep the doghouse the col...   \n",
       " 17032  white :: it does not atract as much lite as a ...   \n",
       " 17033  black :: because black absorbs heat more then ...   \n",
       " 17034  white :: The color white absorbs less heat the...   \n",
       " 17035  black :: The color black prevents more cold an...   \n",
       " 17036  light gray ::  Light gray might affect the ins...   \n",
       " 17037  light gray ::  Painting the doghouse light gra...   \n",
       " 17038  white :: white becuase if you live in a hot pl...   \n",
       " 17039  light gray :: This color will affect the dogho...   \n",
       " 17040  light gray :: i think light gray would work th...   \n",
       " 17041  dark gray ::  if they painted the doghouse dar...   \n",
       " 17042  white :: the black would affect the doghouse b...   \n",
       " \n",
       "                                                    clean  \\\n",
       " 15403  black black might affect the dog house because...   \n",
       " 15404  white white because you would n't want the dog...   \n",
       " 15405  white the color white will reflect the suns ra...   \n",
       " 15406  white this color will keep the dog house coole...   \n",
       " 15407  white i would use a light color like white in ...   \n",
       " 15408  light gray the darker of color the , the hoter...   \n",
       " 15409  light gray i think it will affect it by the do...   \n",
       " 15410  black the color would absorb more heat from th...   \n",
       " 15411  light gray i chose light gray because i was th...   \n",
       " 15412  white it will kepp the dog house cool and not ...   \n",
       " 15413  black the black colored doghouse would be warm...   \n",
       " 15414  white the color white will reflect the sunligh...   \n",
       " 15415  white it would reflect a lot of the light and ...   \n",
       " 15416  white it will give the dog more cool air becau...   \n",
       " 15417  black well the darker the lid is the more temp...   \n",
       " 15418  white they would want to choose white for the ...   \n",
       " 15419  dark gray all choosable colors are waht you mi...   \n",
       " 15420  black it will keep the inside of the dog house...   \n",
       " 15421  light gray light gray is the best color to use...   \n",
       " 15422  white brandi and jerry should use white paint ...   \n",
       " 15423  white the color white would affect the doghous...   \n",
       " 15424  light gray on cold days the light gray absorbs...   \n",
       " 15425  white the best color to paint the dog house wo...   \n",
       " 15426  light gray the doghouse should be light gray b...   \n",
       " 15427  white if they colored the doghouse white then ...   \n",
       " 15428  dark gray the air inside will be hotter than i...   \n",
       " 15429  white the color might affect the dog house jus...   \n",
       " 15430  black it will effect the dog house by it being...   \n",
       " 15431  light gray the light gray color would reflect ...   \n",
       " 15432            black no color will affect the doghouse   \n",
       " ...                                                  ...   \n",
       " 17013  white the black was  ( deg ) c warmer then the...   \n",
       " 17014  white painting the dog house white will keep t...   \n",
       " 17015  white white you be the best color for brandi a...   \n",
       " 17016  dark gray it wo n't be too hot or too cold in ...   \n",
       " 17017  white bright colers reflect the light so the d...   \n",
       " 17018  black the color black would keep the the dog warm   \n",
       " 17019  light gray the color wouyld affect the dog hou...   \n",
       " 17020  light gray a light gray paint on the doghouse ...   \n",
       " 17021  white i think it would be white because then w...   \n",
       " 17022  light gray it will affect the dog house by kee...   \n",
       " 17023  black the color black will be more warmer insi...   \n",
       " 17024  black the black paint might make the dog house...   \n",
       " 17025       white because white dose not atract anything   \n",
       " 17026  white white would be better because during the...   \n",
       " 17027  dark gray it will warm the dog house but not g...   \n",
       " 17028  light gray light gray would be a little cool i...   \n",
       " 17029  light gray in all seasons the doghouse will st...   \n",
       " 17030  white the white paint color would be cooler be...   \n",
       " 17031  black white would keep the doghouse the coldes...   \n",
       " 17032  white it does not atract as much lite as a dar...   \n",
       " 17033  black because black absorbs heat more then the...   \n",
       " 17034  white the color white absorbs less heat theref...   \n",
       " 17035  black the color black prevents more cold and a...   \n",
       " 17036  light gray light gray might affect the inside ...   \n",
       " 17037  light gray painting the doghouse light gray wo...   \n",
       " 17038  white white becuase if you live in a hot place...   \n",
       " 17039  light gray this color will affect the doghouse...   \n",
       " 17040  light gray i think light gray would work the b...   \n",
       " 17041  dark gray if they painted the doghouse dark gr...   \n",
       " 17042  white the black would affect the doghouse beca...   \n",
       " \n",
       "                                                        X  \\\n",
       " 15403  [89, 89, 362, 350, 1, 90, 140, 13, 31, 1, 144,...   \n",
       " 15404  [63, 63, 13, 32, 15, 50, 193, 1, 90, 2, 2141, ...   \n",
       " 15405  [63, 1, 113, 63, 66, 1182, 1, 2503, 2992, 7, 1...   \n",
       " 15406  [63, 18, 113, 66, 192, 1, 90, 140, 331, 3498, ...   \n",
       " 15407  [63, 34, 15, 117, 6, 131, 113, 74, 63, 5, 18, ...   \n",
       " 15408  [131, 107, 1, 458, 7, 113, 1, 1, 3225, 9, 704,...   \n",
       " 15409  [131, 107, 34, 166, 9, 66, 350, 9, 30, 1, 2521...   \n",
       " 15410  [89, 1, 113, 15, 431, 55, 202, 42, 1, 526, 298...   \n",
       " 15411  [131, 107, 34, 789, 131, 107, 13, 34, 19, 1284...   \n",
       " 15412  [63, 9, 66, 4389, 1, 90, 140, 415, 3, 37, 2, 1...   \n",
       " 15413  [89, 1, 89, 1093, 144, 15, 14, 478, 27, 1, 197...   \n",
       " 15414  [63, 1, 113, 63, 66, 1182, 1, 1196, 137, 74, 1...   \n",
       " 15415  [63, 9, 15, 1182, 6, 366, 7, 1, 131, 3, 202, 2...   \n",
       " 15416  [63, 9, 66, 372, 1, 90, 55, 415, 402, 13, 2648...   \n",
       " 15417  [89, 200, 1, 458, 1, 229, 4, 1, 55, 15134, 154...   \n",
       " 15418  [63, 10, 15, 193, 2, 753, 63, 20, 1, 442, 13, ...   \n",
       " 15419  [167, 107, 87, 15136, 325, 11, 4297, 32, 362, ...   \n",
       " 15420  [89, 9, 66, 192, 1, 197, 7, 1, 90, 140, 268, 9...   \n",
       " 15421  [131, 107, 131, 107, 4, 1, 231, 113, 2, 117, 1...   \n",
       " 15422  [63, 730, 3, 710, 82, 117, 63, 287, 2073, 1121...   \n",
       " 15423  [63, 1, 113, 63, 15, 350, 1, 144, 9, 59, 9, 39...   \n",
       " 15424  [131, 107, 27, 293, 771, 1, 131, 107, 647, 1, ...   \n",
       " 15425  [63, 1, 231, 113, 2, 287, 1, 90, 140, 15, 81, ...   \n",
       " 15426  [131, 107, 1, 144, 82, 14, 131, 107, 13, 9, 13...   \n",
       " 15427  [63, 79, 10, 1093, 1, 144, 63, 31, 9, 1939, 19...   \n",
       " 15428  [167, 107, 1, 402, 197, 66, 14, 815, 129, 9, 1...   \n",
       " 15429  [63, 1, 113, 362, 350, 1, 90, 140, 137, 6, 123...   \n",
       " 15430  [89, 9, 66, 282, 1, 90, 140, 30, 9, 148, 55, 1...   \n",
       " 15431  [131, 107, 1, 131, 107, 113, 15, 1182, 163, 7,...   \n",
       " 15432  [89, 277, 113, 66, 350, 1, 144, 0, 0, 0, 0, 0,...   \n",
       " ...                                                  ...   \n",
       " 17013  [63, 1, 89, 19, 121, 111, 478, 31, 1, 63, 1093...   \n",
       " 17014  [63, 1106, 1, 90, 140, 63, 66, 192, 1, 197, 33...   \n",
       " 17015  [63, 63, 32, 14, 1, 231, 113, 20, 730, 3, 1593...   \n",
       " 17016  [167, 107, 9, 611, 50, 14, 191, 179, 62, 191, ...   \n",
       " 17017  [63, 1812, 15940, 1182, 1, 131, 59, 1, 90, 193...   \n",
       " 17018  [89, 1, 113, 89, 15, 192, 1, 1, 90, 268, 0, 0,...   \n",
       " 17019  [131, 107, 1, 113, 15941, 350, 1, 90, 140, 5, ...   \n",
       " 17020  [131, 107, 6, 131, 107, 287, 27, 1, 144, 362, ...   \n",
       " 17021  [63, 34, 166, 9, 15, 14, 63, 13, 31, 48, 34, 2...   \n",
       " 17022  [131, 107, 9, 66, 350, 1, 90, 140, 30, 823, 9,...   \n",
       " 17023  [89, 1, 113, 89, 66, 14, 55, 478, 197, 129, 30...   \n",
       " 17024  [89, 1, 89, 287, 362, 97, 1, 90, 140, 5243, 52...   \n",
       " 17025  [63, 13, 63, 1761, 37, 2212, 257, 0, 0, 0, 0, ...   \n",
       " 17026  [63, 63, 15, 14, 208, 13, 451, 1, 5233, 909, 9...   \n",
       " 17027  [167, 107, 9, 66, 268, 1, 90, 140, 52, 37, 109...   \n",
       " 17028  [131, 107, 131, 107, 15, 14, 6, 244, 415, 5, 1...   \n",
       " 17029  [131, 107, 5, 87, 2891, 1, 144, 66, 421, 78, 6...   \n",
       " 17030  [63, 1, 63, 287, 113, 15, 14, 331, 1742, 452, ...   \n",
       " 17031  [89, 63, 15, 192, 1, 144, 1, 1767, 1896, 5246,...   \n",
       " 17032  [63, 9, 130, 37, 2212, 38, 70, 15961, 38, 6, 4...   \n",
       " 17033  [89, 13, 89, 647, 202, 55, 31, 1, 2591, 904, 3...   \n",
       " 17034  [63, 1, 113, 63, 647, 422, 202, 508, 917, 2301...   \n",
       " 17035  [89, 1, 113, 89, 4233, 55, 293, 3, 431, 448, 5...   \n",
       " 17036  [131, 107, 131, 107, 362, 350, 1, 197, 7, 1, 9...   \n",
       " 17037  [131, 107, 1106, 1, 144, 131, 107, 15, 192, 20...   \n",
       " 17038  [63, 63, 1078, 79, 32, 160, 5, 6, 179, 310, 1,...   \n",
       " 17039  [131, 107, 18, 113, 66, 350, 1, 144, 20, 1, 81...   \n",
       " 17040  [131, 107, 34, 166, 131, 107, 15, 228, 1, 231,...   \n",
       " 17041  [167, 107, 79, 10, 864, 1, 144, 167, 107, 31, ...   \n",
       " 17042  [63, 1, 89, 15, 350, 1, 144, 13, 205, 12, 526,...   \n",
       " \n",
       "                                                      X_1  \n",
       " 15403  [89, 89, 362, 350, 1, 90, 140, 13, 31, 1, 144,...  \n",
       " 15404  [63, 63, 13, 32, 15, 50, 193, 1, 90, 2, 2141, ...  \n",
       " 15405  [63, 1, 113, 63, 66, 1182, 1, 2503, 2992, 7, 1...  \n",
       " 15406  [63, 18, 113, 66, 192, 1, 90, 140, 331, 3498, ...  \n",
       " 15407  [63, 34, 15, 117, 6, 131, 113, 74, 63, 5, 18, ...  \n",
       " 15408  [131, 107, 1, 458, 7, 113, 1, 1, 3225, 9, 704,...  \n",
       " 15409  [131, 107, 34, 166, 9, 66, 350, 9, 30, 1, 2521...  \n",
       " 15410  [89, 1, 113, 15, 431, 55, 202, 42, 1, 526, 298...  \n",
       " 15411  [131, 107, 34, 789, 131, 107, 13, 34, 19, 1284...  \n",
       " 15412  [63, 9, 66, 4389, 1, 90, 140, 415, 3, 37, 2, 1...  \n",
       " 15413  [89, 1, 89, 1093, 144, 15, 14, 478, 27, 1, 197...  \n",
       " 15414  [63, 1, 113, 63, 66, 1182, 1, 1196, 137, 74, 1...  \n",
       " 15415  [63, 9, 15, 1182, 6, 366, 7, 1, 131, 3, 202, 2...  \n",
       " 15416  [63, 9, 66, 372, 1, 90, 55, 415, 402, 13, 2648...  \n",
       " 15417  [89, 200, 1, 458, 1, 229, 4, 1, 55, 15134, 154...  \n",
       " 15418  [63, 10, 15, 193, 2, 753, 63, 20, 1, 442, 13, ...  \n",
       " 15419  [167, 107, 87, 15136, 325, 11, 4297, 32, 362, ...  \n",
       " 15420  [89, 9, 66, 192, 1, 197, 7, 1, 90, 140, 268, 9...  \n",
       " 15421  [131, 107, 131, 107, 4, 1, 231, 113, 2, 117, 1...  \n",
       " 15422  [63, 730, 3, 710, 82, 117, 63, 287, 2073, 1121...  \n",
       " 15423  [63, 1, 113, 63, 15, 350, 1, 144, 9, 59, 9, 39...  \n",
       " 15424  [131, 107, 27, 293, 771, 1, 131, 107, 647, 1, ...  \n",
       " 15425  [63, 1, 231, 113, 2, 287, 1, 90, 140, 15, 81, ...  \n",
       " 15426  [131, 107, 1, 144, 82, 14, 131, 107, 13, 9, 13...  \n",
       " 15427  [63, 79, 10, 1093, 1, 144, 63, 31, 9, 1939, 19...  \n",
       " 15428  [167, 107, 1, 402, 197, 66, 14, 815, 129, 9, 1...  \n",
       " 15429  [63, 1, 113, 362, 350, 1, 90, 140, 137, 6, 123...  \n",
       " 15430  [89, 9, 66, 282, 1, 90, 140, 30, 9, 148, 55, 1...  \n",
       " 15431  [131, 107, 1, 131, 107, 113, 15, 1182, 163, 7,...  \n",
       " 15432  [89, 277, 113, 66, 350, 1, 144, 0, 0, 0, 0, 0,...  \n",
       " ...                                                  ...  \n",
       " 17013  [63, 1, 89, 19, 121, 111, 478, 31, 1, 63, 1093...  \n",
       " 17014  [63, 1106, 1, 90, 140, 63, 66, 192, 1, 197, 33...  \n",
       " 17015  [63, 63, 32, 14, 1, 231, 113, 20, 730, 3, 1593...  \n",
       " 17016  [167, 107, 9, 611, 50, 14, 191, 179, 62, 191, ...  \n",
       " 17017  [63, 1812, 15940, 1182, 1, 131, 59, 1, 90, 193...  \n",
       " 17018  [89, 1, 113, 89, 15, 192, 1, 1, 90, 268, 0, 0,...  \n",
       " 17019  [131, 107, 1, 113, 15941, 350, 1, 90, 140, 5, ...  \n",
       " 17020  [131, 107, 6, 131, 107, 287, 27, 1, 144, 362, ...  \n",
       " 17021  [63, 34, 166, 9, 15, 14, 63, 13, 31, 48, 34, 2...  \n",
       " 17022  [131, 107, 9, 66, 350, 1, 90, 140, 30, 823, 9,...  \n",
       " 17023  [89, 1, 113, 89, 66, 14, 55, 478, 197, 129, 30...  \n",
       " 17024  [89, 1, 89, 287, 362, 97, 1, 90, 140, 5243, 52...  \n",
       " 17025  [63, 13, 63, 1761, 37, 2212, 257, 0, 0, 0, 0, ...  \n",
       " 17026  [63, 63, 15, 14, 208, 13, 451, 1, 5233, 909, 9...  \n",
       " 17027  [167, 107, 9, 66, 268, 1, 90, 140, 52, 37, 109...  \n",
       " 17028  [131, 107, 131, 107, 15, 14, 6, 244, 415, 5, 1...  \n",
       " 17029  [131, 107, 5, 87, 2891, 1, 144, 66, 421, 78, 6...  \n",
       " 17030  [63, 1, 63, 287, 113, 15, 14, 331, 1742, 452, ...  \n",
       " 17031  [89, 63, 15, 192, 1, 144, 1, 1767, 1896, 5246,...  \n",
       " 17032  [63, 9, 130, 37, 2212, 38, 70, 15961, 38, 6, 4...  \n",
       " 17033  [89, 13, 89, 647, 202, 55, 31, 1, 2591, 904, 3...  \n",
       " 17034  [63, 1, 113, 63, 647, 422, 202, 508, 917, 2301...  \n",
       " 17035  [89, 1, 113, 89, 4233, 55, 293, 3, 431, 448, 5...  \n",
       " 17036  [131, 107, 131, 107, 362, 350, 1, 197, 7, 1, 9...  \n",
       " 17037  [131, 107, 1106, 1, 144, 131, 107, 15, 192, 20...  \n",
       " 17038  [63, 63, 1078, 79, 32, 160, 5, 6, 179, 310, 1,...  \n",
       " 17039  [131, 107, 18, 113, 66, 350, 1, 144, 20, 1, 81...  \n",
       " 17040  [131, 107, 34, 166, 131, 107, 15, 228, 1, 231,...  \n",
       " 17041  [167, 107, 79, 10, 864, 1, 144, 167, 107, 31, ...  \n",
       " 17042  [63, 1, 89, 15, 350, 1, 144, 13, 205, 12, 526,...  \n",
       " \n",
       " [1640 rows x 8 columns]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths=[len(data[data['EssaySet'] == i]) for i in np.unique(data['EssaySet'])]\n",
    "le = []\n",
    "le = list([data[data['EssaySet'] == i] for i in np.unique(data['EssaySet'])])\n",
    "le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1672, 1278, 1808, 1657, 1795, 1797, 1799, 1799, 1798, 1640]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list_ref=[]\n",
    "for i in np.unique(data['EssaySet']):\n",
    "    for j in le:\n",
    "        final_list_ref.append(list(X_ref)[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['Ref_ans'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-6128395ddb65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Ref_ans'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mfinal_list_ref\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6334\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6335\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 6336\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   6337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6338\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6349\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[1;32m   6350\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6351\u001b[0;31m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[1;32m   6352\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     60\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                          validate=validate)\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n\u001b[0;32m--> 574\u001b[0;31m                                                      rdata.items, rsuf)\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mlindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mleft_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mitems_overlap_with_suffix\u001b[0;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[1;32m   5242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5243\u001b[0m             raise ValueError('columns overlap but no suffix specified: '\n\u001b[0;32m-> 5244\u001b[0;31m                              '{rename}'.format(rename=to_rename))\n\u001b[0m\u001b[1;32m   5245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5246\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlrenamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['Ref_ans'], dtype='object')"
     ]
    }
   ],
   "source": [
    "p = pd.DataFrame({'Ref_ans' : final_list_ref})\n",
    "data = data.join(p)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1321,    0,    0, ...,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reflist[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLSTM with Reference Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag):\n",
    "    input_sentence= Input(shape=(max_length,),name='Sentence')    \n",
    "    input_reference = Input(shape=(max_length,),name='Reference')\n",
    "    \n",
    "    embedding=Embedding(vocab_size, len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = False)\n",
    "\n",
    "    context = embedding(input_sentence)\n",
    "    reference = embedding(input_reference)\n",
    "    \n",
    "    combined= concatenate([context, reference])\n",
    "    combined=Dropout(0.5)(combined)\n",
    "    c = Conv1D(150,5,activation='relu')(combined)\n",
    "    \n",
    "    hidden,_,_,_,_ = Bidirectional(LSTM(300, return_sequences=True, return_state = True, dropout=0.25, recurrent_dropout=0.1))(c)\n",
    "    \n",
    "    a = Attention()(hidden)\n",
    "    #a = AveragePooling1D(a)\n",
    "\n",
    "    x=Dense(300,activation='relu')(a)\n",
    "        \n",
    "    output=Dense(4,activation='softmax')(x)\n",
    "\n",
    "    model= Model(inputs=[input_sentence,input_reference] ,outputs=output)\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda,Reshape,concatenate,Input, Embedding, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=300,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix_glove',\n",
    "                     em_trainable_flag=False\n",
    "                    )\n",
    "\n",
    "EarlyStop= EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(x=[X_train,X_ref_train],y=y_train, epochs=100,batch_size=32,\n",
    "              callbacks=[EarlyStop],validation_data=([X_test,X_ref_test],y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_metrics as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([X_test,X_ref_test])\n",
    "out= get_class_from_pred(pred)\n",
    "actual= get_class_from_pred(y_test)\n",
    "\n",
    "print(metrics.quadratic_weighted_kappa(actual,out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"ASAP_TRAIN_ESSAY1_DATA.dat\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pickle.load(open(\"ASAP_Test_DATA.json\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = pickle.load(open(\"ASAP_Ques1_DATA.dat\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefAns1 = ['You need to know how much vinegar was used in each container. You need to know what type of vinegar was used in each container. You need to know what materials to test. You need to know what size/surface area of materials should be used. You need to know how long each sample was rinsed in distilled water. You need to know what drying method to use. You need to know what size/type of container to use.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>EssaySet</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>EssayText</th>\n",
       "      <th>clean</th>\n",
       "      <th>X</th>\n",
       "      <th>X_1</th>\n",
       "      <th>error_words</th>\n",
       "      <th>total_errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Some additional information that we would need...</td>\n",
       "      <td>some additional information that we would need...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>After reading the expirement, I realized that ...</td>\n",
       "      <td>after reading the expirement , i realized that...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[expirement, expireiment, amant, yar, expireme...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What you need is more trials, a control set up...</td>\n",
       "      <td>what you need is more trials , a control set u...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The student should list what rock is better an...</td>\n",
       "      <td>the student should list what rock is better an...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>For the students to be able to make a replicat...</td>\n",
       "      <td>for the students to be able to make a replicat...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[tipe, expirement]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  EssaySet  Score1  Score2  \\\n",
       "0   1         1       1       1   \n",
       "1   2         1       1       1   \n",
       "2   3         1       1       1   \n",
       "3   4         1       0       0   \n",
       "4   5         1       2       2   \n",
       "\n",
       "                                           EssayText  \\\n",
       "0  Some additional information that we would need...   \n",
       "1  After reading the expirement, I realized that ...   \n",
       "2  What you need is more trials, a control set up...   \n",
       "3  The student should list what rock is better an...   \n",
       "4  For the students to be able to make a replicat...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  some additional information that we would need...   \n",
       "1  after reading the expirement , i realized that...   \n",
       "2  what you need is more trials , a control set u...   \n",
       "3  the student should list what rock is better an...   \n",
       "4  for the students to be able to make a replicat...   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                                 X_1  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                         error_words  total_errors  \n",
       "0                                                 []             0  \n",
       "1  [expirement, expireiment, amant, yar, expireme...             6  \n",
       "2                                                 []             0  \n",
       "3                                                 []             0  \n",
       "4                                 [tipe, expirement]             2  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r'\\d+', '', string)\n",
    "\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=779"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os as os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def load_training_data(training_path, EssaySet):   #tokenizing and cleaning #REMEMBER THIS ESSAY_SET = 1 intialization \n",
    "    # resolved score for essay set 1\n",
    "    resolved_score = data[data['EssaySet'] == EssaySet]['Score1']\n",
    "    essay_ids = data[data['EssaySet'] == EssaySet]['Id']\n",
    "    essays = data[data['EssaySet'] == EssaySet]['X_1']\n",
    "\n",
    "    return essays, resolved_score.tolist(), essay_ids.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(test_path, EssaySet):   #tokenizing and cleaning #REMEMBER THIS ESSAY_SET = 1 intialization \n",
    "    # resolved score for essay set 1\n",
    "    essay_ids = test[test['EssaySet'] == EssaySet]['Id']\n",
    "    essays = test[test['EssaySet'] == EssaySet]['X_2']\n",
    "    # turn an essay to a list of words\n",
    "   \n",
    "    return essays, essay_ids.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = [clean_str(i) for i in data['EssayText']]\n",
    "data['clean']=essay\n",
    "essay = [clean_str(i) for i in test['EssayText']]\n",
    "test['clean']=essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTIONS\n",
    "q_essay = [clean_str(i) for i in df['Questions']]\n",
    "df['clean_Q']=q_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>clean_Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After reading the group’s procedure, describe ...</td>\n",
       "      <td>after reading the group s procedure , describe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Draw a conclusion based on the student’s data....</td>\n",
       "      <td>draw a conclusion based on the student s data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explain how pandas in China are similar to koa...</td>\n",
       "      <td>explain how pandas in china are similar to koa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain the significance of the word “invasive...</td>\n",
       "      <td>explain the significance of the word invasive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Starting with mRNA leaving the nucleus, list a...</td>\n",
       "      <td>starting with mrna leaving the nucleus , list ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>List and describe three processes used by cell...</td>\n",
       "      <td>list and describe three processes used by cell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Identify ONE trait that can describe Rose base...</td>\n",
       "      <td>identify one trait that can describe rose base...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>During the story, the reader gets background i...</td>\n",
       "      <td>during the story , the reader gets background ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the author organize the article? Supp...</td>\n",
       "      <td>how does the author organize the article ? sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the effect of different lid colors on ...</td>\n",
       "      <td>what is the effect of different lid colors on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Questions  \\\n",
       "0  After reading the group’s procedure, describe ...   \n",
       "1  Draw a conclusion based on the student’s data....   \n",
       "2  Explain how pandas in China are similar to koa...   \n",
       "3  Explain the significance of the word “invasive...   \n",
       "4  Starting with mRNA leaving the nucleus, list a...   \n",
       "5  List and describe three processes used by cell...   \n",
       "6  Identify ONE trait that can describe Rose base...   \n",
       "7  During the story, the reader gets background i...   \n",
       "8  How does the author organize the article? Supp...   \n",
       "9  What is the effect of different lid colors on ...   \n",
       "\n",
       "                                             clean_Q  \n",
       "0  after reading the group s procedure , describe...  \n",
       "1  draw a conclusion based on the student s data ...  \n",
       "2  explain how pandas in china are similar to koa...  \n",
       "3  explain the significance of the word invasive ...  \n",
       "4  starting with mrna leaving the nucleus , list ...  \n",
       "5  list and describe three processes used by cell...  \n",
       "6  identify one trait that can describe rose base...  \n",
       "7  during the story , the reader gets background ...  \n",
       "8  how does the author organize the article ? sup...  \n",
       "9  what is the effect of different lid colors on ...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(data['clean'])\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(\"GLOVE.DAT\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_matrix(model):\n",
    "    embedding_matrix = np.memmap('test.mymemmap', dtype='float32', mode='w+', shape=(vocab_size, 100))\n",
    "#    embedding_matrix = np.random.uniform(-0.001, 0.001, (vocab_size, 200))\n",
    "    w2v={}\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector= None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "            w2v[word]=i\n",
    "\n",
    "    return embedding_matrix,w2v\n",
    "\n",
    "#embedding_matrix = build_embedding_matrix(vocab_size, 300,tokenizer)\n",
    "\n",
    "embedding_matrix_glove,glove_w2v=get_glove_matrix(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "#        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(learning_rate,dropout,lstm_out,n_hidden_layer,em,em_trainable_flag,free_em_dim):\n",
    "    model1 = Sequential()\n",
    "    model1.add(Embedding(vocab_size , len(eval(em)[0]), weights = [eval(em)],input_length=max_length,trainable = em_trainable_flag))\n",
    " #   model1.add(Dropout(0.2))\n",
    "#    model1.add(Conv1D(50, 5, activation='relu'))\n",
    "#    model1.add(MaxPooling1D(pool_size=4))\n",
    "    model1.add(Bidirectional(LSTM(lstm_out, return_sequences = True)))\n",
    "    model1.add(Attention())\n",
    "    for i in range(n_hidden_layer):\n",
    "        model1.add(Dense(int((2*lstm_out+4)/2),activation='relu'))\n",
    "    model1.add(Dense(4,activation='softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate ) #, rho = 0.9, clipnorm = 10\n",
    "    model1.compile(loss = 'categorical_crossentropy', optimizer=optimizer,metrics = ['accuracy'])\n",
    "    print(model1.summary())\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 779, 100)          235500    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 779, 500)          702000    \n",
      "_________________________________________________________________\n",
      "attention_6 (Attention)      (None, 500)               1279      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 252)               126252    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 1012      \n",
      "=================================================================\n",
      "Total params: 1,066,043\n",
      "Trainable params: 830,543\n",
      "Non-trainable params: 235,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model1 = define_model(learning_rate=0.001,\n",
    "                     dropout=0.5,\n",
    "                     lstm_out=250,\n",
    "                     n_hidden_layer=1,\n",
    "                     em='embedding_matrix_glove',\n",
    "                     em_trainable_flag=False,\n",
    "                     free_em_dim=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks = [\n",
    "     EarlyStopping(monitor='val_acc' , patience = 5 , verbose=1)\n",
    "    #ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>EssaySet</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>EssayText</th>\n",
       "      <th>clean</th>\n",
       "      <th>X</th>\n",
       "      <th>X_1</th>\n",
       "      <th>error_words</th>\n",
       "      <th>total_errors</th>\n",
       "      <th>CosineSim_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Some additional information that we would need...</td>\n",
       "      <td>some additional information that we would need...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.868468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>After reading the expirement, I realized that ...</td>\n",
       "      <td>after reading the expirement , i realized that...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[expirement, expireiment, amant, yar, expireme...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.836522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What you need is more trials, a control set up...</td>\n",
       "      <td>what you need is more trials , a control set u...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.835054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The student should list what rock is better an...</td>\n",
       "      <td>the student should list what rock is better an...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>For the students to be able to make a replicat...</td>\n",
       "      <td>for the students to be able to make a replicat...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[tipe, expirement]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.878116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  EssaySet  Score1  Score2  \\\n",
       "0   1         1       1       1   \n",
       "1   2         1       1       1   \n",
       "2   3         1       1       1   \n",
       "3   4         1       0       0   \n",
       "4   5         1       2       2   \n",
       "\n",
       "                                           EssayText  \\\n",
       "0  Some additional information that we would need...   \n",
       "1  After reading the expirement, I realized that ...   \n",
       "2  What you need is more trials, a control set up...   \n",
       "3  The student should list what rock is better an...   \n",
       "4  For the students to be able to make a replicat...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  some additional information that we would need...   \n",
       "1  after reading the expirement , i realized that...   \n",
       "2  what you need is more trials , a control set u...   \n",
       "3  the student should list what rock is better an...   \n",
       "4  for the students to be able to make a replicat...   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                                 X_1  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                         error_words  total_errors  \\\n",
       "0                                                 []             0   \n",
       "1  [expirement, expireiment, amant, yar, expireme...             6   \n",
       "2                                                 []             0   \n",
       "3                                                 []             0   \n",
       "4                                 [tipe, expirement]             2   \n",
       "\n",
       "   CosineSim_2  \n",
       "0     0.868468  \n",
       "1     0.836522  \n",
       "2     0.835054  \n",
       "3     0.710318  \n",
       "4     0.878116  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 1 1]\n",
      "(1672, 779) (1672,)\n",
      "(1114, 779) (1114,)\n",
      "Train on 1114 samples, validate on 558 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[19,33] = 7361 is not in [0, 2355)\n\t [[Node: embedding_6/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_6/embeddings/read, embedding_6/Cast, embedding_6/embedding_lookup/axis)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-94c14185bf8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[19,33] = 7361 is not in [0, 2355)\n\t [[Node: embedding_6/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_6/embeddings/read, embedding_6/Cast, embedding_6/embedding_lookup/axis)]]"
     ]
    }
   ],
   "source": [
    "count=0;\n",
    "test_count=0;\n",
    "cvscores = []\n",
    "final_Core= [] \n",
    "ID=[]\n",
    "NUMBER=[]\n",
    "\n",
    "#for i in np.unique(data['EssaySet']):\n",
    "    \n",
    "#train\n",
    "essay_list, resolved_scores, essay_id = load_training_data(data, 1) #REMEMBER THIS ESSAY_SET = 1 intialization\n",
    "\n",
    "dataX = essay_list\n",
    "dataY = resolved_scores\n",
    "no_of_classes = len(set(resolved_scores))\n",
    "\n",
    "trainX = [np.asarray(i) for i in essay_list]\n",
    "trainX = np.asarray(trainX)\n",
    "trainY = dataY\n",
    "trainY = np.asarray(trainY)\n",
    "\n",
    "print(trainY) \n",
    "\n",
    "print ((trainX.shape),(trainY.shape))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "skf.get_n_splits(trainX, trainY)\n",
    "\n",
    "for train_index, test_index in skf.split(trainX,trainY):\n",
    "    #print (train_index , test_index)\n",
    "    X_train, X_test = trainX[train_index], trainX[test_index]\n",
    "    y_train, y_test = trainY[train_index], trainY[test_index]\n",
    "    print (X_train.shape, y_train.shape)\n",
    "    y_train = to_categorical(y_train , 4)\n",
    "    y_test = to_categorical(y_test , 4)\n",
    "    model1.fit(X_train, y_train, epochs=20,batch_size=32, callbacks = callbacks ,validation_data = (X_test,y_test))  \n",
    "    score = model1.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model1.metrics_names[1], score[1]*100))\n",
    "    print(\"model%d\" %1)\n",
    "    cvscores.append(score[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test_essay_list, test_essay_id = load_test_data(test, 1)\n",
    "\n",
    "\n",
    "# testX= [np.asarray(i) for i in test_essay_list]\n",
    "# testX = np.asarray(testX)\n",
    "\n",
    "# #predict scores\n",
    "# testY = model.predict_classes(testX)\n",
    "# final_Core.append(testY)\n",
    "# ID.append(test_essay_id)\n",
    "\n",
    "\n",
    "#for j in list(testY):\n",
    "#    NUMBER.append(i)\n",
    "\n",
    "\n",
    "       \n",
    "    #model_json = model.to_json()\n",
    " #   with open(\"model%d.json \" %i, \"w\") as json_file:\n",
    "#  json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    " #   model.save_weights(\"model%d.h5\" %i)\n",
    "  #  print(\"Saved model%d to disk\" %i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = data['clean'][-500:]\n",
    "y = data['Score1'][-500:]\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model1 to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model1.to_json()\n",
    "with open(\"model%d.json \" %1, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    #serialize weights to HDF5\n",
    "    model1.save_weights(\"model%d.h5\" %1)\n",
    "    print(\"Saved model%d to disk\" %1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipeline = make_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(556, 4)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose a sample from test set\n",
    "idx = 11\n",
    "text_sample = X_test[idx]\n",
    "class_names = ['0', '1' ,'2' , '3']\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from collections import OrderedDict\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "for i in test :\n",
    "    all_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample= X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 11: last 80 words (only part used by the model)\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_1_input to have shape (779,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-9c7b884c250c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(\" \".join(text_sample.split()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Probability(positive) ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'True class: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_final_estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             warnings.warn('Network returning invalid probability values. '\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                              'argument.')\n\u001b[1;32m   1146\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_1_input to have shape (779,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "print('Sample {}: last 80 words (only part used by the model)'.format(idx))\n",
    "print('-'*50)\n",
    "print(\" \".join(text_sample.split()))\n",
    "print('-'*50)\n",
    "print('Probability(positive) =', pipeline.predict_proba([text_sample])[0,1])\n",
    "print('True class: %s' % class_names[y_test[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aruhi/tensorflow/venv/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-dccb79b50954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexplanation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/lime/lime_text.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    388\u001b[0m         data, yss, distances = self.__data_labels_distances(\n\u001b[1;32m    389\u001b[0m             \u001b[0mindexed_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             distance_metric=distance_metric)\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/lime/lime_text.py\u001b[0m in \u001b[0;36m__data_labels_distances\u001b[0;34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minactive\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0minverse_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_removing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minactive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_final_estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             warnings.warn('Network returning invalid probability values. '\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                              'argument.')\n\u001b[1;32m   1146\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "explanation = explainer.explain_instance(text_sample, pipeline.predict_proba, num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = OrderedDict(explanation.as_list())\n",
    "lime_weights = pd.DataFrame({'words': list(weights.keys()), 'weights': list(weights.values())})\n",
    "\n",
    "sns.barplot(x=\"words\", y=\"weights\", data=lime_weights);\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Sample {} features weights given by LIME'.format(idx));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = training_data_instance_number\n",
    "\n",
    "explanation = explainer.explain_instance(\n",
    "    X_test[i],\n",
    "    forest_reg_grid.best_estimator_.predict,\n",
    "    num_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>EssaySet</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>EssayText</th>\n",
       "      <th>clean</th>\n",
       "      <th>X</th>\n",
       "      <th>X_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17038</th>\n",
       "      <td>27584</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>white :: white becuase if you live in a hot pl...</td>\n",
       "      <td>white white becuase if you live in a hot place...</td>\n",
       "      <td>[63, 63, 1078, 79, 32, 160, 5, 6, 179, 310, 1,...</td>\n",
       "      <td>[63, 63, 1078, 79, 32, 160, 5, 6, 179, 310, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17039</th>\n",
       "      <td>27585</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>light gray :: This color will affect the dogho...</td>\n",
       "      <td>light gray this color will affect the doghouse...</td>\n",
       "      <td>[131, 107, 18, 113, 66, 350, 1, 144, 20, 1, 81...</td>\n",
       "      <td>[131, 107, 18, 113, 66, 350, 1, 144, 20, 1, 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17040</th>\n",
       "      <td>27586</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>light gray :: i think light gray would work th...</td>\n",
       "      <td>light gray i think light gray would work the b...</td>\n",
       "      <td>[131, 107, 34, 166, 131, 107, 15, 228, 1, 231,...</td>\n",
       "      <td>[131, 107, 34, 166, 131, 107, 15, 228, 1, 231,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17041</th>\n",
       "      <td>27587</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dark gray ::  if they painted the doghouse dar...</td>\n",
       "      <td>dark gray if they painted the doghouse dark gr...</td>\n",
       "      <td>[167, 107, 79, 10, 864, 1, 144, 167, 107, 31, ...</td>\n",
       "      <td>[167, 107, 79, 10, 864, 1, 144, 167, 107, 31, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17042</th>\n",
       "      <td>27588</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>white :: the black would affect the doghouse b...</td>\n",
       "      <td>white the black would affect the doghouse beca...</td>\n",
       "      <td>[63, 1, 89, 15, 350, 1, 144, 13, 205, 12, 526,...</td>\n",
       "      <td>[63, 1, 89, 15, 350, 1, 144, 13, 205, 12, 526,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  EssaySet  Score1  Score2  \\\n",
       "17038  27584        10       1       1   \n",
       "17039  27585        10       1       1   \n",
       "17040  27586        10       1       1   \n",
       "17041  27587        10       1       0   \n",
       "17042  27588        10       0       1   \n",
       "\n",
       "                                               EssayText  \\\n",
       "17038  white :: white becuase if you live in a hot pl...   \n",
       "17039  light gray :: This color will affect the dogho...   \n",
       "17040  light gray :: i think light gray would work th...   \n",
       "17041  dark gray ::  if they painted the doghouse dar...   \n",
       "17042  white :: the black would affect the doghouse b...   \n",
       "\n",
       "                                                   clean  \\\n",
       "17038  white white becuase if you live in a hot place...   \n",
       "17039  light gray this color will affect the doghouse...   \n",
       "17040  light gray i think light gray would work the b...   \n",
       "17041  dark gray if they painted the doghouse dark gr...   \n",
       "17042  white the black would affect the doghouse beca...   \n",
       "\n",
       "                                                       X  \\\n",
       "17038  [63, 63, 1078, 79, 32, 160, 5, 6, 179, 310, 1,...   \n",
       "17039  [131, 107, 18, 113, 66, 350, 1, 144, 20, 1, 81...   \n",
       "17040  [131, 107, 34, 166, 131, 107, 15, 228, 1, 231,...   \n",
       "17041  [167, 107, 79, 10, 864, 1, 144, 167, 107, 31, ...   \n",
       "17042  [63, 1, 89, 15, 350, 1, 144, 13, 205, 12, 526,...   \n",
       "\n",
       "                                                     X_1  \n",
       "17038  [63, 63, 1078, 79, 32, 160, 5, 6, 179, 310, 1,...  \n",
       "17039  [131, 107, 18, 113, 66, 350, 1, 144, 20, 1, 81...  \n",
       "17040  [131, 107, 34, 166, 131, 107, 15, 228, 1, 231,...  \n",
       "17041  [167, 107, 79, 10, 864, 1, 144, 167, 107, 31, ...  \n",
       "17042  [63, 1, 89, 15, 350, 1, 144, 13, 205, 12, 526,...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17043"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTD of MAIN BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_an = pd.DataFrame(data={'id_':np.asarray(ID),'SCORE':np.asarray(final_Core) ,'set_' : np.asarray(NUMBER)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(set(test['EssaySet'])):\n",
    "    # load json and create model\n",
    "#    json_file = open('model%d.json' %i, 'r')\n",
    "#    loaded_model_json = json_file.read()\n",
    "#    json_file.close()\n",
    "#    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    # load weights into new model\n",
    "#    loaded_model.load_weights(\"model%d.h5\" %i)\n",
    "#    print(\"Loaded model%d from disk\" %i)\n",
    "    \n",
    "    #test\n",
    "    test_essay_list, test_essay_id = load_test_data(test, i)\n",
    "    \n",
    "    testX = test_essay_list\n",
    "    testX = np.asarray(testX)\n",
    "    \n",
    "    #predict scores\n",
    "    testY = model.predict_classes(testX)\n",
    "    # evaluate loaded model on test data\n",
    "    loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    score = loaded_model.evaluate(testX, testY, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "import sys\n",
    "from keras.models import load_model\n",
    "import warnings\n",
    "\n",
    "class ExponentialMovingAverage(Callback):\n",
    "    \"\"\"create a copy of trainable weights which gets updated at every\n",
    "       batch using exponential weight decay. The moving average weights along\n",
    "       with the other states of original model(except original model trainable\n",
    "       weights) will be saved at every epoch if save_mv_ave_model is True.\n",
    "       If both save_mv_ave_model and save_best_only are True, the latest \n",
    "       best moving average model according to the quantity monitored \n",
    "       will not be overwritten. Of course, save_best_only can be True\n",
    "       only if there is a validation set.\n",
    "       This is equivalent to save_best_only mode of ModelCheckpoint\n",
    "       callback with similar code. custom_objects is a dictionary \n",
    "       holding name and Class implementation for custom layers.\n",
    "       At end of every batch, the update is as follows:\n",
    "       mv_weight -= (1 - decay) * (mv_weight - weight)\n",
    "       where weight and mv_weight is the ordinal model weight and the moving\n",
    "       averaged weight respectively. At the end of the training, the moving\n",
    "       averaged weights are transferred to the original model.\n",
    "       \"\"\"\n",
    "    def __init__(self, decay=0.999, filepath='/temp_weight.hdf5',\n",
    "                 save_mv_ave_model=True, verbose=0,\n",
    "                 save_best_only=False, monitor='val_loss', mode='auto',\n",
    "                 save_weights_only=False, custom_objects={}):\n",
    "        self.decay = decay\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.save_mv_ave_model = save_mv_ave_model\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.save_best_only = save_best_only\n",
    "        self.monitor = monitor\n",
    "        self.custom_objects = custom_objects  # dictionary of custom layers\n",
    "        self.sym_trainable_weights = None  # trainable weights of model\n",
    "        self.mv_trainable_weights_vals = None  # moving averaged values\n",
    "        super(ExponentialMovingAverage, self).__init__()\n",
    "        \n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        for weight in self.sym_trainable_weights:\n",
    "            old_val = self.mv_trainable_weights_vals[weight.name]\n",
    "            self.mv_trainable_weights_vals[weight.name] -= \\\n",
    "                (1.0 - self.decay) * (old_val - K.get_value(weight))\n",
    "            \n",
    "    def _make_mv_model(self, filepath):\n",
    "        \"\"\" Create a model with moving averaged weights. Other variables are\n",
    "        the same as original mode. We first save original model to save its\n",
    "        state. Then copy moving averaged weights over.\"\"\"\n",
    "        self.model.save(filepath, overwrite=True)\n",
    "        model2 = load_model(filepath, custom_objects=self.custom_objects)\n",
    "        for sym_weight in collect_trainable_weights(model2):\n",
    "            K.set_value(sym_weight, self.mv_trainable_weights_vals[sym_weight.name])\n",
    "        return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = ExponentialMovingAverage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Embedding(vocab_size, 100, input_length=50))\n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(50, 5, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(Bidirectional(LSTM(400)))\n",
    "    model_conv.add(Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = create_conv_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lol (string):\n",
    "    string = re.sub(r\"\\(\", \" \", string)\n",
    "    string = re.sub(r\"\\)\", \" \", string)\n",
    "    string = re.sub(r\"\\[\", \" \", string)\n",
    "    string = re.sub(r\"\\]\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "l = lol(str(list(trainX.shape[1:])))\n",
    "int(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME IMPLEMENTED BY ARUSHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>EssaySet</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>EssayText</th>\n",
       "      <th>clean</th>\n",
       "      <th>X</th>\n",
       "      <th>X_1</th>\n",
       "      <th>CosineSim</th>\n",
       "      <th>CosineSim_2</th>\n",
       "      <th>error_words</th>\n",
       "      <th>total_errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Some additional information that we would need...</td>\n",
       "      <td>some additional information that we would need...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>0.601346</td>\n",
       "      <td>0.885387</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>After reading the expirement, I realized that ...</td>\n",
       "      <td>after reading the expirement , i realized that...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>0.877983</td>\n",
       "      <td>0.847917</td>\n",
       "      <td>[expirement, expireiment, amant, yar, expireme...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What you need is more trials, a control set up...</td>\n",
       "      <td>what you need is more trials , a control set u...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>0.932329</td>\n",
       "      <td>0.826848</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The student should list what rock is better an...</td>\n",
       "      <td>the student should list what rock is better an...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>0.760292</td>\n",
       "      <td>0.802420</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>For the students to be able to make a replicat...</td>\n",
       "      <td>for the students to be able to make a replicat...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>0.850489</td>\n",
       "      <td>0.902783</td>\n",
       "      <td>[tipe, expirement]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  EssaySet  Score1  Score2  \\\n",
       "0   1         1       1       1   \n",
       "1   2         1       1       1   \n",
       "2   3         1       1       1   \n",
       "3   4         1       0       0   \n",
       "4   5         1       2       2   \n",
       "\n",
       "                                           EssayText  \\\n",
       "0  Some additional information that we would need...   \n",
       "1  After reading the expirement, I realized that ...   \n",
       "2  What you need is more trials, a control set up...   \n",
       "3  The student should list what rock is better an...   \n",
       "4  For the students to be able to make a replicat...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  some additional information that we would need...   \n",
       "1  after reading the expirement , i realized that...   \n",
       "2  what you need is more trials , a control set u...   \n",
       "3  the student should list what rock is better an...   \n",
       "4  for the students to be able to make a replicat...   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                                 X_1  CosineSim  CosineSim_2  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   0.601346     0.885387   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   0.877983     0.847917   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   0.932329     0.826848   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   0.760292     0.802420   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   0.850489     0.902783   \n",
       "\n",
       "                                         error_words  total_errors  \n",
       "0                                                 []             0  \n",
       "1  [expirement, expireiment, amant, yar, expireme...             6  \n",
       "2                                                 []             0  \n",
       "3                                                 []             0  \n",
       "4                                 [tipe, expirement]             2  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Essay1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts_train, texts_test, y_train, y_test = \\\n",
    "    train_test_split(Essay1[\"clean\"].values, Essay1['Score1'].values, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Max number of different word, i.e. model input dimension\n",
    "maxlen = 80  # Max number of words kept at the end of each text\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.pipeline import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class TextsToSequences(Tokenizer, BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Sklearn transformer to convert texts to indices list \n",
    "    (e.g. [[\"the cute cat\"], [\"the dog\"]] -> [[1, 2, 3], [1, 4]])\"\"\"\n",
    "    def __init__(self,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def fit(self, texts, y=None):\n",
    "        self.fit_on_texts(texts)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts, y=None):\n",
    "        return np.array(self.texts_to_sequences(texts))\n",
    "        \n",
    "sequencer = TextsToSequences(num_words=vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Padder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Pad and crop uneven lists to the same length. \n",
    "    Only the end of lists longernthan the maxlen attribute are\n",
    "    kept, and lists shorter than maxlen are left-padded with zeros\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    maxlen: int\n",
    "        sizes of sequences after padding\n",
    "    max_index: int\n",
    "        maximum index known by the Padder, if a higher index is met during \n",
    "        transform it is transformed to a 0\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen=500):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_index = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.max_index = pad_sequences(X, maxlen=self.maxlen).max()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = pad_sequences(X, maxlen=self.maxlen)\n",
    "        X[X > self.max_index] = 0\n",
    "        return X\n",
    "\n",
    "padder = Padder(maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1254/1254 [==============================] - 7s 5ms/step - loss: -1.1340 - acc: 0.2560\n",
      "Epoch 2/2\n",
      "1254/1254 [==============================] - 5s 4ms/step - loss: -6.7561 - acc: 0.2608\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Bidirectional, LSTM\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "batch_size = 64\n",
    "max_features = vocab_size + 1\n",
    "\n",
    "def create_model(max_features):\n",
    "    \"\"\" Model creation function: returns a compiled Bidirectional LSTM\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128))\n",
    "    model.add(LSTM(128, dropout=0.5, recurrent_dropout=0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Use Keras Scikit-learn wrapper to instantiate a LSTM with all methods\n",
    "# required by Scikit-learn for the last step of a Pipeline\n",
    "sklearn_lstm = KerasClassifier(build_fn=create_model, epochs=2, batch_size=batch_size, \n",
    "                               max_features=max_features, verbose=1)\n",
    "\n",
    "# Build the Scikit-learn pipeline\n",
    "pipeline = make_pipeline(sequencer, padder, sklearn_lstm)\n",
    "\n",
    "pipeline.fit(texts_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing predictions on test set...\n",
      "418/418 [==============================] - 1s 2ms/step\n",
      "Test accuracy: 24.40 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('Computing predictions on test set...')\n",
    "y_preds = pipeline.predict(texts_test)\n",
    "\n",
    "print('Test accuracy: {:.2f} %'.format(100*metrics.accuracy_score(y_preds, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 11: last 80 words (only part used by the model)\n",
      "--------------------------------------------------\n",
      "one thing they could have that they do n't is a control of this procedure the next thing they could have changed that they could have said how to get or determine the mas of each of the building material the final thing is that i think with starting and ending mass they should have multipletllals\n",
      "--------------------------------------------------\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Probability(positive) = 1.0\n",
      "True class: negative\n"
     ]
    }
   ],
   "source": [
    "idx = 11\n",
    "text_sample = texts_test[idx]\n",
    "class_names = ['negative', 'positive']\n",
    "\n",
    "print('Sample {}: last 80 words (only part used by the model)'.format(idx))\n",
    "print('-'*50)\n",
    "print(\" \".join(text_sample.split()[-80:]))\n",
    "print('-'*50)\n",
    "print('Probability(positive) =', pipeline.predict_proba([text_sample])[0,1])\n",
    "print('True class: %s' % class_names[y_test[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aruhi/tensorflow/venv/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 5s 976us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from collections import OrderedDict\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "explanation = explainer.explain_instance(text_sample, pipeline.predict_proba, num_features=10)\n",
    "\n",
    "weights = OrderedDict(explanation.as_list())\n",
    "lime_weights = pd.DataFrame({'words': list(weights.keys()), 'weights': list(weights.values())})\n",
    "\n",
    "sns.barplot(x=\"words\", y=\"weights\", data=lime_weights);\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Sample {} features weights given by LIME'.format(idx));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'EssaySet', 'Score1', 'Score2', 'EssayText', 'clean', 'X', 'X_1',\n",
       "       'CosineSim', 'CosineSim_2', 'error_words', 'total_errors'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Essay1.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = Essay1[['CosineSim', 'CosineSim_2', 'error_words', 'total_errors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-91a28e463262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m explainer = lime_tabular.RecurrentTabularExplainer(texts_train, training_labels=y_train, feature_names=data_columns,\n\u001b[1;32m      3\u001b[0m                                                    \u001b[0mdiscretize_continuous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                                    \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"3\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                                                    )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.6/site-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, training_data, training_labels, feature_names, categorical_features, categorical_names, kernel_width, verbose, class_names, feature_selection, discretize_continuous, discretizer, random_state)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# Reshape X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         training_data = np.transpose(training_data, axes=(0, 2, 1)).reshape(\n\u001b[1;32m    495\u001b[0m                 n_samples, n_timesteps * n_features)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "from lime import lime_tabular\n",
    "explainer = lime_tabular.RecurrentTabularExplainer(texts_train, training_labels=y_train, feature_names=data_columns,\n",
    "                                                   discretize_continuous=True,\n",
    "                                                   class_names=[\"0\",\"1\",\"2\",\"3\"]\n",
    "                                                   )\n",
    "\n",
    "\n",
    "exp = explainer.explain_instance(X_test[50], pipeline.predict, num_features=10, labels=(1,))\n",
    "exp.show_in_notebook()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>EssaySet</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>EssayText</th>\n",
       "      <th>clean</th>\n",
       "      <th>X</th>\n",
       "      <th>X_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Some additional information that we would need...</td>\n",
       "      <td>some additional information that we would need...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>After reading the expirement, I realized that ...</td>\n",
       "      <td>after reading the expirement , i realized that...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What you need is more trials, a control set up...</td>\n",
       "      <td>what you need is more trials , a control set u...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The student should list what rock is better an...</td>\n",
       "      <td>the student should list what rock is better an...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>For the students to be able to make a replicat...</td>\n",
       "      <td>for the students to be able to make a replicat...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  EssaySet  Score1  Score2  \\\n",
       "0   1         1       1       1   \n",
       "1   2         1       1       1   \n",
       "2   3         1       1       1   \n",
       "3   4         1       0       0   \n",
       "4   5         1       2       2   \n",
       "\n",
       "                                           EssayText  \\\n",
       "0  Some additional information that we would need...   \n",
       "1  After reading the expirement, I realized that ...   \n",
       "2  What you need is more trials, a control set up...   \n",
       "3  The student should list what rock is better an...   \n",
       "4  For the students to be able to make a replicat...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  some additional information that we would need...   \n",
       "1  after reading the expirement , i realized that...   \n",
       "2  what you need is more trials , a control set u...   \n",
       "3  the student should list what rock is better an...   \n",
       "4  for the students to be able to make a replicat...   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                                 X_1  \n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...  \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...  \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...  \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...  \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(spell_errors, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_words</th>\n",
       "      <th>total_errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[expirement, expireiment, amant, yar, expireme...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tipe, expirement]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         error_words  total_errors\n",
       "0                                                 []             0\n",
       "1  [expirement, expireiment, amant, yar, expireme...             6\n",
       "2                                                 []             0\n",
       "3                                                 []             0\n",
       "4                                 [tipe, expirement]             2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>EssaySet</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>EssayText</th>\n",
       "      <th>clean</th>\n",
       "      <th>X</th>\n",
       "      <th>X_1</th>\n",
       "      <th>error_words</th>\n",
       "      <th>total_errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Some additional information that we would need...</td>\n",
       "      <td>some additional information that we would need...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>After reading the expirement, I realized that ...</td>\n",
       "      <td>after reading the expirement , i realized that...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[expirement, expireiment, amant, yar, expireme...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What you need is more trials, a control set up...</td>\n",
       "      <td>what you need is more trials , a control set u...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The student should list what rock is better an...</td>\n",
       "      <td>the student should list what rock is better an...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>For the students to be able to make a replicat...</td>\n",
       "      <td>for the students to be able to make a replicat...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[tipe, expirement]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  EssaySet  Score1  Score2  \\\n",
       "0   1         1       1       1   \n",
       "1   2         1       1       1   \n",
       "2   3         1       1       1   \n",
       "3   4         1       0       0   \n",
       "4   5         1       2       2   \n",
       "\n",
       "                                           EssayText  \\\n",
       "0  Some additional information that we would need...   \n",
       "1  After reading the expirement, I realized that ...   \n",
       "2  What you need is more trials, a control set up...   \n",
       "3  The student should list what rock is better an...   \n",
       "4  For the students to be able to make a replicat...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  some additional information that we would need...   \n",
       "1  after reading the expirement , i realized that...   \n",
       "2  what you need is more trials , a control set u...   \n",
       "3  the student should list what rock is better an...   \n",
       "4  for the students to be able to make a replicat...   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                                 X_1  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                         error_words  total_errors  \n",
       "0                                                 []             0  \n",
       "1  [expirement, expireiment, amant, yar, expireme...             6  \n",
       "2                                                 []             0  \n",
       "3                                                 []             0  \n",
       "4                                 [tipe, expirement]             2  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def load_google_word2vec(file_name):\n",
    "    return KeyedVectors.load_word2vec_format(file_name, binary=True)\n",
    "\n",
    "\n",
    "def build_embedding_matrix(vocab_size, embed_dim, tokenizer ):\n",
    "    fname = '/home/aruhi/Word Embeddings/GoogleNews-vectors-negative300.bin' \n",
    "\n",
    "    model=load_google_word2vec(fname)\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "    word2vec={}\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n",
    "            word2vec[word]=i\n",
    "\n",
    "\n",
    "\n",
    "    return embedding_matrix,word2vec\n",
    "\n",
    "embedding, w2v= build_embedding_matrix(vocab_size,300,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2vec=[]\n",
    "for i in data['clean']:\n",
    "    vec=[]\n",
    "    for word in i.lower().split(\" \"):\n",
    "        try:\n",
    "            vec.append(embedding[w2v[word]])\n",
    "            \n",
    "        except:\n",
    "            v=np.zeros((300),dtype=float)\n",
    "            vec.append(v)\n",
    "            \n",
    "        \n",
    "    sentence2vec.append(np.sum(vec,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ans = 'You need to know how much vinegar was used in each container. You need to know what type of vinegar was used in each container. You need to know what materials to test. You need to know what size/surface area of materials should be used. You need to know how long each sample was rinsed in distilled water. You need to know what drying method to use. You need to know what size or type of container to use.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence2vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sentence2vec=[]\n",
    "for word in ref_ans.lower().split(\" \"):\n",
    "    try:\n",
    "        ref_sentence2vec.append(embedding[w2v[word]])\n",
    "\n",
    "    except:\n",
    "        v=np.zeros((300),dtype=float)\n",
    "        ref_sentence2vec.append(v)\n",
    "        \n",
    "ref_sentence2vec=np.sum(ref_sentence2vec,axis=0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_answer= [ref_sentence2vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ref_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence2vec[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.868467704365016,\n",
       " 0.8365223502935273,\n",
       " 0.8350536310897093,\n",
       " 0.7103181283822403,\n",
       " 0.878116198372477,\n",
       " 0.8659535142888942,\n",
       " 0.8366568089997555,\n",
       " 0.9257906617986468,\n",
       " 0.9212596346937105,\n",
       " 0.8557629259013935,\n",
       " 0.7105554735306945,\n",
       " 0.8915602179081233,\n",
       " 0.7937025153885792,\n",
       " 0.7342045668841304,\n",
       " 0.8588230701482903,\n",
       " 0.7768037513080851,\n",
       " 0.8624085191767932,\n",
       " 0.8063082810434317,\n",
       " 0.773971658115786,\n",
       " 0.8632518573193816,\n",
       " 0.8241436655109078,\n",
       " 0.8373284687423114,\n",
       " 0.7922314518553706,\n",
       " 0.9247706139028312,\n",
       " 0.805584347731497,\n",
       " 0.8606979477900618,\n",
       " 0.8193227731895854,\n",
       " 0.6306805977175522,\n",
       " 0.8550815753020921,\n",
       " 0.8904728184183132,\n",
       " 0.8498247078190577,\n",
       " 0.8312844646462336,\n",
       " 0.901371193299927,\n",
       " 0.8897566653154866,\n",
       " 0.9405639747321971,\n",
       " 0.9133459532226014,\n",
       " 0.8872656258595615,\n",
       " 0.8601966208624421,\n",
       " 0.7994507878511117,\n",
       " 0.8854890003865453,\n",
       " 0.8130267300625353,\n",
       " 0.9241225956916682,\n",
       " 0.8888565658323216,\n",
       " 0.8714999741704461,\n",
       " 0.8590223497913986,\n",
       " 0.8264361369698163,\n",
       " 0.8477210885050034,\n",
       " 0.7744721877507803,\n",
       " 0.8200918256347562,\n",
       " 0.8469532429941152,\n",
       " 0.8544823619341051,\n",
       " 0.772241309483299,\n",
       " 0.7094689137155007,\n",
       " 0.921128455509946,\n",
       " 0.9083833297498805,\n",
       " 0.783061235177104,\n",
       " 0.7595120360174764,\n",
       " 0.8770042142968348,\n",
       " 0.9118107721692568,\n",
       " 0.79620554939205,\n",
       " 0.8919379804996745,\n",
       " 0.8951986565923106,\n",
       " 0.8006998198956254,\n",
       " 0.8862501504448704,\n",
       " 0.8416628999472657,\n",
       " 0.8056350375673055,\n",
       " 0.8404295846422898,\n",
       " 0.8410876164912356,\n",
       " 0.5933563466706396,\n",
       " 0.8767862344246409,\n",
       " 0.8982357114324548,\n",
       " 0.900431757638436,\n",
       " 0.7988939140006763,\n",
       " 0.7440153874318135,\n",
       " 0.7991574029070582,\n",
       " 0.9293624716529127,\n",
       " 0.8578407035892974,\n",
       " 0.7726638363218157,\n",
       " 0.9042677116934574,\n",
       " 0.8519103807070195,\n",
       " 0.8256156832484164,\n",
       " 0.8900540326761088,\n",
       " 0.9172076116040851,\n",
       " 0.8335852985654835,\n",
       " 0.8036024947006049,\n",
       " 0.905280579211047,\n",
       " 0.7261043854676738,\n",
       " 0.8840996472791468,\n",
       " 0.6785108667018815,\n",
       " 0.8821085225581518,\n",
       " 0.8936104970320145,\n",
       " 0.7767610636998241,\n",
       " 0.9122910740854926,\n",
       " 0.8664534778123654,\n",
       " 0.7709339877470331,\n",
       " 0.8006467131998743,\n",
       " 0.8696538627932547,\n",
       " 0.7543652933362208,\n",
       " 0.8081437541758446,\n",
       " 0.7548606531218109,\n",
       " 0.8530989558912186,\n",
       " 0.9079631876116938,\n",
       " 0.858428212678794,\n",
       " 0.7791759683233697,\n",
       " 0.8593759008460848,\n",
       " 0.8647276916349322,\n",
       " 0.8437479455652029,\n",
       " 0.8556534463720671,\n",
       " 0.7565090109106434,\n",
       " 0.7796349121921854,\n",
       " 0.7728845522415139,\n",
       " 0.8320310036504174,\n",
       " 0.8994629684656764,\n",
       " 0.836994170497479,\n",
       " 0.8997658701345025,\n",
       " 0.5889523917035655,\n",
       " 0.9067614191254679,\n",
       " 0.8606095764173389,\n",
       " 0.6521032228955548,\n",
       " 0.8040289800495106,\n",
       " 0.7889881977910045,\n",
       " 0.8772892106679214,\n",
       " 0.9377636763918151,\n",
       " 0.8907692389968511,\n",
       " 0.6362455691076526,\n",
       " 0.9564646899029451,\n",
       " 0.8069776925022142,\n",
       " 0.7439156726444021,\n",
       " 0.7794793277217332,\n",
       " 0.8317427474627986,\n",
       " 0.8748457456976775,\n",
       " 0.7692826042604199,\n",
       " 0.9002057297323459,\n",
       " 0.8804292779896906,\n",
       " 0.9284061689690513,\n",
       " 0.8689454905327216,\n",
       " 0.8398556608524154,\n",
       " 0.8536399852515827,\n",
       " 0.8520952798864804,\n",
       " 0.8632951191160433,\n",
       " 0.8875224209538998,\n",
       " 0.8903721299800857,\n",
       " 0.9336635171663742,\n",
       " 0.8195259735388303,\n",
       " 0.853579685304097,\n",
       " 0.8501406004302847,\n",
       " 0.8735357582063262,\n",
       " 0.8349683502826492,\n",
       " 0.7494965979439888,\n",
       " 0.841026734020975,\n",
       " 0.8889314897818194,\n",
       " 0.8737622783473272,\n",
       " 0.8034599460208344,\n",
       " 0.7945362434586427,\n",
       " 0.9151058491913812,\n",
       " 0.82989291463428,\n",
       " 0.7331776399476144,\n",
       " 0.8539928317283911,\n",
       " 0.8352377186623279,\n",
       " 0.8592353589095302,\n",
       " 0.9198589615077063,\n",
       " 0.8516672629483715,\n",
       " 0.8849330512888117,\n",
       " 0.8830098099026834,\n",
       " 0.8958289753738242,\n",
       " 0.8196638981599421,\n",
       " 0.9023635236827574,\n",
       " 0.8933734049007701,\n",
       " 0.6846502976113142,\n",
       " 0.9313391535795278,\n",
       " 0.8713338291511618,\n",
       " 0.830311286430776,\n",
       " 0.745427407960403,\n",
       " 0.8819588863145281,\n",
       " 0.8561983224606683,\n",
       " 0.845891671489806,\n",
       " 0.842518116089929,\n",
       " 0.8264433484192082,\n",
       " 0.875962420542756,\n",
       " 0.6795864738556778,\n",
       " 0.8872171647953432,\n",
       " 0.9213058876142168,\n",
       " 0.8309387486521617,\n",
       " 0.8656901744821257,\n",
       " 0.7864425594984984,\n",
       " 0.7379355520486853,\n",
       " 0.8457538363905609,\n",
       " 0.8770788733610767,\n",
       " 0.7109411560053429,\n",
       " 0.9237560723554906,\n",
       " 0.775423906539987,\n",
       " 0.7906048212549909,\n",
       " 0.8144558591995764,\n",
       " 0.8259064842925594,\n",
       " 0.9013329824418097,\n",
       " 0.8931661826575705,\n",
       " 0.875069723849896,\n",
       " 0.8922104519625133,\n",
       " 0.9469995714535501,\n",
       " 0.797745461174474,\n",
       " 0.6311030792470632,\n",
       " 0.7847073432557454,\n",
       " 0.7595044831412467,\n",
       " 0.8778186591160424,\n",
       " 0.8608414283948569,\n",
       " 0.9194794641426539,\n",
       " 0.8601216280471219,\n",
       " 0.8420811943312071,\n",
       " 0.8893831554076932,\n",
       " 0.766357486502721,\n",
       " 0.8389577182372531,\n",
       " 0.31867756214811843,\n",
       " 0.8189420475498087,\n",
       " 0.8781516143415206,\n",
       " 0.8830611130739144,\n",
       " 0.8848386188561119,\n",
       " 0.7894256176457832,\n",
       " 0.8030075946085169,\n",
       " 0.8970616433167345,\n",
       " 0.8077996086387291,\n",
       " 0.8223279022557686,\n",
       " 0.8548156443092254,\n",
       " 0.8270348083194728,\n",
       " 0.8226309170404011,\n",
       " 0.23379190267193148,\n",
       " 0.8379943732267893,\n",
       " 0.8571764697224389,\n",
       " 0.8807216155011016,\n",
       " 0.9073167143134108,\n",
       " 0.6762672626544375,\n",
       " 0.9013493340490194,\n",
       " 0.8848760523804688,\n",
       " 0.5726598023938966,\n",
       " 0.8292991280346824,\n",
       " 0.7527734670458645,\n",
       " 0.8999283016893161,\n",
       " 0.783376679933546,\n",
       " 0.9124268933334032,\n",
       " 0.880048896340163,\n",
       " 0.9359480322880324,\n",
       " 0.838718354728454,\n",
       " 0.8820504368151464,\n",
       " 0.844685721220572,\n",
       " 0.8491057845789729,\n",
       " 0.9045755742690075,\n",
       " 0.7038596330309415,\n",
       " 0.8184907173265551,\n",
       " 0.8169840435451152,\n",
       " 0.8452140500816987,\n",
       " 0.7984281691071402,\n",
       " 0.8278850142548642,\n",
       " 0.8813990464991017,\n",
       " 0.7983933815597234,\n",
       " 0.8739527490336008,\n",
       " 0.7995290860225571,\n",
       " 0.8886731786686632,\n",
       " 0.9075359164446258,\n",
       " 0.9002172840385182,\n",
       " 0.931272276211288,\n",
       " 0.7824848095167208,\n",
       " 0.8053023927669436,\n",
       " 0.8611365148263519,\n",
       " 0.836968927422787,\n",
       " 0.7408422616250011,\n",
       " 0.8880109728751995,\n",
       " 0.8690603251914268,\n",
       " 0.7806984372539989,\n",
       " 0.8055777940540744,\n",
       " 0.818360441828661,\n",
       " 0.9041684171733361,\n",
       " 0.9123675741063135,\n",
       " 0.8659843658587375,\n",
       " 0.9218541489282047,\n",
       " 0.9398958677016274,\n",
       " 0.9113479042636754,\n",
       " 0.8162274794825555,\n",
       " 0.7451540712517254,\n",
       " 0.8260668933463041,\n",
       " 0.8810080974460145,\n",
       " 0.8668635104415118,\n",
       " 0.8316586070529007,\n",
       " 0.8229187759890755,\n",
       " 0.8544341544460261,\n",
       " 0.8272511635411567,\n",
       " 0.8412158117615165,\n",
       " 0.8728464511416465,\n",
       " 0.8204001839999214,\n",
       " 0.8495821051941294,\n",
       " 0.9116102474596403,\n",
       " 0.8585992371453662,\n",
       " 0.8483100212695601,\n",
       " 0.8076346616142204,\n",
       " 0.8065457624388924,\n",
       " 0.846329605905204,\n",
       " 0.8694232970293762,\n",
       " 0.8642914262863749,\n",
       " 0.8713976596118109,\n",
       " 0.858038714283837,\n",
       " 0.8171996955620411,\n",
       " 0.7703205271232466,\n",
       " 0.8566217567416942,\n",
       " 0.859600050174538,\n",
       " 0.8866672330504889,\n",
       " 0.7761970422039661,\n",
       " 0.8820422861078178,\n",
       " 0.7565046818176697,\n",
       " 0.8799928116759356,\n",
       " 0.8774708121443274,\n",
       " 0.8964918375575507,\n",
       " 0.6992885453469588,\n",
       " 0.8871589973225646,\n",
       " 0.8608072993563831,\n",
       " 0.8171356565543699,\n",
       " 0.8793072116001384,\n",
       " 0.6253683775661351,\n",
       " 0.8490966289769399,\n",
       " 0.8476765254937413,\n",
       " 0.9016729340815594,\n",
       " 0.8835308235897833,\n",
       " 0.6650260414346468,\n",
       " 0.889284406558255,\n",
       " 0.8103654593157247,\n",
       " 0.9133227037869759,\n",
       " 0.8788102134429686,\n",
       " 0.8845443779943982,\n",
       " 0.7654880775144707,\n",
       " 0.9002731461684214,\n",
       " 0.8783607122219962,\n",
       " 0.8896847583659007,\n",
       " 0.8847779473574703,\n",
       " 0.8355744943891993,\n",
       " 0.8725286078610552,\n",
       " 0.7334100220115868,\n",
       " 0.8389927656977657,\n",
       " 0.8268323221154515,\n",
       " 0.840256723090354,\n",
       " 0.9324114737690851,\n",
       " 0.8723574766028515,\n",
       " 0.7796389308992688,\n",
       " 0.8250875343128371,\n",
       " 0.8348537897437676,\n",
       " 0.8945840647511967,\n",
       " 0.9233588588804994,\n",
       " 0.7997800634917187,\n",
       " 0.7894240866571826,\n",
       " 0.8268493973684808,\n",
       " 0.7262167910060865,\n",
       " 0.9015713506903014,\n",
       " 0.8441706317111018,\n",
       " 0.8311034320194076,\n",
       " 0.8696028980919982,\n",
       " 0.7675923421766305,\n",
       " 0.769251676798634,\n",
       " 0.8576338497146729,\n",
       " 0.9405671154884641,\n",
       " 0.8978394366965761,\n",
       " 0.8720739964480054,\n",
       " 0.8638916558294215,\n",
       " 0.8275002033701602,\n",
       " 0.8207698033809986,\n",
       " 0.8689409310611174,\n",
       " 0.8657865716798718,\n",
       " 0.8056556078129535,\n",
       " 0.8283355403796203,\n",
       " 0.8781566921577741,\n",
       " 0.8853376712471498,\n",
       " 0.916155103377436,\n",
       " 0.6896567131693248,\n",
       " 0.8724875051664658,\n",
       " 0.8742072820440501,\n",
       " 0.7370892743544991,\n",
       " 0.8032725018878761,\n",
       " 0.8471212705224458,\n",
       " 0.8756607254802157,\n",
       " 0.8433321252618204,\n",
       " 0.8202355285892067,\n",
       " 0.9021763637003012,\n",
       " 0.8079108204512682,\n",
       " 0.856202193929972,\n",
       " 0.9277348600755974,\n",
       " 0.8641593416154363,\n",
       " 0.8763885459216293,\n",
       " 0.8680724897767854,\n",
       " 0.9158762572604571,\n",
       " 0.8269387089262402,\n",
       " 0.8942539901934226,\n",
       " 0.6148728555837116,\n",
       " 0.8703521422811064,\n",
       " 0.9094483596809336,\n",
       " 0.9256856241583667,\n",
       " 0.9179851912554556,\n",
       " 0.8047436046320883,\n",
       " 0.9506860970216885,\n",
       " 0.8577149287652358,\n",
       " 0.7956547908566225,\n",
       " 0.8003028340760895,\n",
       " 0.7515294190732874,\n",
       " 0.8458885957976757,\n",
       " 0.7121657272418872,\n",
       " 0.8673561720315422,\n",
       " 0.878813477392113,\n",
       " 0.8510107771059774,\n",
       " 0.808964866692261,\n",
       " 0.6850666378046055,\n",
       " 0.8879040869385737,\n",
       " 0.8402794817058604,\n",
       " 0.7421346184702172,\n",
       " 0.8082201820555449,\n",
       " 0.8881972524686977,\n",
       " 0.7389329952070692,\n",
       " 0.6970432510833041,\n",
       " 0.921842381489425,\n",
       " 0.8344281089199631,\n",
       " 0.726468961117908,\n",
       " 0.8775638166039859,\n",
       " 0.8983886673250355,\n",
       " 0.8197610511660833,\n",
       " 0.8391020523596971,\n",
       " 0.8406312081884605,\n",
       " 0.8792846966433115,\n",
       " 0.8522092132324425,\n",
       " 0.8739780441506707,\n",
       " 0.8708453263357422,\n",
       " 0.9236279347357917,\n",
       " 0.8152673997201215,\n",
       " 0.853063522201032,\n",
       " 0.8448335150067053,\n",
       " 0.8225864263539981,\n",
       " 0.7546582791503372,\n",
       " 0.8326637769745806,\n",
       " 0.7494159104910794,\n",
       " 0.7499673274143174,\n",
       " 0.8306236534522474,\n",
       " 0.8348319011329678,\n",
       " 0.8117477845339965,\n",
       " 0.811272187331021,\n",
       " 0.7218764086349441,\n",
       " 0.8367004013281842,\n",
       " 0.7919969402257439,\n",
       " 0.7672576103182369,\n",
       " 0.8653603351691365,\n",
       " 0.909561943944516,\n",
       " 0.8455588165504936,\n",
       " 0.8820414977489404,\n",
       " 0.8945842457675945,\n",
       " 0.7198661608229974,\n",
       " 0.8186511235446872,\n",
       " 0.8804392478791816,\n",
       " 0.6704771276302897,\n",
       " 0.8840466231956543,\n",
       " 0.8089508510272008,\n",
       " 0.9291898444062533,\n",
       " 0.8211311442904011,\n",
       " 0.9289952521822642,\n",
       " 0.8836340532588655,\n",
       " 0.8775748515021428,\n",
       " 0.8041231180143522,\n",
       " 0.8808314186515954,\n",
       " 0.9001682186448486,\n",
       " 0.7103110312627063,\n",
       " 0.8023119160255194,\n",
       " 0.9025286531939518,\n",
       " 0.8641353458302062,\n",
       " 0.8617170219546116,\n",
       " 0.8971906926460663,\n",
       " 0.9529620065259201,\n",
       " 0.7802252802675546,\n",
       " 0.9125307536014446,\n",
       " 0.9310550558691868,\n",
       " 0.710515008168961,\n",
       " 0.893195053216035,\n",
       " 0.8372424304552255,\n",
       " 0.9045079981920252,\n",
       " 0.8621125064280539,\n",
       " 0.7813788547105863,\n",
       " 0.9361891182667346,\n",
       " 0.8909417870255694,\n",
       " 0.8669545405798548,\n",
       " 0.8384433361737751,\n",
       " 0.8205744233874556,\n",
       " 0.8644968904130665,\n",
       " 0.8992160905941662,\n",
       " 0.9094852493678333,\n",
       " 0.6082729056929612,\n",
       " 0.6492078152058324,\n",
       " 0.8379131838081845,\n",
       " 0.0,\n",
       " 0.7492298579217873,\n",
       " 0.7296549675098901,\n",
       " 0.9071019329451819,\n",
       " 0.8881427863479001,\n",
       " 0.8137922179872259,\n",
       " 0.7709432912401941,\n",
       " 0.8870192915143441,\n",
       " 0.8967689802409232,\n",
       " 0.8342536316160204,\n",
       " 0.8783570298916971,\n",
       " 0.8384812722579738,\n",
       " 0.900443084520427,\n",
       " 0.8788635129981051,\n",
       " 0.8380532758372188,\n",
       " 0.9397378738499311,\n",
       " 0.8756319823473194,\n",
       " 0.8394694109442187,\n",
       " 0.9161578998822288,\n",
       " 0.8746929809584465,\n",
       " 0.9009621105415228,\n",
       " 0.8727128611697523,\n",
       " 0.8615799165555977,\n",
       " 0.8739950241703242,\n",
       " 0.9167563831301592,\n",
       " 0.698628203996044,\n",
       " 0.8473599013555867,\n",
       " 0.8605833714046208,\n",
       " 0.8748995966439428,\n",
       " 0.8791602512176121,\n",
       " 0.8074071605839389,\n",
       " 0.8800232117020212,\n",
       " 0.8385598188430017,\n",
       " 0.6855574528183922,\n",
       " 0.8466577864661184,\n",
       " 0.8229463655688701,\n",
       " 0.9028366995745435,\n",
       " 0.8012425349563761,\n",
       " 0.7424510607733114,\n",
       " 0.9090584940909539,\n",
       " 0.845212723519891,\n",
       " 0.8620128662811539,\n",
       " 0.8213408299496646,\n",
       " 0.885532709000937,\n",
       " 0.85810827335593,\n",
       " 0.6133569021407878,\n",
       " 0.7414832096286524,\n",
       " 0.92232307121529,\n",
       " 0.6290519985239781,\n",
       " 0.8745706586201998,\n",
       " 0.8284082494763845,\n",
       " 0.5695931537485316,\n",
       " 0.8759351815074743,\n",
       " 0.8615947612027062,\n",
       " 0.851914217427683,\n",
       " 0.7973849526885706,\n",
       " 0.7996457983676012,\n",
       " 0.8509192689345674,\n",
       " 0.6416685839428021,\n",
       " 0.9284997444468072,\n",
       " 0.9067251455838305,\n",
       " 0.8386534177946999,\n",
       " 0.8301930106831583,\n",
       " 0.8838426735077121,\n",
       " 0.7778358579431903,\n",
       " 0.8961519863256948,\n",
       " 0.8275669686759503,\n",
       " 0.8346506296619578,\n",
       " 0.88843556842555,\n",
       " 0.9520463400824858,\n",
       " 0.720882313946736,\n",
       " 0.9376988690284975,\n",
       " 0.889438880018157,\n",
       " 0.8698379231990613,\n",
       " 0.8793628823911489,\n",
       " 0.8282506289228457,\n",
       " 0.8775091539419458,\n",
       " 0.8456463677888465,\n",
       " 0.8807264579937611,\n",
       " 0.8871597264871448,\n",
       " 0.8907470415951808,\n",
       " 0.875440328029245,\n",
       " 0.8045580628045009,\n",
       " 0.8498111651910389,\n",
       " 0.9396926671907909,\n",
       " 0.829980869406681,\n",
       " 0.8607580321868239,\n",
       " 0.8312036022970655,\n",
       " 0.8999242703167566,\n",
       " 0.8421446145090886,\n",
       " 0.8568048078384807,\n",
       " 0.9186472197809837,\n",
       " 0.943092446967628,\n",
       " 0.7274677256118339,\n",
       " 0.8840962788774114,\n",
       " 0.7913546295806573,\n",
       " 0.8813066047272125,\n",
       " 0.7837181207301256,\n",
       " 0.8932704257699411,\n",
       " 0.7625211246061063,\n",
       " 0.8988917380859693,\n",
       " 0.9079931767218646,\n",
       " 0.8497613689820734,\n",
       " 0.8602286675296253,\n",
       " 0.7656881128005655,\n",
       " 0.8934515722097143,\n",
       " 0.8570406417696188,\n",
       " 0.7267619732355795,\n",
       " 0.8949150971412695,\n",
       " 0.804664573247702,\n",
       " 0.8512457456449977,\n",
       " 0.8554647323434856,\n",
       " 0.8629515285086172,\n",
       " 0.8332126378279472,\n",
       " 0.8272558317064509,\n",
       " 0.8327701518675856,\n",
       " 0.8708538267167326,\n",
       " 0.8938449612444669,\n",
       " 0.9270234145766517,\n",
       " 0.8444578622460511,\n",
       " 0.8991062248238413,\n",
       " 0.772429668612408,\n",
       " 0.8598243995115568,\n",
       " 0.8393094008380043,\n",
       " 0.9061982792962331,\n",
       " 0.7479842908267949,\n",
       " 0.7443953012803022,\n",
       " 0.8640639211285172,\n",
       " 0.8465914966181859,\n",
       " 0.8564211080200105,\n",
       " 0.953958244100612,\n",
       " 0.850864841962264,\n",
       " 0.6805857411527747,\n",
       " 0.7298275878426993,\n",
       " 0.8334302115225771,\n",
       " 0.9017372378559266,\n",
       " 0.8373853193063774,\n",
       " 0.8910689412122507,\n",
       " 0.8847799034959745,\n",
       " 0.8214537357529267,\n",
       " 0.7938384875507911,\n",
       " 0.7936424723826261,\n",
       " 0.8761943797686679,\n",
       " 0.8651773545234093,\n",
       " 0.925996774226277,\n",
       " 0.8258286089983057,\n",
       " 0.8152349073375962,\n",
       " 0.8574584465532261,\n",
       " 0.9237084693516471,\n",
       " 0.8116810099813082,\n",
       " 0.7158757019263767,\n",
       " 0.8839914455685709,\n",
       " 0.8980471142274751,\n",
       " 0.8789181653234825,\n",
       " 0.8375635150149302,\n",
       " 0.8223407639151931,\n",
       " 0.9546238286381563,\n",
       " 0.669161294850162,\n",
       " 0.8488016618956625,\n",
       " 0.635693795411003,\n",
       " 0.8403915498024838,\n",
       " 0.8033334368372851,\n",
       " 0.8915668812534211,\n",
       " 0.8502489073977914,\n",
       " 0.9253457751362937,\n",
       " 0.8896640869251552,\n",
       " 0.8595960905946815,\n",
       " 0.8568553497409775,\n",
       " 0.8647678454949188,\n",
       " 0.8783406788791541,\n",
       " 0.9073196640722482,\n",
       " 0.8711621094154145,\n",
       " 0.934721011325482,\n",
       " 0.8244105419997064,\n",
       " 0.8810893921122673,\n",
       " 0.7874230878825299,\n",
       " 0.896270008538134,\n",
       " 0.7918623850883454,\n",
       " 0.8669554965017843,\n",
       " 0.8351515835585628,\n",
       " 0.8925983280245645,\n",
       " 0.8550461454299901,\n",
       " 0.8058776050859581,\n",
       " 0.6304439275169623,\n",
       " 0.9387080431725178,\n",
       " 0.8639960856592964,\n",
       " 0.8198839093741415,\n",
       " 0.9386773795410324,\n",
       " 0.8945159321876301,\n",
       " 0.6398444204880976,\n",
       " 0.8775863444052832,\n",
       " 0.8195577323090073,\n",
       " 0.7350447417447732,\n",
       " 0.8045698005747605,\n",
       " 0.8504102112283337,\n",
       " 0.7478024080789478,\n",
       " 0.90380782422405,\n",
       " 0.8445946659669459,\n",
       " 0.8770586479347202,\n",
       " 0.9230986471526552,\n",
       " 0.8827731813098775,\n",
       " 0.9108945325609856,\n",
       " 0.8584546172067757,\n",
       " 0.7354744450687268,\n",
       " 0.8068497468015455,\n",
       " 0.8968101775583421,\n",
       " 0.8855182838211998,\n",
       " 0.8910805824357151,\n",
       " 0.8011045734299096,\n",
       " 0.8070850997016938,\n",
       " 0.8839957194611542,\n",
       " 0.7718774618740764,\n",
       " 0.6246577270997142,\n",
       " 0.9020765664402727,\n",
       " 0.8783195959334991,\n",
       " 0.8195388832489505,\n",
       " 0.8718357709520479,\n",
       " 0.8471501867030832,\n",
       " 0.8305466481665186,\n",
       " 0.9019831084558877,\n",
       " 0.9040603287885707,\n",
       " 0.8219422555220305,\n",
       " 0.9534896364364345,\n",
       " 0.8690248590252228,\n",
       " 0.7096680262892797,\n",
       " 0.9035924722299299,\n",
       " 0.7966308474450687,\n",
       " 0.7770620327508658,\n",
       " 0.8920076931783548,\n",
       " 0.8421369721966474,\n",
       " 0.8003983984607068,\n",
       " 0.8368870538299666,\n",
       " 0.8357331332402108,\n",
       " 0.8509356175203773,\n",
       " 0.8554861211303741,\n",
       " 0.7411909976517345,\n",
       " 0.8531697873243542,\n",
       " 0.8309779183500975,\n",
       " 0.8351809662051248,\n",
       " 0.8965793706109776,\n",
       " 0.796552995513153,\n",
       " 0.8975179342011891,\n",
       " 0.8431768361506231,\n",
       " 0.8711962478394859,\n",
       " 0.8091637061825543,\n",
       " 0.9125507979247737,\n",
       " 0.9214629965252911,\n",
       " 0.8544948471619077,\n",
       " 0.6845707308837422,\n",
       " 0.7975923551717875,\n",
       " 0.8216967617495006,\n",
       " 0.7885577192862767,\n",
       " 0.8274594204142122,\n",
       " 0.8355946057565723,\n",
       " 0.7141424495246659,\n",
       " 0.7600200391187779,\n",
       " 0.7181012179363441,\n",
       " 0.8152411465641161,\n",
       " 0.8972802254596128,\n",
       " 0.8482930078437848,\n",
       " 0.9188529344365288,\n",
       " 0.7712678926135166,\n",
       " 0.8440180775436442,\n",
       " 0.9254210756561744,\n",
       " 0.8961305226901316,\n",
       " 0.7744247465679546,\n",
       " 0.876673285747246,\n",
       " 0.8597782361147663,\n",
       " 0.7153492418061169,\n",
       " 0.904973563326903,\n",
       " 0.6876050187635088,\n",
       " 0.8606902158096214,\n",
       " 0.7150416269378455,\n",
       " 0.905053280960047,\n",
       " 0.8429058631062595,\n",
       " 0.860115699499286,\n",
       " 0.837844776201082,\n",
       " 0.8801440821870476,\n",
       " 0.7145189211075401,\n",
       " 0.8638203181665997,\n",
       " 0.9266880893112969,\n",
       " 0.826458733108943,\n",
       " 0.9409078462563306,\n",
       " 0.811467292295419,\n",
       " 0.9118448022409076,\n",
       " 0.8547397629952243,\n",
       " 0.8883422392500403,\n",
       " 0.8382845693620815,\n",
       " 0.8023063609919191,\n",
       " 0.8200878932252893,\n",
       " 0.7537822556080629,\n",
       " 0.9171042779769486,\n",
       " 0.800430853480247,\n",
       " 0.6159221510767593,\n",
       " 0.8410425954295633,\n",
       " 0.7525156901087224,\n",
       " 0.8785420132292252,\n",
       " 0.8036161910072066,\n",
       " 0.8892472130165622,\n",
       " 0.7961070729234946,\n",
       " 0.8772054886949495,\n",
       " 0.8478050803687154,\n",
       " 0.8086780222045595,\n",
       " 0.8945550816918867,\n",
       " 0.7270376393436376,\n",
       " 0.8500452284375216,\n",
       " 0.8836866003964694,\n",
       " 0.8286459500525718,\n",
       " 0.8719117421429264,\n",
       " 0.7345368378693832,\n",
       " 0.7546856522203851,\n",
       " 0.859786226403774,\n",
       " 0.8142517522090114,\n",
       " 0.8422686931663956,\n",
       " 0.7996962505660397,\n",
       " 0.6290779240294717,\n",
       " 0.8011929780512675,\n",
       " 0.9396609223628086,\n",
       " 0.8928256868475746,\n",
       " 0.7806639287139259,\n",
       " 0.8508494020859679,\n",
       " 0.7829923405832856,\n",
       " 0.7753287588742568,\n",
       " 0.8802916234437737,\n",
       " 0.9328034657814764,\n",
       " 0.8954525411558746,\n",
       " 0.4238891552718156,\n",
       " 0.7951368524807182,\n",
       " 0.8409313555651654,\n",
       " 0.8103728737253768,\n",
       " 0.8953040015520354,\n",
       " 0.8062514262999506,\n",
       " 0.8774722460434656,\n",
       " 0.8753411013144726,\n",
       " 0.8394420793282152,\n",
       " 0.868463436088433,\n",
       " 0.49761297722974906,\n",
       " 0.8263841675080278,\n",
       " 0.8854737607921277,\n",
       " 0.832632895400151,\n",
       " 0.9490310362353591,\n",
       " 0.9179249071708971,\n",
       " 0.9039227882063035,\n",
       " 0.8375032016939248,\n",
       " 0.8433820513413949,\n",
       " 0.8906121752355847,\n",
       " 0.8975451872300627,\n",
       " 0.8588162701981091,\n",
       " 0.8529416251580944,\n",
       " 0.8966345860248646,\n",
       " 0.8683466553756475,\n",
       " 0.9331634086018735,\n",
       " 0.5944403242367466,\n",
       " 0.5822279329671918,\n",
       " 0.8730691741485745,\n",
       " 0.9097204875693314,\n",
       " 0.8549672974016512,\n",
       " 0.9071388725661254,\n",
       " 0.8315502512577827,\n",
       " 0.8612967768439584,\n",
       " 0.8086350387894498,\n",
       " 0.8878042077933094,\n",
       " 0.8469183192271033,\n",
       " 0.8810784288958357,\n",
       " 0.7208286707617939,\n",
       " 0.8851910612779182,\n",
       " 0.4320230728520605,\n",
       " 0.8289365626349108,\n",
       " 0.8668244281110206,\n",
       " 0.6278184505146419,\n",
       " 0.8816245983866289,\n",
       " 0.8219062936483685,\n",
       " 0.864883125080949,\n",
       " 0.897738780372241,\n",
       " 0.8681377180871409,\n",
       " 0.9244562623926186,\n",
       " 0.6703874199748682,\n",
       " 0.8403330950410574,\n",
       " 0.8760827907092883,\n",
       " 0.902545086095916,\n",
       " 0.7786044570407011,\n",
       " 0.8521717961304259,\n",
       " 0.9207489684167004,\n",
       " 0.8866455149802724,\n",
       " 0.9148820977821757,\n",
       " 0.8462674505702231,\n",
       " 0.862931272978173,\n",
       " 0.8522989543733223,\n",
       " 0.8233517446969579,\n",
       " 0.8475875259060394,\n",
       " 0.8932604874170273,\n",
       " 0.7068010751745419,\n",
       " 0.7813068030925384,\n",
       " 0.9071684326613395,\n",
       " 0.620888497413447,\n",
       " 0.7334011490506767,\n",
       " 0.8635212271502497,\n",
       " 0.8904237166475587,\n",
       " 0.8490617303151748,\n",
       " 0.8799602358931042,\n",
       " 0.866825014992122,\n",
       " 0.8662374569011378,\n",
       " 0.8902313502172596,\n",
       " 0.8844339913466998,\n",
       " 0.7987266083891864,\n",
       " 0.8866514832429763,\n",
       " 0.8710005806449285,\n",
       " 0.8176359113857248,\n",
       " 0.8713702818926528,\n",
       " 0.8898789097383675,\n",
       " 0.7338045473293451,\n",
       " 0.8505428021085456,\n",
       " 0.8174776418021662,\n",
       " 0.8343568572327087,\n",
       " 0.8997055856666921,\n",
       " 0.8189699853580309,\n",
       " 0.8101960272705178,\n",
       " 0.9216668006422841,\n",
       " 0.8823145660298995,\n",
       " 0.8798255632251136,\n",
       " 0.8040251370774716,\n",
       " 0.6405985787155974,\n",
       " 0.9457254486943265,\n",
       " 0.8822212411700321,\n",
       " 0.7229586517140595,\n",
       " 0.8214687262661527,\n",
       " 0.9253284406295663,\n",
       " 0.9086598306747119,\n",
       " 0.9065288230116462,\n",
       " 0.7081216225832117,\n",
       " 0.6741335854250262,\n",
       " 0.8774203384675899,\n",
       " 0.867786716909825,\n",
       " 0.9264184672597803,\n",
       " 0.832071356131443,\n",
       " 0.8487290983571567,\n",
       " 0.8933565391720879,\n",
       " 0.9227652447458105,\n",
       " 0.8581490002606715,\n",
       " 0.8387876981493104,\n",
       " 0.8901951204144578,\n",
       " 0.936493130127376,\n",
       " 0.855888216420608,\n",
       " 0.830293656670174,\n",
       " 0.8825164059096084,\n",
       " 0.8920400076326417,\n",
       " 0.8288124878873316,\n",
       " 0.890161771304652,\n",
       " 0.9187597369227553,\n",
       " 0.8629452946034545,\n",
       " 0.828079635184315,\n",
       " 0.9152806917568304,\n",
       " 0.857272006750636,\n",
       " 0.9116058404202215,\n",
       " 0.8462700855697697,\n",
       " 0.8092432240359635,\n",
       " 0.9025906332990407,\n",
       " 0.9005551286360572,\n",
       " 0.7639480236345578,\n",
       " 0.8017129362623583,\n",
       " 0.9270245153139126,\n",
       " 0.8068412980857949,\n",
       " 0.8582531672588278,\n",
       " 0.8472899678735649,\n",
       " 0.825713935758472,\n",
       " 0.5804740413660858,\n",
       " 0.8762644722077032,\n",
       " 0.9320503847503678,\n",
       " 0.8264077770850544,\n",
       " 0.9106196913136557,\n",
       " 0.8835242716295333,\n",
       " 0.8264426080862978,\n",
       " 0.8181646733541936,\n",
       " 0.6801921971606449,\n",
       " 0.8437362893268014,\n",
       " 0.8192596527225007,\n",
       " 0.8458395004337335,\n",
       " 0.9095674456716815,\n",
       " 0.9074065365591648,\n",
       " 0.817980720881941,\n",
       " 0.8945242582078509,\n",
       " 0.8975906823352096,\n",
       " 0.8401845843937428,\n",
       " 0.8881629529492588,\n",
       " 0.8647757346814088,\n",
       " 0.7913959232873162,\n",
       " 0.9265633212639752,\n",
       " 0.7268388530860724,\n",
       " 0.8581676970201437,\n",
       " 0.8220821059195829,\n",
       " 0.9127018238229035,\n",
       " 0.8584440690528279,\n",
       " 0.90570113841939,\n",
       " 0.8842449307798949,\n",
       " 0.8734289682226448,\n",
       " 0.9045091337389718,\n",
       " 0.826371111410713,\n",
       " 0.8662550328403268,\n",
       " 0.7057241843557595,\n",
       " 0.7542556519445996,\n",
       " 0.8670625533625577,\n",
       " 0.924331366736606,\n",
       " 0.9004336286998934,\n",
       " 0.8590165158743959,\n",
       " 0.8185097133471032,\n",
       " 0.8029253902716189,\n",
       " 0.8513199928077659,\n",
       " 0.8475179383072237,\n",
       " 0.8569949107761853,\n",
       " 0.8583733336563708,\n",
       " 0.8235147629501097,\n",
       " 0.8457661225021351,\n",
       " 0.8649957914422004,\n",
       " 0.8915715640371771,\n",
       " ...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "result = []\n",
    "for i in sentence2vec:\n",
    "    res = cosine_similarity(i.reshape(1,-1), ref_sentence2vec.reshape(1,-1))\n",
    "    result.append(res[0][0])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'CosineSim_2':result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>EssaySet</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>EssayText</th>\n",
       "      <th>clean</th>\n",
       "      <th>X</th>\n",
       "      <th>X_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Some additional information that we would need...</td>\n",
       "      <td>some additional information that we would need...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>After reading the expirement, I realized that ...</td>\n",
       "      <td>after reading the expirement , i realized that...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What you need is more trials, a control set up...</td>\n",
       "      <td>what you need is more trials , a control set u...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The student should list what rock is better an...</td>\n",
       "      <td>the student should list what rock is better an...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>For the students to be able to make a replicat...</td>\n",
       "      <td>for the students to be able to make a replicat...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  EssaySet  Score1  Score2  \\\n",
       "0   1         1       1       1   \n",
       "1   2         1       1       1   \n",
       "2   3         1       1       1   \n",
       "3   4         1       0       0   \n",
       "4   5         1       2       2   \n",
       "\n",
       "                                           EssayText  \\\n",
       "0  Some additional information that we would need...   \n",
       "1  After reading the expirement, I realized that ...   \n",
       "2  What you need is more trials, a control set up...   \n",
       "3  The student should list what rock is better an...   \n",
       "4  For the students to be able to make a replicat...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  some additional information that we would need...   \n",
       "1  after reading the expirement , i realized that...   \n",
       "2  what you need is more trials , a control set u...   \n",
       "3  the student should list what rock is better an...   \n",
       "4  for the students to be able to make a replicat...   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                                 X_1  \n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...  \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...  \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...  \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...  \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>EssaySet</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>EssayText</th>\n",
       "      <th>clean</th>\n",
       "      <th>X</th>\n",
       "      <th>X_1</th>\n",
       "      <th>error_words</th>\n",
       "      <th>total_errors</th>\n",
       "      <th>CosineSim_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Some additional information that we would need...</td>\n",
       "      <td>some additional information that we would need...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.868468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>After reading the expirement, I realized that ...</td>\n",
       "      <td>after reading the expirement , i realized that...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...</td>\n",
       "      <td>[expirement, expireiment, amant, yar, expireme...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.836522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What you need is more trials, a control set up...</td>\n",
       "      <td>what you need is more trials , a control set u...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.835054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The student should list what rock is better an...</td>\n",
       "      <td>the student should list what rock is better an...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>For the students to be able to make a replicat...</td>\n",
       "      <td>for the students to be able to make a replicat...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...</td>\n",
       "      <td>[tipe, expirement]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.878116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  EssaySet  Score1  Score2  \\\n",
       "0   1         1       1       1   \n",
       "1   2         1       1       1   \n",
       "2   3         1       1       1   \n",
       "3   4         1       0       0   \n",
       "4   5         1       2       2   \n",
       "\n",
       "                                           EssayText  \\\n",
       "0  Some additional information that we would need...   \n",
       "1  After reading the expirement, I realized that ...   \n",
       "2  What you need is more trials, a control set up...   \n",
       "3  The student should list what rock is better an...   \n",
       "4  For the students to be able to make a replicat...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  some additional information that we would need...   \n",
       "1  after reading the expirement , i realized that...   \n",
       "2  what you need is more trials , a control set u...   \n",
       "3  the student should list what rock is better an...   \n",
       "4  for the students to be able to make a replicat...   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                                 X_1  \\\n",
       "0  [163, 299, 67, 8, 188, 15, 49, 2, 147, 1, 54, ...   \n",
       "1  [114, 225, 1, 1107, 34, 684, 8, 1, 299, 67, 32...   \n",
       "2  [23, 32, 49, 4, 55, 199, 6, 237, 875, 127, 3, ...   \n",
       "3  [1, 152, 82, 960, 23, 1366, 4, 208, 3, 23, 136...   \n",
       "4  [20, 1, 178, 2, 14, 290, 2, 97, 6, 147, 10, 15...   \n",
       "\n",
       "                                         error_words  total_errors  \\\n",
       "0                                                 []             0   \n",
       "1  [expirement, expireiment, amant, yar, expireme...             6   \n",
       "2                                                 []             0   \n",
       "3                                                 []             0   \n",
       "4                                 [tipe, expirement]             2   \n",
       "\n",
       "   CosineSim_2  \n",
       "0     0.868468  \n",
       "1     0.836522  \n",
       "2     0.835054  \n",
       "3     0.710318  \n",
       "4     0.878116  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data,open('ASAP_withoutCosineSim_DATA.dat','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "p =pickle.load(open('ASAP_withoutCosineSim_DATA.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'EssaySet', 'Score1', 'Score2', 'EssayText', 'clean', 'X', 'X_1',\n",
       "       'error_words', 'total_errors', 'CosineSim_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
